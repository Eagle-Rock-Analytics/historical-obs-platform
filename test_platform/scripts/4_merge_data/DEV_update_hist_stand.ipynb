{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c006653",
   "metadata": {},
   "source": [
    "# Hourly Standardization Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0230415",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e15531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce83b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ds_to_df(ds):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for processing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xr.Dataset\n",
    "        Data object with information about each network and station\n",
    "    verbose: boolean\n",
    "        Flag as to whether to print runtime statements to terminal. Default is False. Set in ALLNETWORKS_merge.py run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Table object with information about each network and station\n",
    "    MultiIndex: pd.DataFrame (I think)\n",
    "        Original multi-index of station and time, to be used on conversion back to ds\n",
    "    attrs:\n",
    "        Save ds attributes to inherent to the final merged file\n",
    "    var_attrs:\n",
    "        Save variable attributes to inherent to the final merged file\n",
    "    \"\"\"\n",
    "\n",
    "    # Save attributes to inherent them to the final merged file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    df = ds.to_dataframe()\n",
    "\n",
    "    # Save instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    return df, MultiIndex, attrs, var_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a247adb",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_6001.zarr\"\n",
    "url = \"s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/ASOSAWOS_72493023230.zarr\"\n",
    "ds = xr.open_zarr(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcad3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, MultiIndex, attrs, var_attrs = merge_ds_to_df(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e023c6d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def qaqc_flag_fcn(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Used for resampling QAQC flag columns. Ensures that the final standardized dataframe\n",
    "    does not contain any empty strings by returning 'nan' when given an empty input (i.e. in time gaps).\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    x : array_like\n",
    "        sub-hourly timestep data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : final flag value\n",
    "\n",
    "    \"\"\"\n",
    "    if len(x) == 0:\n",
    "        return \"nan\"\n",
    "    else:\n",
    "        return \",\".join(x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def _modify_infill(df: pd.DataFrame, constant_vars: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function does two things:\n",
    "    1. Flags rows that were infilled by resampling in the hourly standardization process, where\n",
    "        there were time gaps in the input dataframe. These infilled rows will NOT count towards\n",
    "        the total observations count when calculating flag rates for the success report\n",
    "    2. Infills constant variables (ie those in \"constant_vars\") observations that were left empty because \n",
    "        they were in a time gap. They are infilled with the first non-nan value of each column, and set to\n",
    "        np.nan if there are no non-nan values.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : pd.Dataframe\n",
    "        hourly standardized dataframe\n",
    "    constant_vars: list\n",
    "        variables that are constant throughout time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.Dataframe\n",
    "        dataframe with updates added to rows infilled by hourly standardization\n",
    "\n",
    "    \"\"\"\n",
    "    # Mask for rows where station is None (or np.nan)\n",
    "    mask = df[\"station\"].isnull()\n",
    "\n",
    "    # Initialize dict to hold first non-NaN values\n",
    "    first_valids = {}\n",
    "\n",
    "    # Populate first_valids only for existing columns\n",
    "    for col in constant_vars:\n",
    "        if col in df.columns and col != \"time\":\n",
    "            first_valids[col] = (\n",
    "                df[col].dropna().iloc[0] if df[col].notna().any() else np.nan\n",
    "            )\n",
    "\n",
    "    # Update values in masked rows for existing columns\n",
    "    for col, val in first_valids.items():\n",
    "        df.loc[mask, col] = val\n",
    "    \n",
    "    # Add or update 'standardized_infill' column\n",
    "    df[\"standardized_infill\"] = np.where(mask, \"y\", \"n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75008308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def merge_hourly_standardization(\n",
    "    df: pd.DataFrame, var_attrs: dict\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        station dataset converted to dataframe through QAQC pipeline\n",
    "    var_attrs: library\n",
    "        attributes for sub-hourly variables\n",
    "    logger : logging.Logger\n",
    "        Logger instance for recording messages during processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame | None\n",
    "        returns a dataframe with all columns resampled to one hour (column name retained)\n",
    "    var_attrs : dict | None\n",
    "        returns variable attributes dictionary updated to note that sub-hourly variables are now hourly\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Rules:\n",
    "    1. Top of the hour: take the first value in each hour. Standard convention for temperature, dewpoint, wind speed, direction, relative humidity, air pressure.\n",
    "    2. Summation across the hour: sum observations within each hour. Standard convention for precipitation and solar radiation.\n",
    "    3. Constant across the hour: take the first value in each hour. This applied to variables that do not change.\n",
    "    \"\"\"\n",
    "\n",
    "    # Variables that remain constant within each hour\n",
    "    constant_vars = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"elevation\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]\n",
    "\n",
    "    # Aggregation across hour variables, standard meteorological convention: precipitation and solar radiation\n",
    "    sum_vars = [\n",
    "        \"time\",\n",
    "        \"pr\",\n",
    "        \"pr_localmid\",\n",
    "        \"pr_24h\",\n",
    "        \"pr_1h\",\n",
    "        \"pr_15min\",\n",
    "        \"pr_5min\",\n",
    "        \"rsds\",\n",
    "    ]\n",
    "\n",
    "    # Top of the hour variables, standard meteorological convention: temperature, dewpoint temperature, pressure, humidity, winds\n",
    "    instant_vars = [\n",
    "        \"hurs_derived\",\n",
    "        \"time\",\n",
    "        \"tas\",\n",
    "        \"tas_derived\",\n",
    "        \"tdps\",\n",
    "        \"tdps_derived\",\n",
    "        \"ps\",\n",
    "        \"psl\",\n",
    "        \"ps_altimeter\",\n",
    "        \"ps_derived\",\n",
    "        \"hurs\",\n",
    "        \"sfcWind\",\n",
    "        \"sfcWind_dir\",\n",
    "    ]\n",
    "\n",
    "    # QAQC flags, which remain constants within each hour\n",
    "    qaqc_var_pieces = [\"qc\", \"eraqc\", \"duration\", \"method\", \"flag\", \"depth\", \"process\"]\n",
    "\n",
    "    try:\n",
    "\n",
    "        qaqc_vars = [\n",
    "            var for var in df.columns if any(item in var for item in qaqc_var_pieces)\n",
    "        ]\n",
    "\n",
    "        # Subset the dataframe according to rules\n",
    "        constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "        qaqc_df = df[[col for col in qaqc_vars if col in df.columns if col != \"time\"]]\n",
    "        qaqc_df = qaqc_df.astype(str)\n",
    "        qaqc_df.insert(0, \"time\", df[\"time\"])\n",
    "\n",
    "        sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "\n",
    "        instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "\n",
    "        # Performing hourly aggregation, only if subset contains more than one (ie more than the 'time' time) column\n",
    "        # This is to account for input dataframes that do not contain ALL subsets of variables defined above - just a subset of them.\n",
    "        result_list = []\n",
    "        if len(constant_df.columns) > 1:\n",
    "            constant_result = constant_df.resample(\"1h\", on=\"time\").first()\n",
    "            result_list.append(constant_result)\n",
    "\n",
    "        if len(instant_df.columns) > 1:\n",
    "            instant_result = instant_df.resample(\"1h\", on=\"time\").first()\n",
    "            result_list.append(instant_result)\n",
    "\n",
    "        if len(sum_df.columns) > 1:\n",
    "            sum_result = sum_df.resample(\"1h\", on=\"time\").apply(\n",
    "                lambda x: np.nan if x.isna().all() else x.sum(skipna=True)\n",
    "            )\n",
    "            result_list.append(sum_result)\n",
    "\n",
    "        if len(qaqc_df.columns) > 1:\n",
    "            qaqc_result = qaqc_df.resample(\"1h\", on=\"time\").apply(\n",
    "                lambda x: qaqc_flag_fcn(x)\n",
    "            )  # concatenating unique flags\n",
    "            result_list.append(qaqc_result)\n",
    "\n",
    "        # Aggregate and output reduced dataframe - this merges all dataframes defined\n",
    "        # This function sets \"time\" to the index; reset index to return to original index\n",
    "        result = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=[\"time\"], how=\"outer\"),\n",
    "            result_list,\n",
    "        )\n",
    "        result.reset_index(inplace=True)  # Convert time index --> column\n",
    "\n",
    "        # Infill constant values and flag rows added through resampling\n",
    "        result = _modify_infill(result, constant_vars)\n",
    "\n",
    "        # Update attributes for sub-hourly variables\n",
    "        sub_hourly_vars = [i for i in df.columns if \"min\" in i and \"qc\" not in i]\n",
    "        for var in sub_hourly_vars:\n",
    "            var_attrs[var][\"standardization\"] = (\n",
    "                \"{} has been standardized to an hourly timestep, but will retain its original name\".format(\n",
    "                    var\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return result, var_attrs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721348bd",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f21b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, var_attrs_test = merge_hourly_standardization(df, var_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86393b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to change this - this df will just used to find time gaps, \n",
    "# easier to view with fewer columns\n",
    "df_gaps = df[[\"time\", \"station\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48940109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time differences between consecutive rows\n",
    "# (I know I probably don't need to define that extra \"timestamp\" \n",
    "# column, but I was getting an error without it and didn't want \n",
    "# to investigate it when this is only for testing)\n",
    "\n",
    "df_gaps[\"timestamp\"] = pd.to_datetime(df_gaps[\"time\"])\n",
    "df_gaps[\"time_diff\"] = df_gaps[\"timestamp\"].diff()\n",
    "threshold = pd.Timedelta(minutes=60)\n",
    "gaps = df_gaps[df_gaps[\"time_diff\"] > threshold]\n",
    "\n",
    "gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20390435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filtert the standardized and original dataframes to a time gap\n",
    "df_time_filt_test = df_test.loc[\n",
    "    (df_test[\"time\"] >= \"1981-02-05 05:00:00\t\") & (df_test[\"time\"] < \"1981-02-05 10:00:00\t\")\n",
    "]\n",
    "\n",
    "df_time_filt = df.loc[\n",
    "    (df[\"time\"] >= \"1981-02-05 05:00:00\t\")\n",
    "    & (df[\"time\"] < \"1981-02-05 10:00:00\t\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_filt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562790b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_attrs_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
