{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Merge Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12/17/24\n",
    "\n",
    "#### Option 1\n",
    "\n",
    "break apart and treat each group uniquely (could be a good starting point)\n",
    "- get it to WORK! can always go back in the future to streamline, backlog\n",
    "\n",
    "#### Option 2\n",
    "make library, use custom resampler for static (use .first()) #TODO\n",
    "- good bc could modify library in future\n",
    "- include comment in code \n",
    "\n",
    "general suggestions\n",
    "\n",
    "\n",
    "include comment for each bloc, print statements in functions where necessary\n",
    "remember that this will go public eventually and publish manuscript PLUS automating\n",
    "process AND producing gridded product\n",
    "SO aim for scripts that area ready for this high visibility! \n",
    "clear doc strings, \"black formatting\" (way to organize script)\n",
    "future convos on standardization\n",
    "\n",
    "\n",
    "commend in the script for future Vanessa! \n",
    "err on overcommenting \n",
    "\n",
    "\n",
    "use notes to yourself in Slack!!!! to reflect on script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import datetime\n",
    "import math\n",
    "import shapely\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "import time # Used for progress bar \n",
    "import sys # Used for progress bar \n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) # Warning is raised when creating Point object from coords. Can't figure out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary \n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir): \n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
    "    \"\"\"\n",
    "    Print a progress bar to console \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "    \n",
    "    \"\"\"\n",
    "    count = len(it)\n",
    "    start = time.time() # time estimate start\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        # time estimate calculation and string\n",
    "        remaining = ((time.time() - start) / j) * (count - j)        \n",
    "        mins, sec = divmod(remaining, 60) # limited to minutes\n",
    "        time_str = f\"{int(mins):02}:{sec:03.1f}\"\n",
    "        print(f\"{prefix}[{u'█'*x}{('.'*(size-x))}] {j}/{count} Est wait {time_str}\", end='\\r', file=out, flush=True)\n",
    "    show(0.1) # avoid div/0 \n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    print(\"\\n\", flush=True, file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str \n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    station_data: xr.Dataset \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file \n",
    "    I'd like to see us use a zarr workflow if possible to avoid this. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir = temp_dir, \n",
    "        prefix = \"\", \n",
    "        suffix = \".nc\",\n",
    "        delete = True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system \n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = 's3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc'.format(network_name, station_id)\n",
    "\n",
    "    # Read in the data using xarray \n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine='h5netcdf').load()\n",
    "\n",
    "    # Close temporary file \n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation; \n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key,val in ds.variables.items():\n",
    "        if val.dtype==object:\n",
    "            if key=='station':\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "                \n",
    "    exclude_qaqc = [\"time\", \"station\", \"lat\", \"lon\", \n",
    "                    \"qaqc_process\", \"sfcWind_method\", \n",
    "                    \"pr_duration\", \"pr_depth\", \"PREC_flag\",\n",
    "                    \"rsds_duration\", \"rsds_flag\", \n",
    "                    \"anemometer_height_m\",\n",
    "                    \"thermometer_height_m\"\n",
    "                   ] # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = [] # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = [] # our ERA qc variable\n",
    "    old_era_qc_vars = [] # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if 'q_code' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variable, need to keep for comparison, then drop\n",
    "        if '_qc' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "        if '_eraqc' in var:\n",
    "            era_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\" # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var])*np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "    \n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars)-len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars)!=n_qc:    \n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "    \n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var:ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if 'anemometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['anemometer_height_m'] = np.ones(ds['time'].shape)*ds.anemometer_height_m\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df['anemometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if 'thermometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['thermometer_height_m'] = np.ones(ds['time'].shape)*ds.thermometer_height_m\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df['thermometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "           \n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df['station'] = station\n",
    "    \n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "    \n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "    df['day'] = pd.to_datetime(df['time']).dt.day \n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month \n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year \n",
    "    df['date']  = pd.to_datetime(df['time']).dt.date\n",
    "    \n",
    "    return df#, MultiIndex, attrs, var_attrs, era_qc_vars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitation Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and explore sample precipication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "['tas_eraqc', 'pr_5min_eraqc', 'pr_1h_eraqc', 'elevation_eraqc']\n",
      "0 created era_qc variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pr_5min', 'pr_5min_qc', 'pr_1h', 'pr_5min_eraqc', 'pr_1h_eraqc']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "#ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir) #cleaned, but no sub-hourly precip data\n",
    "ds = read_nc_from_s3('CRN', 'CRN_AGPC2', temp_dir) #contains sub-hourly precipitation data\n",
    "#convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n",
    "\n",
    "# check if precipittion data present\n",
    "all_pr_vars = [var for var in df.columns if 'pr' in var]\n",
    "all_pr_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break dataframe into four sub-dataframes\n",
    "\n",
    "1. static variables: this include variables like elevation and station name, which stay the same through time\n",
    "- take the first value to \"replace\" in the new time stamp\n",
    "2. time variables: \n",
    "- Only the \"time\" column will be modified, to be the top of the hour. All others (day, month, year, date) are constant within the hour.\n",
    "3. QC flags\n",
    "- start off with treating them like strings, and concatenanting, with a comma\n",
    "- There's an exception here: if the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "4. meteorological variables\n",
    "- These will be resampled. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupings of columns that fall into each category\n",
    "# TODO: enentually add all possible variables - the below is just a subset based on the CRN network\n",
    "\n",
    "constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\"]\n",
    "\n",
    "time_vars = [\"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                    \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",\"elevation_eraqc\"]\n",
    "\n",
    "met_vars = [\"time\",\"tas\", \"pr_5min\",\"pr_1h\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_df = df[constant_vars]\n",
    "#constant_df.columns\n",
    "\n",
    "time_df = df[time_vars] \n",
    "#time_df.columns\n",
    "\n",
    "qaqc_df = df[qaqc_vars]\n",
    "qaqc_df = qaqc_df[qaqc_vars].astype(str)\n",
    "#qaqc_df.columns\n",
    "\n",
    "met_df = df[met_vars] \n",
    "#met_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tas</th>\n",
       "      <th>pr_5min</th>\n",
       "      <th>pr_1h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-16 22:00:00</th>\n",
       "      <td>2255.260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-16 23:00:00</th>\n",
       "      <td>3377.180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 00:00:00</th>\n",
       "      <td>3360.760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 01:00:00</th>\n",
       "      <td>3332.630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 02:00:00</th>\n",
       "      <td>3313.460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 19:00:00</th>\n",
       "      <td>3656.360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 20:00:00</th>\n",
       "      <td>3675.440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 21:00:00</th>\n",
       "      <td>3671.684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 22:00:00</th>\n",
       "      <td>3649.745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 23:00:00</th>\n",
       "      <td>3603.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82922 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          tas  pr_5min  pr_1h\n",
       "time                                         \n",
       "2013-03-16 22:00:00  2255.260      0.0    0.0\n",
       "2013-03-16 23:00:00  3377.180      0.0    0.0\n",
       "2013-03-17 00:00:00  3360.760      0.0    0.0\n",
       "2013-03-17 01:00:00  3332.630      0.0    0.0\n",
       "2013-03-17 02:00:00  3313.460      0.0    0.0\n",
       "...                       ...      ...    ...\n",
       "2022-08-31 19:00:00  3656.360      0.0    0.0\n",
       "2022-08-31 20:00:00  3675.440      0.0    0.0\n",
       "2022-08-31 21:00:00  3671.684      0.0    0.0\n",
       "2022-08-31 22:00:00  3649.745      0.0    0.0\n",
       "2022-08-31 23:00:00  3603.667      0.0    0.0\n",
       "\n",
       "[82922 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resample meteorological variables \n",
    "met_result =  met_df.resample('1h',on='time').sum()\n",
    "met_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elevation</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-16 22:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-16 23:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 00:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 01:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 02:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 19:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 20:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 21:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 22:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 23:00:00</th>\n",
       "      <td>CRN_AGPC2</td>\n",
       "      <td>40.155</td>\n",
       "      <td>-103.14167</td>\n",
       "      <td>1383.792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82922 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       station     lat        lon  elevation  \\\n",
       "time                                                           \n",
       "2013-03-16 22:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2013-03-16 23:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2013-03-17 00:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2013-03-17 01:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2013-03-17 02:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "...                        ...     ...        ...        ...   \n",
       "2022-08-31 19:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2022-08-31 20:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2022-08-31 21:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2022-08-31 22:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "2022-08-31 23:00:00  CRN_AGPC2  40.155 -103.14167   1383.792   \n",
       "\n",
       "                     anemometer_height_m  thermometer_height_m  \n",
       "time                                                            \n",
       "2013-03-16 22:00:00                  NaN                   NaN  \n",
       "2013-03-16 23:00:00                  NaN                   NaN  \n",
       "2013-03-17 00:00:00                  NaN                   NaN  \n",
       "2013-03-17 01:00:00                  NaN                   NaN  \n",
       "2013-03-17 02:00:00                  NaN                   NaN  \n",
       "...                                  ...                   ...  \n",
       "2022-08-31 19:00:00                  NaN                   NaN  \n",
       "2022-08-31 20:00:00                  NaN                   NaN  \n",
       "2022-08-31 21:00:00                  NaN                   NaN  \n",
       "2022-08-31 22:00:00                  NaN                   NaN  \n",
       "2022-08-31 23:00:00                  NaN                   NaN  \n",
       "\n",
       "[82922 rows x 6 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the first value in each hour\n",
    "constant_result =  constant_df.resample('1h',on='time').first()\n",
    "constant_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qaqc_result \u001b[38;5;241m=\u001b[39m \u001b[43mqaqc_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;66;03m# .agg({qaqc_df: lambda x: ''.join(x)})\u001b[39;00m\n\u001b[1;32m      2\u001b[0m qaqc_result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/generic.py:9771\u001b[0m, in \u001b[0;36mNDFrame.resample\u001b[0;34m(self, rule, axis, closed, label, convention, kind, on, level, origin, offset, group_keys)\u001b[0m\n\u001b[1;32m   9768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   9769\u001b[0m     convention \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 9771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_resampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSeries | DataFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9776\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9781\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9782\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9784\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/resample.py:2050\u001b[0m, in \u001b[0;36mget_resampler\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;124;03mCreate a TimeGrouper and return our resampler.\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2049\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 2050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_resampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/resample.py:2272\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, TimedeltaIndex):\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaIndexResampler(\n\u001b[1;32m   2265\u001b[0m         obj,\n\u001b[1;32m   2266\u001b[0m         timegrouper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2269\u001b[0m         gpr_index\u001b[38;5;241m=\u001b[39max,\n\u001b[1;32m   2270\u001b[0m     )\n\u001b[0;32m-> 2272\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly valid with DatetimeIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2274\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ax)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"
     ]
    }
   ],
   "source": [
    "qaqc_result = qaqc_df.resample('1h',on='time').sum() # .agg({qaqc_df: lambda x: ''.join(x)})\n",
    "qaqc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2013-03-16 22:00:00                nannannannannannannannan\n",
       "2013-03-16 23:00:00    nannannannannannannannannannannannan\n",
       "2013-03-17 00:00:00    nannannannannannannannannannannannan\n",
       "2013-03-17 01:00:00    nannannannannannannannannannannannan\n",
       "2013-03-17 02:00:00    nannannannannannannannannannannannan\n",
       "                                       ...                 \n",
       "2022-08-31 19:00:00    nannannannannannannannannannannannan\n",
       "2022-08-31 20:00:00    nannannannannannannannannannannannan\n",
       "2022-08-31 21:00:00    nannannannannannannannannannannannan\n",
       "2022-08-31 22:00:00    nannannannannannannannannannannannan\n",
       "2022-08-31 23:00:00    nannannannannannannannannannannannan\n",
       "Freq: h, Name: tas_qc, Length: 82922, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_result[\"tas_qc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-16 22:00:00</th>\n",
       "      <td>22.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2013-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-16 23:00:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2013-03-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2013-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 01:00:00</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2013-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-17 02:00:00</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2013-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 19:00:00</th>\n",
       "      <td>19.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 20:00:00</th>\n",
       "      <td>20.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 21:00:00</th>\n",
       "      <td>21.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 22:00:00</th>\n",
       "      <td>22.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-31 23:00:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>2022-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82922 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hour   day  month    year        date\n",
       "time                                                      \n",
       "2013-03-16 22:00:00  22.0  16.0    3.0  2013.0  2013-03-16\n",
       "2013-03-16 23:00:00  23.0  16.0    3.0  2013.0  2013-03-16\n",
       "2013-03-17 00:00:00   0.0  17.0    3.0  2013.0  2013-03-17\n",
       "2013-03-17 01:00:00   1.0  17.0    3.0  2013.0  2013-03-17\n",
       "2013-03-17 02:00:00   2.0  17.0    3.0  2013.0  2013-03-17\n",
       "...                   ...   ...    ...     ...         ...\n",
       "2022-08-31 19:00:00  19.0  31.0    8.0  2022.0  2022-08-31\n",
       "2022-08-31 20:00:00  20.0  31.0    8.0  2022.0  2022-08-31\n",
       "2022-08-31 21:00:00  21.0  31.0    8.0  2022.0  2022-08-31\n",
       "2022-08-31 22:00:00  22.0  31.0    8.0  2022.0  2022-08-31\n",
       "2022-08-31 23:00:00  23.0  31.0    8.0  2022.0  2022-08-31\n",
       "\n",
       "[82922 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the first value in each hour\n",
    "time_result =  time_df.resample('1h',on='time').first()\n",
    "time_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precip_hourly_standardization(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Rules:\n",
    "    ------\n",
    "        1) pr_5min < pr_1h < pr_24h\n",
    "        2) pr_localmid should never exceed pr_24h\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "        df [pd.DataFrame]: station dataset converted to dataframe through QAQC pipeline\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "        if success:\n",
    "            df [pd.DataFrame]: QAQC dataframe with additional column \"pr_1hr\"\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running: precip_hourly_standardization\", flush=True)\n",
    "    \n",
    "    # identify which precipitation vars are reported \n",
    "    all_pr_vars = [var for var in df.columns if 'pr' in var] # can be variable length depending if there is a raw qc var\n",
    "    pr_vars = [var for var in all_pr_vars if not any(True for item in vars_to_remove if item in var)] # remove all qc variables so they do not also run through: raw, eraqc, qaqc_process\n",
    "    \n",
    "    # break dataframe into four sub-dataframes\n",
    "    # category 1: \n",
    "\n",
    "    # TODO: add to log file\n",
    "    try:        \n",
    "        # if station does not report any precipitation values, or only one, bypass\n",
    "        if len(pr_vars) == 0:\n",
    "            print('Station does not report precipitation variables - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_1h' in pr_vars and 'pr_5min' not in pr_vars:\n",
    "            print('Station already reports hourly precipitation - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_24h' in pr_vars and 'pr_5min' not in pr_vars:\n",
    "            print('Station reports daily precipitation - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_5min' in pr_vars and 'pr_24h' not in pr_vars and 'pr_1h' not in pr_vars:\n",
    "            print('attempting hourly aggregation', flush=True)\n",
    "            df = df.resample('1h',on='time').sum()\n",
    "            return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"precip_hourly_standardization failed with Exception: {0}\".format(e), flush=true)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: precip_hourly_standardization\n"
     ]
    }
   ],
   "source": [
    "result = precip_hourly_standardization(df)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agg_funcs = {\n",
    "#     col:'sum' if col in met_vars else continue\n",
    "#     for col in df.columns\n",
    "# }\n",
    "\n",
    "#df_resampled = df.resample('D').agg(agg_funcs)\n",
    "\n",
    "# result = df.resample('1h',on='time').add_funcs()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
    "    'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'value': [10, 15, 12, 18, 14, 20, 16, 22, 18, 24]\n",
    "})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Group by the variable and apply different resampling techniques\n",
    "resampled_df = df.groupby('category').resample('W').agg({\n",
    "    'value': {'A': 'mean', 'B': 'first'}\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
