{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Merge Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1\n",
    "Split the data into subsets, perform different resampling scheme for each, then combine.\n",
    "\n",
    "\n",
    "--> This is the strategy that this script currently utlizes\n",
    "\n",
    "### Option 2\n",
    "Make a library and use a custom resampler on the entire dataset\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "- Decide on whether we should use \"reduce\" from \"functools\" to combine the susbets, or use repeated merges. The second option involves more code, but the first requires an additional module to be used. Unclear which is computatinoally more intensive.\n",
    "\n",
    "\n",
    "- If the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "- Option 2 above could be used in the future\n",
    "\n",
    "- the function does not output the expected message when fed a dataset with no sub-hourly precipitation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the input dataset into four subsets and then perform resampling on each one between combining them into a single result.\n",
    "\n",
    "\n",
    "**1. constant variables**\n",
    "\n",
    "\n",
    "Includes: columns like elevation and station name, which stay the same through time\n",
    "\n",
    "\n",
    "Method: Take the first value to \"replace\" in the new time stamp. Use \"first()\" resampler.\n",
    "\n",
    "\n",
    "**2. time variables**\n",
    "\n",
    "\n",
    "Includes: any time-related columns, like date and year\n",
    "\n",
    "\n",
    "Method: Only the \"time\" column will be modified, to be the top of the hour. All others (day, month, year, date) are constant within each hour. Use \"first()\" resampler.\n",
    "\n",
    "\n",
    "**3. QA/QC flags**\n",
    "\n",
    "\n",
    "Method: start off with treating them like strings, and concatenanting, with a comma\n",
    "\n",
    "\n",
    "Notes: There's an exception here: if the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "**4. meteorological variables**\n",
    "\n",
    "\n",
    "Method: Precipitation is summed within each hour (using \".sum() resampler\"). This script currently applies to other meteorological variables as well, but this will change in the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: include this module as required module OR use merge instead\n",
    "from functools import reduce\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import datetime\n",
    "import math\n",
    "import shapely\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "import time # Used for progress bar \n",
    "import sys # Used for progress bar \n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) # Warning is raised when creating Point object from coords. Can't figure out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary \n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir): \n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
    "    \"\"\"\n",
    "    Print a progress bar to console \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "    \n",
    "    \"\"\"\n",
    "    count = len(it)\n",
    "    start = time.time() # time estimate start\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        # time estimate calculation and string\n",
    "        remaining = ((time.time() - start) / j) * (count - j)        \n",
    "        mins, sec = divmod(remaining, 60) # limited to minutes\n",
    "        time_str = f\"{int(mins):02}:{sec:03.1f}\"\n",
    "        print(f\"{prefix}[{u'â–ˆ'*x}{('.'*(size-x))}] {j}/{count} Est wait {time_str}\", end='\\r', file=out, flush=True)\n",
    "    show(0.1) # avoid div/0 \n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    print(\"\\n\", flush=True, file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str \n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    station_data: xr.Dataset \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file \n",
    "    I'd like to see us use a zarr workflow if possible to avoid this. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir = temp_dir, \n",
    "        prefix = \"\", \n",
    "        suffix = \".nc\",\n",
    "        delete = True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system \n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = 's3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc'.format(network_name, station_id)\n",
    "\n",
    "    # Read in the data using xarray \n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine='h5netcdf').load()\n",
    "\n",
    "    # Close temporary file \n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation; \n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key,val in ds.variables.items():\n",
    "        if val.dtype==object:\n",
    "            if key=='station':\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "                \n",
    "    exclude_qaqc = [\"time\", \"station\", \"lat\", \"lon\", \n",
    "                    \"qaqc_process\", \"sfcWind_method\", \n",
    "                    \"pr_duration\", \"pr_depth\", \"PREC_flag\",\n",
    "                    \"rsds_duration\", \"rsds_flag\", \n",
    "                    \"anemometer_height_m\",\n",
    "                    \"thermometer_height_m\"\n",
    "                   ] # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = [] # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = [] # our ERA qc variable\n",
    "    old_era_qc_vars = [] # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if 'q_code' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variable, need to keep for comparison, then drop\n",
    "        if '_qc' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "        if '_eraqc' in var:\n",
    "            era_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\" # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var])*np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "    \n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars)-len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars)!=n_qc:    \n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "    \n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var:ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if 'anemometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['anemometer_height_m'] = np.ones(ds['time'].shape)*ds.anemometer_height_m\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df['anemometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if 'thermometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['thermometer_height_m'] = np.ones(ds['time'].shape)*ds.thermometer_height_m\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df['thermometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "           \n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df['station'] = station\n",
    "    \n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "    \n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "    df['day'] = pd.to_datetime(df['time']).dt.day \n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month \n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year \n",
    "    df['date']  = pd.to_datetime(df['time']).dt.date\n",
    "    \n",
    "    return df#, MultiIndex, attrs, var_attrs, era_qc_vars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "['ps_eraqc', 'tas_eraqc', 'tdps_eraqc', 'pr_eraqc', 'sfcWind_eraqc', 'sfcWind_dir_eraqc', 'elevation_eraqc', 'ps_altimeter_eraqc', 'psl_eraqc']\n",
      "0 created era_qc variables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pr', 'qaqc_process', 'pr_qc', 'pr_duration', 'pr_depth_qc', 'pr_eraqc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3('CRN', 'CRN_AGPC2', temp_dir) \n",
    "#ds = read_nc_from_s3('CRN', 'CRN_CETC2', temp_dir) \n",
    "#ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir) \n",
    "\n",
    "#convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n",
    "\n",
    "# check if precipitation data present\n",
    "all_pr_vars = [var for var in df.columns if 'pr' in var]\n",
    "all_pr_vars\n",
    "\n",
    "\n",
    "# datasets that have been checked\n",
    "# - CRN_AGPC2\n",
    "# - CRN_CETC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'ps', 'tas', 'tdps', 'pr', 'sfcWind', 'sfcWind_dir',\n",
      "       'elevation', 'qaqc_process', 'ps_qc', 'ps_altimeter', 'ps_altimeter_qc',\n",
      "       'psl', 'psl_qc', 'tas_qc', 'tdps_qc', 'pr_qc', 'pr_duration',\n",
      "       'pr_depth_qc', 'sfcWind_qc', 'sfcWind_method', 'sfcWind_dir_qc', 'lat',\n",
      "       'lon', 'ps_eraqc', 'tas_eraqc', 'tdps_eraqc', 'pr_eraqc',\n",
      "       'sfcWind_eraqc', 'sfcWind_dir_eraqc', 'elevation_eraqc',\n",
      "       'ps_altimeter_eraqc', 'psl_eraqc', 'anemometer_height_m',\n",
      "       'thermometer_height_m', 'station', 'hour', 'day', 'month', 'year',\n",
      "       'date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#check the columns present in the dataset\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupings of columns that fall into each category\n",
    "# TODO: enentually add all possible variables - the below is just a subset based on the CRN network\n",
    "\n",
    "constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration']\n",
    "\n",
    "time_vars = [\"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "met_vars = [\"time\",\"tas\",\"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\",\n",
    "                \"rsds\",\"total\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'station', 'lat', 'lon', 'elevation', 'anemometer_height_m', 'thermometer_height_m', 'sfcWind_method', 'pr_duration', 'time', 'hour', 'day', 'month', 'year', 'date', 'tas_qc', 'tas_eraqc', 'pr_5min_eraqc', 'pr_1h_eraqc', 'pr_5min_qc', 'pr_eraqc', 'pr_depth_qc', 'ps_qc', 'ps_altimeter_qc', 'ps_eraqc', 'ps_altimeter_eraqc', 'psl_qc', 'psl_eraqc', 'tdps_qc', 'tdps_eraqc', 'sfcWind_qc', 'sfcWind_dir_qc', 'sfcWind_eraqc', 'sfcWind_dir_eraqcelevation_eraqc', 'qaqc_process', 'time', 'tas', 'pr', 'pr_localmid', 'pr_24h', 'pr_5min', 'pr_1h', 'tdps', 'tdps_derived', 'ps', 'psl', 'ps_altimeter', 'hurs', 'sfcwind', 'sfcwind_dir', 'rsds', 'total']\n"
     ]
    }
   ],
   "source": [
    "all_vars = constant_vars + time_vars + qaqc_vars + met_vars\n",
    "print(all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "#constant_df.columns\n",
    "\n",
    "time_df = df[[col for col in time_vars if col in df.columns]]\n",
    "#time_df.columns\n",
    "\n",
    "#TODO: this throws an warning - come back and address it\n",
    "\n",
    "qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "qaqc_vars_subset.remove(\"time\")\n",
    "qaqc_df[qaqc_vars_subset].astype(str)\n",
    "#qaqc_df.columns\n",
    "\n",
    "met_df = df[[col for col in met_vars if col in df.columns]]\n",
    "#met_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tas_qc', 'tas_eraqc', 'pr_eraqc', 'pr_depth_qc', 'ps_qc', 'ps_altimeter_qc', 'ps_eraqc', 'ps_altimeter_eraqc', 'psl_qc', 'psl_eraqc', 'tdps_qc', 'tdps_eraqc', 'sfcWind_qc', 'sfcWind_dir_qc', 'sfcWind_eraqc', 'qaqc_process']\n"
     ]
    }
   ],
   "source": [
    "print(qaqc_vars_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'tas_qc', 'tas_eraqc', 'pr_eraqc', 'pr_depth_qc', 'ps_qc',\n",
       "       'ps_altimeter_qc', 'ps_eraqc', 'ps_altimeter_eraqc', 'psl_qc',\n",
       "       'psl_eraqc', 'tdps_qc', 'tdps_eraqc', 'sfcWind_qc', 'sfcWind_dir_qc',\n",
       "       'sfcWind_eraqc', 'qaqc_process'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qaqc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         <class 'float'>\n",
      "1         <class 'float'>\n",
      "2         <class 'float'>\n",
      "3         <class 'float'>\n",
      "4         <class 'float'>\n",
      "               ...       \n",
      "464372    <class 'float'>\n",
      "464373    <class 'float'>\n",
      "464374    <class 'float'>\n",
      "464375    <class 'float'>\n",
      "464376    <class 'float'>\n",
      "Name: tas_eraqc, Length: 464377, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check column entry tyme, to confirm tha the type conversion for qaqc_df was successful\n",
    "qaqc_df[qaqc_vars_subset].astype(str)\n",
    "qaqc_df[\"tas_eraqc\"].astype(str)\n",
    "print(qaqc_df['tas_eraqc'].apply(type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# concatenate entries within each hour\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m qaqc_result \u001b[38;5;241m=\u001b[39m \u001b[43mqaqc_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(qaqc_result)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/resample.py:355\u001b[0m, in \u001b[0;36mResampler.aggregate\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     how \u001b[38;5;241m=\u001b[39m func\n\u001b[0;32m--> 355\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_groupby_and_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/resample.py:451\u001b[0m, in \u001b[0;36mResampler._groupby_and_aggregate\u001b[0;34m(self, how, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(how):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# TODO: test_resample_apply_with_additional_args fails if we go\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m#  through the non-lambda path, not clear that it should.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: how(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 451\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     result \u001b[38;5;241m=\u001b[39m grouped\u001b[38;5;241m.\u001b[39maggregate(how, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:1482\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m gba \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, [func], args\u001b[38;5;241m=\u001b[39m(), kwargs\u001b[38;5;241m=\u001b[39m{})\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1482\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgba\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/apply.py:193\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_dict_like()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[1;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mget_cython_func(func)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/apply.py:326\u001b[0m, in \u001b[0;36mApply.agg_list_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_list_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a list-like argument.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/apply.py:1571\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_list_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[38;5;66;03m# Only set as_index=True on groupby objects, not Window or Resample\u001b[39;00m\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;66;03m# that inherit from this class.\u001b[39;00m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1569\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1570\u001b[0m ):\n\u001b[0;32m-> 1571\u001b[0m     keys, results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_list_like(keys, results)\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/apply.py:385\u001b[0m, in \u001b[0;36mApply.compute_list_like\u001b[0;34m(self, op_name, selected_obj, kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m colg \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_gotitem(col, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, subset\u001b[38;5;241m=\u001b[39mselected_obj\u001b[38;5;241m.\u001b[39miloc[:, index])\n\u001b[1;32m    380\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    381\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs]\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_axis(op_name, colg)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m    384\u001b[0m )\n\u001b[0;32m--> 385\u001b[0m new_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(new_res)\n\u001b[1;32m    387\u001b[0m indices\u001b[38;5;241m.\u001b[39mappend(index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:257\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m    256\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[0;32m--> 257\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_multiple_funcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:362\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[1;32m    361\u001b[0m         key \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mOutputKey(label\u001b[38;5;241m=\u001b[39mname, position\u001b[38;5;241m=\u001b[39midx)\n\u001b[0;32m--> 362\u001b[0m         results[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:294\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:327\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[0;32m--> 327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    882\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/groupby/generic.py:324\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    322\u001b[0m     alias \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39m_builtin_table_alias[func]\n\u001b[1;32m    323\u001b[0m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[0;32m--> 324\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m    327\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(obj, f)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/resample.py:450\u001b[0m, in \u001b[0;36mResampler._groupby_and_aggregate.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(how):\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# TODO: test_resample_apply_with_additional_args fails if we go\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;66;03m#  through the non-lambda path, not clear that it should.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m         func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mhow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m         result \u001b[38;5;241m=\u001b[39m grouped\u001b[38;5;241m.\u001b[39maggregate(func)\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# concatenate entries within each hour\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m qaqc_result \u001b[38;5;241m=\u001b[39m qaqc_df\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m'\u001b[39m,on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(qaqc_result)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "# concatenate entries within each hour\n",
    "qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x))\n",
    "print(qaqc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        tas   pr    tdps        ps       psl  ps_altimeter\n",
      "time                                                                      \n",
      "1980-01-01 00:00:00  573.50  0.0  285.35  204060.0  204200.0      204200.0\n",
      "1980-01-01 01:00:00  287.55  0.0  285.35  204060.0  102100.0      204200.0\n",
      "1980-01-01 02:00:00  287.55  0.0  284.85  102030.0  102100.0      102100.0\n",
      "1980-01-01 03:00:00  285.95  0.0  284.25  102100.0  102170.0      102170.0\n",
      "1980-01-01 04:00:00  285.35  0.0  283.75  102130.0  102200.0      102200.0\n",
      "...                     ...  ...     ...       ...       ...           ...\n",
      "2022-08-31 19:00:00  294.25  0.0  285.35  101500.0  101560.0      101560.0\n",
      "2022-08-31 20:00:00  294.25  0.0  285.35  101460.0  101520.0      101520.0\n",
      "2022-08-31 21:00:00  294.85  0.0  285.35  101400.0  101450.0      101460.0\n",
      "2022-08-31 22:00:00  293.75  0.0  285.95  101330.0  101390.0      101390.0\n",
      "2022-08-31 23:00:00  292.55  0.0  285.35  101290.0  101340.0      101350.0\n",
      "\n",
      "[374016 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# resample meteorological variables \n",
    "met_result =  met_df.resample('1h',on='time').sum().rename(columns={'pr_5min': 'pr_1hr_test'})\n",
    "print(met_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  station     lat      lon  elevation  \\\n",
      "time                                                                    \n",
      "1980-01-01 00:00:00  ASOSAWOS_72494023234  37.617 -122.383        2.0   \n",
      "1980-01-01 01:00:00  ASOSAWOS_72494023234  37.617 -122.383        3.0   \n",
      "1980-01-01 02:00:00  ASOSAWOS_72494023234  37.617 -122.383        3.0   \n",
      "1980-01-01 03:00:00  ASOSAWOS_72494023234  37.617 -122.383        2.0   \n",
      "1980-01-01 04:00:00  ASOSAWOS_72494023234  37.617 -122.383        3.0   \n",
      "...                                   ...     ...      ...        ...   \n",
      "2022-08-31 19:00:00  ASOSAWOS_72494023234  37.620 -122.365        5.0   \n",
      "2022-08-31 20:00:00  ASOSAWOS_72494023234  37.620 -122.365        5.0   \n",
      "2022-08-31 21:00:00  ASOSAWOS_72494023234  37.620 -122.365        5.0   \n",
      "2022-08-31 22:00:00  ASOSAWOS_72494023234  37.620 -122.365        5.0   \n",
      "2022-08-31 23:00:00  ASOSAWOS_72494023234  37.620 -122.365        5.0   \n",
      "\n",
      "                     anemometer_height_m  thermometer_height_m sfcWind_method  \\\n",
      "time                                                                            \n",
      "1980-01-01 00:00:00                10.06                   NaN              N   \n",
      "1980-01-01 01:00:00                10.06                   NaN              9   \n",
      "1980-01-01 02:00:00                10.06                   NaN              N   \n",
      "1980-01-01 03:00:00                10.06                   NaN              N   \n",
      "1980-01-01 04:00:00                10.06                   NaN              N   \n",
      "...                                  ...                   ...            ...   \n",
      "2022-08-31 19:00:00                10.06                   NaN              N   \n",
      "2022-08-31 20:00:00                10.06                   NaN              N   \n",
      "2022-08-31 21:00:00                10.06                   NaN              N   \n",
      "2022-08-31 22:00:00                10.06                   NaN              N   \n",
      "2022-08-31 23:00:00                10.06                   NaN              N   \n",
      "\n",
      "                        pr_duration  \n",
      "time                                 \n",
      "1980-01-01 00:00:00 0 days 01:00:00  \n",
      "1980-01-01 01:00:00 0 days 01:00:00  \n",
      "1980-01-01 02:00:00 0 days 01:00:00  \n",
      "1980-01-01 03:00:00 0 days 01:00:00  \n",
      "1980-01-01 04:00:00 0 days 01:00:00  \n",
      "...                             ...  \n",
      "2022-08-31 19:00:00 0 days 01:00:00  \n",
      "2022-08-31 20:00:00 0 days 01:00:00  \n",
      "2022-08-31 21:00:00 0 days 01:00:00  \n",
      "2022-08-31 22:00:00 0 days 01:00:00  \n",
      "2022-08-31 23:00:00 0 days 01:00:00  \n",
      "\n",
      "[374016 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# keep the first value in each hour\n",
    "constant_result =  constant_df.resample('1h',on='time').first()\n",
    "print(constant_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first value in each hour\n",
    "time_result =  time_df.resample('1h',on='time').first()\n",
    "print(time_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_list = [met_result,constant_result,time_result,qaqc_result]\n",
    "\n",
    "result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "                                            how='outer'), result_list)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative method for combinding the subsets that does not require a new module\n",
    "result_test = met_result.merge(constant_result, on='time', how='outer').merge(time_result, on='time', how='outer').merge(qaqc_result, on='time', how='outer')\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hourly Resampling Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precip_hourly_standardization(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Rules:\n",
    "    ------\n",
    "        1) pr_5min < pr_1h < pr_24h\n",
    "        2) pr_localmid should never exceed pr_24h\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "        df [pd.DataFrame]: station dataset converted to dataframe through QAQC pipeline\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "        if success:\n",
    "            df [pd.DataFrame]: QAQC dataframe with additional column \"pr_1hr\"\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running: precip_hourly_standardization\", flush=True)\n",
    "\n",
    "    ##### define the variables for each sub-dataframe #####\n",
    "    constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration']\n",
    "\n",
    "    time_vars = [\"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "    qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "    met_vars = [\"time\",\"tas\",\"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\",\n",
    "                \"rsds\",\"total\"]\n",
    "    \n",
    "    ##### identify which precipitation vars are reported  #####\n",
    "    all_pr_vars = [var for var in df.columns if 'pr' in var] # can be variable length depending if there is a raw qc var\n",
    "    #pr_vars = [var for var in all_pr_vars if not any(True for item in vars_to_remove if item in var)] # remove all qc variables so they do not also run through: raw, eraqc, qaqc_process\n",
    "    \n",
    "    ##### subset the dataframe\n",
    "    print(\"generating subsets\", flush=True)\n",
    "\n",
    "    constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "    time_df = df[[col for col in time_vars if col in df.columns]]\n",
    "\n",
    "    qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "    qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "    qaqc_vars_subset.remove(\"time\")\n",
    "    qaqc_df[qaqc_vars_subset].astype(str)\n",
    "\n",
    "    met_df = df[[col for col in met_vars if col in df.columns]]\n",
    "    \n",
    "    ##### \n",
    "    print(\"checking if dataset contains sub-hourly precipitation data\", flush=True)\n",
    "    # TODO: add to log file\n",
    "    try:        \n",
    "        # if station does not report any precipitation values, or only one, bypass\n",
    "        if len(all_pr_vars) == 0:\n",
    "            print('Station does not report precipitation variables - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_1h' in all_pr_vars and 'pr_5min' not in all_pr_vars:\n",
    "            print('Station already reports hourly precipitation - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_24h' in all_pr_vars and 'pr_5min' not in all_pr_vars:\n",
    "            print('Station reports daily precipitation - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        if 'pr_5min' in all_pr_vars: #and 'pr_24h' not in pr_vars and 'pr_1h' not in pr_vars:\n",
    "            print('performing hourly aggregation', flush=True)\n",
    "            \n",
    "            #perform resampling for each subset separately\n",
    "            met_result =  met_df.resample('1h',on='time').sum().rename(columns={'pr_5min': 'pr_1hr_test'}) #renames the subhourly precipitation column\n",
    "            constant_result =  constant_df.resample('1h',on='time').first()\n",
    "            time_result =  time_df.resample('1h',on='time').first()\n",
    "            qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x))\n",
    "\n",
    "            #merge all subsets on \"time\" column\n",
    "            result_list = [met_result,constant_result,time_result,qaqc_result]\n",
    "            #result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "            #                                how='outer'), result_list)\n",
    "            result = met_result.merge(constant_result, on='time', how='outer').merge(time_result, on='time', how='outer').merge(qaqc_result, on='time', how='outer')\n",
    "            return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"precip_hourly_standardization failed with Exception: {0}\".format(e), flush=true)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: precip_hourly_standardization\n",
      "generating subsets\n",
      "checking if dataset contains sub-hourly precipitation data\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = precip_hourly_standardization(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agg_funcs = {\n",
    "#     col:'sum' if col in met_vars else continue\n",
    "#     for col in df.columns\n",
    "# }\n",
    "\n",
    "#df_resampled = df.resample('D').agg(agg_funcs)\n",
    "\n",
    "# result = df.resample('1h',on='time').add_funcs()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
    "    'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'value': [10, 15, 12, 18, 14, 20, 16, 22, 18, 24]\n",
    "})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Group by the variable and apply different resampling techniques\n",
    "resampled_df = df.groupby('category').resample('W').agg({\n",
    "    'value': {'A': 'mean', 'B': 'first'}\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
