{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Merge Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Outline\n",
    "\n",
    "1. **Methdology**\n",
    "\n",
    "2. **Setup**: Change the intput dataset under \"Load data\"\n",
    "\n",
    "3. **Development section**: For developing parts of the final funciton and troubleshooting.\n",
    "\n",
    "4. **Final Script**: You can skip to this section to run the final function.\n",
    "\n",
    "5. **Code Sandbox**: Contains scraps of code that may be useful for future development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "- Decide on whether we should use \"reduce\" from \"functools\" to combine the susbets, or use repeated merges. The second option involves more code, but the first requires an additional module to be used. Unclear which is computatinoally more intensive.\n",
    "\n",
    "\n",
    "- If the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "- Could, in the future, make a library and use a custom resampler on the entire dataset, rather than splitting it inter subsets and performing resampling separately on each (as is done currently)\n",
    "\n",
    "- control for case in which the input dataset does not contain one of the subsets of variabes\n",
    "\n",
    "- rename resampled columnes\n",
    "\n",
    "- address the following warning: \n",
    "\n",
    "\n",
    "        SettingWithCopyWarning: \n",
    "        A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "        Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the input dataset into four subsets and then perform resampling on each one between combining them into a single result.\n",
    "\n",
    "\n",
    "**1. constant variables**\n",
    "\n",
    "\n",
    "<ins>Includes</ins>: columns like elevation and station name, which stay the same through time AND time variables that are the same within each hour\n",
    "\n",
    "\n",
    "<ins>Method</ins>: Take the first value to \"replace\" in the new time stamp. Use \"first()\" resampler. Only the \"time\" column will be modified, to be the top of the hour. All others (day, month, year, date) are constant within each hour. Use \"first()\" resampler.\n",
    "\n",
    "\n",
    "\n",
    "**2. QA/QC flags**\n",
    "\n",
    "\n",
    "<ins>Method</ins>: start off with treating them like strings, and concatenanting, with a comma\n",
    "\n",
    "\n",
    "<ins>Notes</ins>: There's an exception here: if the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "**3. summed variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to sum across the hour\n",
    "- precipitation\n",
    "- solar radiation\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".sum() resampler\"). \n",
    "\n",
    "\n",
    "**4. instantaneous variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to take one instantaneous value, find observation closest to top of the hour\n",
    "- temperature\n",
    "- dewpoint\n",
    "- wind speed\n",
    "- direction\n",
    "- relative humidity\n",
    "- air pressure\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".first() resampler\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: could use this as an alternative to multiple merges, for combining the subsets into the final output dataframe\n",
    "from functools import reduce\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import datetime\n",
    "import math\n",
    "import shapely\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "import time # Used for progress bar \n",
    "import sys # Used for progress bar \n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) # Warning is raised when creating Point object from coords. Can't figure out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary \n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir): \n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str \n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    station_data: xr.Dataset \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file \n",
    "    I'd like to see us use a zarr workflow if possible to avoid this. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir = temp_dir, \n",
    "        prefix = \"\", \n",
    "        suffix = \".nc\",\n",
    "        delete = True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system \n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = 's3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc'.format(network_name, station_id)\n",
    "\n",
    "    # Read in the data using xarray \n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine='h5netcdf').load()\n",
    "\n",
    "    # Close temporary file \n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation; \n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key,val in ds.variables.items():\n",
    "        if val.dtype==object:\n",
    "            if key=='station':\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "                \n",
    "    exclude_qaqc = [\"time\", \"station\", \"lat\", \"lon\", \n",
    "                    \"qaqc_process\", \"sfcWind_method\", \n",
    "                    \"pr_duration\", \"pr_depth\", \"PREC_flag\",\n",
    "                    \"rsds_duration\", \"rsds_flag\", \n",
    "                    \"anemometer_height_m\",\n",
    "                    \"thermometer_height_m\"\n",
    "                   ] # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = [] # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = [] # our ERA qc variable\n",
    "    old_era_qc_vars = [] # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if 'q_code' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variable, need to keep for comparison, then drop\n",
    "        if '_qc' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "        if '_eraqc' in var:\n",
    "            era_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\" # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var])*np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "    \n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars)-len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars)!=n_qc:    \n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "    \n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var:ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if 'anemometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['anemometer_height_m'] = np.ones(ds['time'].shape)*ds.anemometer_height_m\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df['anemometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if 'thermometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['thermometer_height_m'] = np.ones(ds['time'].shape)*ds.thermometer_height_m\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df['thermometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "           \n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df['station'] = station\n",
    "    \n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "    \n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "    df['day'] = pd.to_datetime(df['time']).dt.day \n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month \n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year \n",
    "    df['date']  = pd.to_datetime(df['time']).dt.date\n",
    "    \n",
    "    return df#, MultiIndex, attrs, var_attrs, era_qc_vars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "['tas_eraqc', 'pr_5min_eraqc', 'pr_1h_eraqc', 'elevation_eraqc']\n",
      "0 created era_qc variables\n"
     ]
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3('CRN', 'CRN_AGPC2', temp_dir) \n",
    "#ds = read_nc_from_s3('CRN', 'CRN_CETC2', temp_dir) \n",
    "#ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir) \n",
    "\n",
    "#convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n",
    "\n",
    "# check if precipitation data present\n",
    "all_pr_vars = [var for var in df.columns if 'pr' in var]\n",
    "#print(all_pr_vars)\n",
    "#print(df.columns)\n",
    "\n",
    "# datasets that have been checked\n",
    "# - CRN_AGPC2\n",
    "# - CRN_CETC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupings of columns that fall into each category\n",
    "\n",
    "constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration',\n",
    "                    \"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "#precipitatino and solar radiation\n",
    "sum_vars = [\"time\",\"tas\",\n",
    "                \"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"rsds\"]\n",
    "\n",
    "#emperature, dewpoint, wind speed, wind direction, relative humidity, air pressure\n",
    "instant_vars = [\"time\",\n",
    "                \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\",\n",
    "                \"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = constant_vars + qaqc_vars + sum_vars + instant_vars\n",
    "print(all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/kbcbbl557b96fgdc4pc5k8mh0000gn/T/ipykernel_5619/2391426794.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into four subsets\n",
    "\n",
    "constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "#print(constant_df.columns)\n",
    "\n",
    "#TODO: this seems clunky, but was made so to avoid a warning I kept getting if I did the following:\n",
    "# qaqc_df[[col for col in qaqc_vars if col in df.columns]] = qaqc_df[[col for col in qaqc_vars if col in df.columns]].astype(str)\n",
    "# where \"qaqc_vars\" does NOT contain \"time\"\n",
    "\n",
    "qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "qaqc_vars_subset.remove(\"time\")\n",
    "qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n",
    "#print(qaqc_df.columns)\n",
    "\n",
    "sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "#print(sum_df.columns)\n",
    "\n",
    "instant_df = df[[col for col in instant_vars if col in df.columns]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          tas  pr_5min  pr_1h\n",
      "time                                         \n",
      "2013-03-16 22:00:00  2255.260      0.0    0.0\n",
      "2013-03-16 23:00:00  3377.180      0.0    0.0\n",
      "2013-03-17 00:00:00  3360.760      0.0    0.0\n",
      "2013-03-17 01:00:00  3332.630      0.0    0.0\n",
      "2013-03-17 02:00:00  3313.460      0.0    0.0\n",
      "...                       ...      ...    ...\n",
      "2022-08-31 19:00:00  3656.360      0.0    0.0\n",
      "2022-08-31 20:00:00  3675.440      0.0    0.0\n",
      "2022-08-31 21:00:00  3671.684      0.0    0.0\n",
      "2022-08-31 22:00:00  3649.745      0.0    0.0\n",
      "2022-08-31 23:00:00  3603.667      0.0    0.0\n",
      "\n",
      "[82922 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# sum across hur aggregation\n",
    "\n",
    "sum_result =  sum_df.resample('1h',on='time').sum()#.rename(columns={'pr_5min': 'pr_1hr_test'})\n",
    "print(sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     nobs_tas_hourstd  nobs_pr_5min_hourstd  \\\n",
      "time                                                          \n",
      "2013-03-16 22:00:00                 8                     0   \n",
      "2013-03-16 23:00:00                12                     0   \n",
      "2013-03-17 00:00:00                12                     0   \n",
      "2013-03-17 01:00:00                12                     0   \n",
      "2013-03-17 02:00:00                12                     0   \n",
      "...                               ...                   ...   \n",
      "2022-08-31 19:00:00                12                    12   \n",
      "2022-08-31 20:00:00                12                    12   \n",
      "2022-08-31 21:00:00                12                    12   \n",
      "2022-08-31 22:00:00                12                    12   \n",
      "2022-08-31 23:00:00                12                     5   \n",
      "\n",
      "                     nobs_pr_1h_hourstd  \n",
      "time                                     \n",
      "2013-03-16 22:00:00                   0  \n",
      "2013-03-16 23:00:00                   0  \n",
      "2013-03-17 00:00:00                   0  \n",
      "2013-03-17 01:00:00                   0  \n",
      "2013-03-17 02:00:00                   0  \n",
      "...                                 ...  \n",
      "2022-08-31 19:00:00                   0  \n",
      "2022-08-31 20:00:00                   1  \n",
      "2022-08-31 21:00:00                   1  \n",
      "2022-08-31 22:00:00                   1  \n",
      "2022-08-31 23:00:00                   1  \n",
      "\n",
      "[82922 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sum_result_counts =  sum_df.resample('1h',on='time').count()\n",
    "sum_result_counts.columns = sum_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "print(sum_result_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantaneous aggregation\n",
    "\n",
    "instant_result =  instant_df.resample('1h',on='time').first()\n",
    "print(instant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    tas_qc tas_eraqc pr_5min_eraqc pr_1h_eraqc pr_5min_qc\n",
      "time                                                                     \n",
      "2013-03-16 22:00:00    nan       nan          16.0         nan        nan\n",
      "2013-03-16 23:00:00    nan       nan          16.0         nan        nan\n",
      "2013-03-17 00:00:00    nan       nan          16.0         nan        nan\n",
      "2013-03-17 01:00:00    nan       nan          16.0         nan        nan\n",
      "2013-03-17 02:00:00    nan       nan          16.0         nan        nan\n",
      "...                    ...       ...           ...         ...        ...\n",
      "2022-08-31 19:00:00    nan       nan          16.0         nan        nan\n",
      "2022-08-31 20:00:00    nan       nan          16.0         nan        nan\n",
      "2022-08-31 21:00:00    nan       nan          16.0         nan        nan\n",
      "2022-08-31 22:00:00    nan       nan          16.0         nan        nan\n",
      "2022-08-31 23:00:00    nan       nan          16.0         nan        nan\n",
      "\n",
      "[82922 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# concatenate qaqc flags within each hour\n",
    "# takes 1.5 minutes to run!\n",
    "qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x.unique()))\n",
    "print(qaqc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first value in each hour\n",
    "constant_result =  constant_df.resample('1h',on='time').first()\n",
    "print(constant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_list = [sum_result,instant_result,constant_result,qaqc_result]\n",
    "result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "                                            how='outer'), result_list)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative method for combinding the subsets that does not require a new module\n",
    "result_merge = sum_result.merge(instant_result, on='time', how='outer').merge(constant_result, on='time', how='outer').merge(qaqc_result, on='time', how='outer')\n",
    "print(result_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_standardization(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Rules:\n",
    "    ------\n",
    "        1) pr_5min < pr_1h < pr_24h\n",
    "        2) pr_localmid should never exceed pr_24h\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "        df [pd.DataFrame]: station dataset converted to dataframe through QAQC pipeline\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "        if success:\n",
    "            df [pd.DataFrame]: QAQC dataframe with all columns resampled to one hour\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running: hourly_standardization\", flush=True)\n",
    "\n",
    "    ##### define the variables for each sub-dataframe #####\n",
    "    constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration',\n",
    "                    \"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "    qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "    #precipitatino and solar radiation\n",
    "    sum_vars = [\"time\",\"tas\",\n",
    "                \"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"rsds\"]\n",
    "\n",
    "    #temperature, dewpoint, wind speed, wind direction, relative humidity, air pressure\n",
    "    instant_vars = [\"time\",\n",
    "                \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\",\n",
    "                \"total\"]\n",
    "    \n",
    "    ##### subset the dataframe\n",
    "    print(\"generating subsets\", flush=True)\n",
    "\n",
    "    constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "    qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "    qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "    qaqc_vars_subset.remove(\"time\")\n",
    "    qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n",
    "\n",
    "    sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "\n",
    "    instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "    \n",
    "    ##### \n",
    "    print(\"checking if dataset contains sub-hourly precipitation data\", flush=True)\n",
    "    # TODO: add to log file\n",
    "    try:        \n",
    "        # if station does not report any precipitation values, or only one, bypass\n",
    "        if len(df.columns) == 0:\n",
    "            print('Empty dataset - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        else: \n",
    "            print('performing hourly aggregation', flush=True)\n",
    "            constant_result =  constant_df.resample('1h',on='time').first()\n",
    "            instant_result =  instant_df.resample('1h',on='time').first()\n",
    "            sum_result =  sum_df.resample('1h',on='time').sum()\n",
    "            qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x.unique()))\n",
    "\n",
    "            print('generating variable counts per hour', flush=True)\n",
    "            constant_result_counts =  constant_df.resample('1h',on='time').count()\n",
    "            constant_result_counts.columns = constant_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "\n",
    "            instant_result_counts =  instant_df.resample('1h',on='time').count()\n",
    "            instant_result_counts.columns = instant_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "\n",
    "            sum_result_counts =  sum_df.resample('1h',on='time').count()\n",
    "            sum_result_counts.columns = sum_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "\n",
    "            qaqc_result_counts =  qaqc_df.resample('1h',on='time').count()\n",
    "            qaqc_result_counts.columns = qaqc_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "\n",
    "            print('producing final result', flush=True)\n",
    "            result_list = [sum_result,instant_result,constant_result,qaqc_result,sum_result_counts,instant_result_counts,constant_result_counts,qaqc_result_counts]\n",
    "            result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "                                            how='outer'), result_list)\n",
    "            return result\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"precip_hourly_standardization failed with Exception: {0}\".format(e), flush=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: hourly_standardization\n",
      "generating subsets\n",
      "checking if dataset contains sub-hourly precipitation data\n",
      "performing hourly aggregation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/kbcbbl557b96fgdc4pc5k8mh0000gn/T/ipykernel_5619/110947639.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating variable counts per hour\n",
      "producing final result\n",
      "                          tas  pr_5min  pr_1h    station     lat        lon  \\\n",
      "time                                                                          \n",
      "2013-03-16 22:00:00  2255.260      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-16 23:00:00  3377.180      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 00:00:00  3360.760      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 01:00:00  3332.630      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 02:00:00  3313.460      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "...                       ...      ...    ...        ...     ...        ...   \n",
      "2022-08-31 19:00:00  3656.360      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 20:00:00  3675.440      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 21:00:00  3671.684      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 22:00:00  3649.745      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 23:00:00  3603.667      0.0    0.0  CRN_AGPC2  40.155 -103.14167   \n",
      "\n",
      "                     elevation  anemometer_height_m  thermometer_height_m  \\\n",
      "time                                                                        \n",
      "2013-03-16 22:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-16 23:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 00:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 01:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 02:00:00   1383.792                  NaN                   NaN   \n",
      "...                        ...                  ...                   ...   \n",
      "2022-08-31 19:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 20:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 21:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 22:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 23:00:00   1383.792                  NaN                   NaN   \n",
      "\n",
      "                     hour  ...  nobs_hour_hourstd  nobs_day_hourstd  \\\n",
      "time                       ...                                        \n",
      "2013-03-16 22:00:00  22.0  ...                  8                 8   \n",
      "2013-03-16 23:00:00  23.0  ...                 12                12   \n",
      "2013-03-17 00:00:00   0.0  ...                 12                12   \n",
      "2013-03-17 01:00:00   1.0  ...                 12                12   \n",
      "2013-03-17 02:00:00   2.0  ...                 12                12   \n",
      "...                   ...  ...                ...               ...   \n",
      "2022-08-31 19:00:00  19.0  ...                 12                12   \n",
      "2022-08-31 20:00:00  20.0  ...                 12                12   \n",
      "2022-08-31 21:00:00  21.0  ...                 12                12   \n",
      "2022-08-31 22:00:00  22.0  ...                 12                12   \n",
      "2022-08-31 23:00:00  23.0  ...                 12                12   \n",
      "\n",
      "                     nobs_month_hourstd nobs_year_hourstd nobs_date_hourstd  \\\n",
      "time                                                                          \n",
      "2013-03-16 22:00:00                   8                 8                 8   \n",
      "2013-03-16 23:00:00                  12                12                12   \n",
      "2013-03-17 00:00:00                  12                12                12   \n",
      "2013-03-17 01:00:00                  12                12                12   \n",
      "2013-03-17 02:00:00                  12                12                12   \n",
      "...                                 ...               ...               ...   \n",
      "2022-08-31 19:00:00                  12                12                12   \n",
      "2022-08-31 20:00:00                  12                12                12   \n",
      "2022-08-31 21:00:00                  12                12                12   \n",
      "2022-08-31 22:00:00                  12                12                12   \n",
      "2022-08-31 23:00:00                  12                12                12   \n",
      "\n",
      "                    nobs_tas_qc_hourstd nobs_tas_eraqc_hourstd  \\\n",
      "time                                                             \n",
      "2013-03-16 22:00:00                   8                      8   \n",
      "2013-03-16 23:00:00                  12                     12   \n",
      "2013-03-17 00:00:00                  12                     12   \n",
      "2013-03-17 01:00:00                  12                     12   \n",
      "2013-03-17 02:00:00                  12                     12   \n",
      "...                                 ...                    ...   \n",
      "2022-08-31 19:00:00                  12                     12   \n",
      "2022-08-31 20:00:00                  12                     12   \n",
      "2022-08-31 21:00:00                  12                     12   \n",
      "2022-08-31 22:00:00                  12                     12   \n",
      "2022-08-31 23:00:00                  12                     12   \n",
      "\n",
      "                    nobs_pr_5min_eraqc_hourstd nobs_pr_1h_eraqc_hourstd  \\\n",
      "time                                                                      \n",
      "2013-03-16 22:00:00                          8                        8   \n",
      "2013-03-16 23:00:00                         12                       12   \n",
      "2013-03-17 00:00:00                         12                       12   \n",
      "2013-03-17 01:00:00                         12                       12   \n",
      "2013-03-17 02:00:00                         12                       12   \n",
      "...                                        ...                      ...   \n",
      "2022-08-31 19:00:00                         12                       12   \n",
      "2022-08-31 20:00:00                         12                       12   \n",
      "2022-08-31 21:00:00                         12                       12   \n",
      "2022-08-31 22:00:00                         12                       12   \n",
      "2022-08-31 23:00:00                         12                       12   \n",
      "\n",
      "                     nobs_pr_5min_qc_hourstd  \n",
      "time                                          \n",
      "2013-03-16 22:00:00                        8  \n",
      "2013-03-16 23:00:00                       12  \n",
      "2013-03-17 00:00:00                       12  \n",
      "2013-03-17 01:00:00                       12  \n",
      "2013-03-17 02:00:00                       12  \n",
      "...                                      ...  \n",
      "2022-08-31 19:00:00                       12  \n",
      "2022-08-31 20:00:00                       12  \n",
      "2022-08-31 21:00:00                       12  \n",
      "2022-08-31 22:00:00                       12  \n",
      "2022-08-31 23:00:00                       12  \n",
      "\n",
      "[82922 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "result = hourly_standardization(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tas', 'pr_5min', 'pr_1h', 'station', 'lat', 'lon', 'elevation',\n",
      "       'anemometer_height_m', 'thermometer_height_m', 'hour', 'day', 'month',\n",
      "       'year', 'date', 'tas_qc', 'tas_eraqc', 'pr_5min_eraqc', 'pr_1h_eraqc',\n",
      "       'pr_5min_qc', 'nobs_tas_hourstd', 'nobs_pr_5min_hourstd',\n",
      "       'nobs_pr_1h_hourstd', 'nobs_station_hourstd', 'nobs_lat_hourstd',\n",
      "       'nobs_lon_hourstd', 'nobs_elevation_hourstd',\n",
      "       'nobs_anemometer_height_m_hourstd', 'nobs_thermometer_height_m_hourstd',\n",
      "       'nobs_hour_hourstd', 'nobs_day_hourstd', 'nobs_month_hourstd',\n",
      "       'nobs_year_hourstd', 'nobs_date_hourstd', 'nobs_tas_qc_hourstd',\n",
      "       'nobs_tas_eraqc_hourstd', 'nobs_pr_5min_eraqc_hourstd',\n",
      "       'nobs_pr_1h_eraqc_hourstd', 'nobs_pr_5min_qc_hourstd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agg_funcs = {\n",
    "#     col:'sum' if col in met_vars else continue\n",
    "#     for col in df.columns\n",
    "# }\n",
    "\n",
    "#df_resampled = df.resample('D').agg(agg_funcs)\n",
    "\n",
    "# result = df.resample('1h',on='time').add_funcs()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try turning one particular column into a string\n",
    "qaqc_df[\"tas_eraqc\"] = qaqc_df[\"tas_eraqc\"].astype(str)\n",
    "# qaqc_df[\"tas_eraqc\"].astype(str) #this does NOT work\n",
    "print(qaqc_df['tas_eraqc'].apply(type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
    "    'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'value': [10, 15, 12, 18, 14, 20, 16, 22, 18, 24]\n",
    "})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Group by the variable and apply different resampling techniques\n",
    "resampled_df = df.groupby('category').resample('W').agg({\n",
    "    'value': {'A': 'mean', 'B': 'first'}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##### identify which precipitation vars are reported  #####\n",
    "    # all_pr_vars = [var for var in df.columns if 'pr' in var] # can be variable length depending if there is a raw qc var\n",
    "    #pr_vars = [var for var in all_pr_vars if not any(True for item in vars_to_remove if item in var)] # remove all qc variables so they do not also run through: raw, eraqc, qaqc_process\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
