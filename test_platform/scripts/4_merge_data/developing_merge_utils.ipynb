{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Merge Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Outline\n",
    "\n",
    "1. **Methdology**\n",
    "\n",
    "2. **Setup**: Change the intput dataset under \"Load data\"\n",
    "\n",
    "3. **Development section**: For developing parts of the final funciton and troubleshooting.\n",
    "\n",
    "4. **Final Script**: You can skip to this section to run the final function.\n",
    "\n",
    "5. **Code Sandbox**: Contains scraps of code that may be useful for future development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "- If the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "- Could, in the future, make a library and use a custom resampler on the entire dataset, rather than splitting it inter subsets and performing resampling separately on each (as is done currently)\n",
    "\n",
    "- The pr_5min and pr_1h resampled data do not match, and we expect they should. Further investigation shows that the pr_5min is qaqc flagged. Suggessful merge may, then require ommitting flagged values in the future. See line 121 in https://github.com/Eagle-Rock-Analytics/historical-obs-platform/blob/main/test_platform/scripts/3_qaqc_data/qaqc_climatological_outlier.py for an example of this. We could implement this function in the future\n",
    "\n",
    "\n",
    "- The hourly standardization function does not HAVE to be in the utils script, dependong on how execution is carried out for the merge step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the input dataset into four subsets and then perform resampling on each one between combining them into a single result.\n",
    "\n",
    "\n",
    "**1. constant variables**\n",
    "\n",
    "\n",
    "<ins>Includes</ins>: columns like elevation and station name, which stay the same through time AND time variables that are the same within each hour\n",
    "\n",
    "\n",
    "<ins>Method</ins>: Take the first value to \"replace\" in the new time stamp. Use \"first()\" resampler. Only the \"time\" column will be modified, to be the top of the hour. All others (day, month, year, date) are constant within each hour. Use \"first()\" resampler.\n",
    "\n",
    "\n",
    "\n",
    "**2. QA/QC flags**\n",
    "\n",
    "\n",
    "<ins>Method</ins>: start off with treating them like strings, and concatenanting, with a comma\n",
    "\n",
    "\n",
    "<ins>Notes</ins>: There's an exception here: if the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "**3. summed variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to sum across the hour\n",
    "- precipitation\n",
    "- solar radiation\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".sum() resampler\"). \n",
    "\n",
    "\n",
    "**4. instantaneous variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to take one instantaneous value, find observation closest to top of the hour\n",
    "- temperature\n",
    "- dewpoint\n",
    "- wind speed\n",
    "- direction\n",
    "- relative humidity\n",
    "- air pressure\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".first() resampler\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: could use this as an alternative to multiple merges, for combining the subsets into the final output dataframe\n",
    "from functools import reduce\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = (\n",
    "    \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    ")\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_log_file_merge(file):\n",
    "    global log_file\n",
    "    log_file = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printf(*args, verbose=True, log_file=None, **kwargs):\n",
    "    import datetime\n",
    "\n",
    "    tLog = lambda: datetime.datetime.utcnow().strftime(\"%m-%d-%Y %H:%M:%S\") + \" : \\t\"\n",
    "    args = [str(a) for a in args]\n",
    "\n",
    "    if verbose:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs) or print(\n",
    "                \" \".join([tLog(), *args]), file=log_file, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs)\n",
    "    else:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), file=log_file, **kwargs)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/s3fs/core.py:112\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/aiobotocore/client.py:358\u001b[0m, in \u001b[0;36mAioBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    357\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load in single dc file from AWS\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mread_nc_from_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRN_AGPC2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ds = read_nc_from_s3('CRN', 'CRN_CETC2', temp_dir)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# convert to formatted pandas dataframe\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m qaqc_ds_to_df(ds, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mread_nc_from_s3\u001b[0;34m(network_name, station_id, temp_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m s3_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://wecc-historical-wx/3_qaqc_wx_dev/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     33\u001b[0m     network_name, station_id\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Read in the data using xarray\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m s3_file_obj \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m station_data \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(temp_file\u001b[38;5;241m.\u001b[39mname, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5netcdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Close temporary file\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/fsspec/asyn.py:113\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/fsspec/asyn.py:98\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/fsspec/asyn.py:53\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     51\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     55\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/fsspec/asyn.py:561\u001b[0m, in \u001b[0;36mAsyncFileSystem._get\u001b[0;34m(self, rpath, lpath, recursive, callback, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     callback\u001b[38;5;241m.\u001b[39mbranch(rpath, lpath, kwargs)\n\u001b[1;32m    560\u001b[0m     coros\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file(rpath, lpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _run_coros_in_chunks(\n\u001b[1;32m    562\u001b[0m     coros, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, callback\u001b[38;5;241m=\u001b[39mcallback\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/fsspec/asyn.py:269\u001b[0m, in \u001b[0;36m_run_coros_in_chunks\u001b[0;34m(coros, batch_size, callback, timeout, return_exceptions, nofiles)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_CALLBACK:\n\u001b[1;32m    264\u001b[0m         [\n\u001b[1;32m    265\u001b[0m             t\u001b[38;5;241m.\u001b[39madd_done_callback(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m__: callback\u001b[38;5;241m.\u001b[39mrelative_update(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chunk\n\u001b[1;32m    267\u001b[0m         ]\n\u001b[1;32m    268\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mchunk, return_exceptions\u001b[38;5;241m=\u001b[39mreturn_exceptions),\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    405\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    411\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/s3fs/core.py:1134\u001b[0m, in \u001b[0;36mS3FileSystem._get_file\u001b[0;34m(self, rpath, lpath, callback, version_id)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_s3(\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1127\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw,\n\u001b[1;32m   1131\u001b[0m     )\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m], resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1134\u001b[0m body, content_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _open_file(\u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1135\u001b[0m callback\u001b[38;5;241m.\u001b[39mset_size(content_length)\n\u001b[1;32m   1137\u001b[0m failed_reads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/s3fs/core.py:1125\u001b[0m, in \u001b[0;36mS3FileSystem._get_file.<locals>._open_file\u001b[0;34m(range)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mrange\u001b[39m:\n\u001b[1;32m   1124\u001b[0m     kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrange\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1125\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_s3(\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1127\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m   1128\u001b[0m     Key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mversion_id_kw(version_id \u001b[38;5;129;01mor\u001b[39;00m vers),\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw,\n\u001b[1;32m   1131\u001b[0m )\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m], resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/s3fs/core.py:339\u001b[0m, in \u001b[0;36mS3FileSystem._call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALL: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, akwarglist, kw2)\n\u001b[1;32m    338\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_s3_method_kwargs(method, \u001b[38;5;241m*\u001b[39makwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _error_wrapper(\n\u001b[1;32m    340\u001b[0m     method, kwargs\u001b[38;5;241m=\u001b[39madditional_kwargs, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries\n\u001b[1;32m    341\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/s3fs/core.py:139\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    137\u001b[0m         err \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    138\u001b[0m err \u001b[38;5;241m=\u001b[39m translate_boto_error(err)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3(\"CRN\", \"CRN_AGPC2\", temp_dir)\n",
    "# ds = read_nc_from_s3('CRN', 'CRN_CETC2', temp_dir)\n",
    "# ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir)\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n",
    "\n",
    "# check if precipitation data present\n",
    "all_pr_vars = [var for var in df.columns if \"pr\" in var]\n",
    "# print(all_pr_vars)\n",
    "# print(df.columns)\n",
    "\n",
    "# datasets that have been checked\n",
    "# - CRN_AGPC2\n",
    "# - CRN_CETC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupings of columns that fall into each category\n",
    "\n",
    "constant_vars = [\n",
    "    \"time\",\n",
    "    \"station\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"elevation\",\n",
    "    \"anemometer_height_m\",\n",
    "    \"thermometer_height_m\",\n",
    "    \"sfcWind_method\",\n",
    "    \"pr_duration\",\n",
    "    \"hour\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "    \"date\",\n",
    "]\n",
    "\n",
    "qaqc_vars = [\n",
    "    \"tas_qc\",\n",
    "    \"tas_eraqc\",\n",
    "    \"pr_5min_eraqc\",\n",
    "    \"pr_1h_eraqc\",\n",
    "    \"pr_5min_qc\",\n",
    "    \"pr_eraqc\",\n",
    "    \"pr_depth_qc\",\n",
    "    \"ps_qc\",\n",
    "    \"ps_altimeter_qc\",\n",
    "    \"ps_eraqc\",\n",
    "    \"ps_altimeter_eraqc\",\n",
    "    \"psl_qc\",\n",
    "    \"psl_eraqc\",\n",
    "    \"tdps_qc\",\n",
    "    \"tdps_eraqc\",\n",
    "    \"sfcWind_qc\",\n",
    "    \"sfcWind_dir_qc\",\n",
    "    \"sfcWind_eraqc\",\n",
    "    \"sfcWind_dir_eraqc\" \"elevation_eraqc\",\n",
    "    \"qaqc_process\",\n",
    "]\n",
    "\n",
    "# precipitatino and solar radiation\n",
    "sum_vars = [\"time\", \"tas\", \"pr\", \"pr_localmid\", \"pr_24h\", \"pr_5min\", \"pr_1h\", \"rsds\"]\n",
    "\n",
    "# emperature, dewpoint, wind speed, wind direction, relative humidity, air pressure\n",
    "instant_vars = [\n",
    "    \"time\",\n",
    "    \"tdps\",\n",
    "    \"tdps_derived\",\n",
    "    \"ps\",\n",
    "    \"psl\",\n",
    "    \"ps_altimeter\",\n",
    "    \"hurs\",\n",
    "    \"sfcwind\",\n",
    "    \"sfcwind_dir\",\n",
    "    \"total\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/cr06mz0n3bjd5jr12_9v6n200000gn/T/ipykernel_3983/1871337678.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into four subsets\n",
    "\n",
    "constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "constant_df.name = \"constant_df\"\n",
    "# print(constant_df.columns)\n",
    "\n",
    "# ensure that qaqc flags are all set as strings, but not \"time\" column\n",
    "qaqc_df = df[[col for col in qaqc_vars if col in df.columns if col != \"time\"]]\n",
    "qaqc_df = qaqc_df.astype(str)\n",
    "qaqc_df.insert(0, \"time\", df[\"time\"])\n",
    "\n",
    "# print(qaqc_df.columns)\n",
    "\n",
    "sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "sum_df.name = \"sum_df\"\n",
    "# print(sum_df.columns)\n",
    "\n",
    "instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "instant_df.name = \"instant_df\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_result = sum_df.resample(\"1h\", on=\"time\").apply(\n",
    "    lambda x: np.nan if x.isna().all() else x.sum(skipna=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the pr_5min and pr_1h data do not match up. This may result from\n",
    "# qaqc issues in the pr_5min data, which indeed contains qaqc flags\n",
    "\n",
    "sum_result.plot(y=[\"pr_5min\", \"pr_1h\"])\n",
    "\n",
    "diff = sum_result[\"pr_5min\"] - sum_result[\"pr_1h\"]\n",
    "diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_result_counts = sum_df.resample(\"1h\", on=\"time\").count()\n",
    "# sum_result_counts.columns = sum_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "print(sum_result_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantaneous aggregation\n",
    "\n",
    "instant_result = instant_df.resample(\"1h\", on=\"time\").first()\n",
    "print(instant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate qaqc flags within each hour\n",
    "# takes 1.5 minutes to run!\n",
    "qaqc_result = qaqc_df.resample(\"1h\", on=\"time\").apply(lambda x: \",\".join(x.unique()))\n",
    "print(qaqc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first value in each hour\n",
    "constant_result = constant_df.resample(\"1h\", on=\"time\").first()\n",
    "print(constant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = [sum_result, instant_result, constant_result, qaqc_result]\n",
    "result = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on=[\"time\"], how=\"outer\"), result_list\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative method for combinding the subsets that does not require a new module\n",
    "result_merge = (\n",
    "    sum_result.merge(instant_result, on=\"time\", how=\"outer\")\n",
    "    .merge(constant_result, on=\"time\", how=\"outer\")\n",
    "    .merge(qaqc_result, on=\"time\", how=\"outer\")\n",
    ")\n",
    "print(result_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_standardization(df):\n",
    "    \"\"\"\n",
    "\n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) top of the hour: take the first value in each hour\n",
    "            - standard convention for temperature, dewpoint, wind speed, direction, relative humidity, air pressure\n",
    "        2.) summation across hour: sum observations within each hour\n",
    "            - standard convention for precipitation and solar radiation\n",
    "        3.) constant across the hour: take the first value in each hour\n",
    "            - this applies to variables, like station name and location, that do not change within each hour\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame\n",
    "            station dataset converted to dataframe through QAQC pipeline\n",
    "        verbose: boolean\n",
    "            input for printf() to print to log file - set in script initialization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            df [pd.DataFrame]\n",
    "                QAQC dataframe with all columns resampled to one hour (column name retained)\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    printf(\n",
    "        \"Running: hourly_standardization\",\n",
    "        # verbose=verbose,\n",
    "        # log_file=log_file,\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    ##### define the variables for each sub-dataframe #####\n",
    "\n",
    "    # Variables that remain constant within each hour\n",
    "    constant_vars = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"elevation\", \n",
    "        \"anemometer_height_m\", \n",
    "        \"thermometer_height_m\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\"\n",
    "    ]\n",
    "\n",
    "    # Aggregation across hour variables, standard meteorological convention: precipitation and solar radiation\n",
    "    sum_vars = [\"time\", 'pr_15min', \"pr\", \"pr_localmid\", \"pr_24h\", \"pr_5min\", \"pr_1h\", \"rsds\"]\n",
    "\n",
    "    # Top of the hour variables, standard meteorological convention: temperature, dewpoint temperature, pressure, humidity, winds\n",
    "    instant_vars = [\n",
    "        \"time\",\n",
    "        \"tas\",\n",
    "        \"tdps\",\n",
    "        \"tdps_derived\",\n",
    "        \"ps\",\n",
    "        \"psl\",\n",
    "        \"ps_altimeter\",\n",
    "        \"ps_derived\",\n",
    "        \"hurs\",\n",
    "        \"sfcwind\",\n",
    "        \"sfcwind_dir\",\n",
    "    ]\n",
    "\n",
    "    # QAQC flags, which remain constants within each hour\n",
    "    # NOTE: Unlike the lists above, this list does not contain 'time', which is added in the next step. \n",
    "    qaqc_vars = [\n",
    "        'elevation_eraqc', \n",
    "        \"tas_qc\",\n",
    "        \"tas_eraqc\",\n",
    "        \"pr_5min_eraqc\",\n",
    "        'pr_15min_eraqc', \n",
    "        \"pr_1h_eraqc\",\n",
    "        \"pr_5min_qc\",\n",
    "        \"pr_eraqc\",\n",
    "        \"pr_depth_qc\",\n",
    "        \"ps_qc\",\n",
    "        \"ps_altimeter_qc\",\n",
    "        \"ps_eraqc\",\n",
    "        \"ps_altimeter_eraqc\",\n",
    "        \"psl_qc\",\n",
    "        \"psl_eraqc\",\n",
    "        \"tdps_qc\",\n",
    "        \"tdps_eraqc\",\n",
    "        'raw_qc', \n",
    "        \"sfcWind_qc\",\n",
    "        \"sfcWind_dir_qc\",\n",
    "        \"sfcWind_eraqc\",\n",
    "        \"sfcWind_dir_eraqc\",\n",
    "        \"qaqc_process\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    # All variables, necessary for producing columns with hourly counts for each variable\n",
    "    # all_vars = constant_vars + sum_vars + instant_vars + qaqc_vars\n",
    "\n",
    "    ##### Subset the dataframe according to rules\n",
    "    constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "    qaqc_df = df[[col for col in qaqc_vars if col in df.columns if col != \"time\"]]\n",
    "    qaqc_df = qaqc_df.astype(str)\n",
    "    qaqc_df.insert(0, \"time\", df[\"time\"])\n",
    "\n",
    "    sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "\n",
    "    instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "\n",
    "    #####\n",
    "\n",
    "    try:\n",
    "        # If station does not report any variable, bypass\n",
    "        if len(df.columns) == 0:\n",
    "            printf(\n",
    "                \"Empty dataset - bypassing hourly aggregation\",\n",
    "                # verbose=verbose,\n",
    "                # log_file=log_file,\n",
    "                flush=True,\n",
    "            )\n",
    "            return df\n",
    "        else:\n",
    "            result_list = []\n",
    "\n",
    "            # Performing hourly aggregation, only if subset contains more than one (ie 'time') column\n",
    "            ## This is to account for input dataframes that do not contain all subsets of variables defined above.\n",
    "            if len(constant_df.columns) <= 1:\n",
    "                print('')\n",
    "                \n",
    "            else:\n",
    "                constant_result = constant_df.resample(\"1h\", on=\"time\").first()\n",
    "                result_list.append(constant_result)\n",
    "                print('appended constant result')\n",
    "\n",
    "            if len(instant_df.columns) <= 1:\n",
    "                print('')\n",
    "            else:\n",
    "                instant_result = instant_df.resample(\"1h\", on=\"time\").first()\n",
    "                result_list.append(instant_result)\n",
    "\n",
    "            if len(sum_df.columns) <= 1:\n",
    "                print('')\n",
    "                \n",
    "            else:\n",
    "                sum_result = sum_df.resample(\"1h\", on=\"time\").apply(\n",
    "                    lambda x: np.nan if x.isna().all() else x.sum(skipna=True)\n",
    "                )\n",
    "                result_list.append(sum_result)\n",
    "                print('appended sum result')\n",
    "\n",
    "            if len(qaqc_df.columns) <= 1:\n",
    "                print('')\n",
    "                \n",
    "            else:\n",
    "                qaqc_result = qaqc_df.resample(\"1h\", on=\"time\").apply(\n",
    "                    lambda x: \",\".join(x.unique())\n",
    "                )  # adding unique flags\n",
    "                result_list.append(qaqc_result)\n",
    "                print('appended qaqc result')\n",
    "\n",
    "            # Aggregating and outputting reduced dataframe\n",
    "            result = reduce(\n",
    "                lambda left, right: pd.merge(left, right, on=[\"time\"], how=\"outer\"),\n",
    "                result_list,\n",
    "            )\n",
    "            return result\n",
    "\n",
    "    except Exception as e:\n",
    "        printf(\n",
    "            \"hourly_standardization failed with Exception: {0}\".format(e),\n",
    "            # verbose=verbose,\n",
    "            # log_file=log_file,\n",
    "            flush=True,\n",
    "        )\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Valley Water stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "['elevation_eraqc', 'pr_15min_eraqc']\n",
      "0 created era_qc variables\n"
     ]
    }
   ],
   "source": [
    "# Get URL to netcdf in S3\n",
    "s3_url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_6001.zarr\"\n",
    "\n",
    "ds = xr.open_zarr(s3_url)\n",
    "# convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = hourly_standardization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elevation</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "      <th>pr_15min</th>\n",
       "      <th>elevation_eraqc</th>\n",
       "      <th>pr_15min_eraqc</th>\n",
       "      <th>raw_qc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1985-02-07 20:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-02-07 21:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-02-07 22:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-02-07 23:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-02-08 00:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-09 19:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Prelimin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-09 20:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Prelimin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-09 21:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Prelimin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-09 22:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Prelimin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-09 23:00:00</th>\n",
       "      <td>VALLEYWATER_6001</td>\n",
       "      <td>37.2471</td>\n",
       "      <td>-121.871</td>\n",
       "      <td>58.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>Prelimin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349948 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              station      lat      lon  elevation  \\\n",
       "time                                                                 \n",
       "1985-02-07 20:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "1985-02-07 21:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "1985-02-07 22:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "1985-02-07 23:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "1985-02-08 00:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "...                               ...      ...      ...        ...   \n",
       "2025-01-09 19:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "2025-01-09 20:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "2025-01-09 21:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "2025-01-09 22:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "2025-01-09 23:00:00  VALLEYWATER_6001  37.2471 -121.871      58.73   \n",
       "\n",
       "                     anemometer_height_m  thermometer_height_m  pr_15min  \\\n",
       "time                                                                       \n",
       "1985-02-07 20:00:00                  NaN                   NaN       0.0   \n",
       "1985-02-07 21:00:00                  NaN                   NaN       0.0   \n",
       "1985-02-07 22:00:00                  NaN                   NaN       0.0   \n",
       "1985-02-07 23:00:00                  NaN                   NaN       0.0   \n",
       "1985-02-08 00:00:00                  NaN                   NaN       0.0   \n",
       "...                                  ...                   ...       ...   \n",
       "2025-01-09 19:00:00                  NaN                   NaN       0.0   \n",
       "2025-01-09 20:00:00                  NaN                   NaN       0.0   \n",
       "2025-01-09 21:00:00                  NaN                   NaN       0.0   \n",
       "2025-01-09 22:00:00                  NaN                   NaN       0.0   \n",
       "2025-01-09 23:00:00                  NaN                   NaN       0.0   \n",
       "\n",
       "                    elevation_eraqc pr_15min_eraqc    raw_qc  \n",
       "time                                                          \n",
       "1985-02-07 20:00:00             3.0            nan  Approved  \n",
       "1985-02-07 21:00:00             3.0            nan  Approved  \n",
       "1985-02-07 22:00:00             3.0            nan  Approved  \n",
       "1985-02-07 23:00:00             3.0            nan  Approved  \n",
       "1985-02-08 00:00:00             3.0            nan  Approved  \n",
       "...                             ...            ...       ...  \n",
       "2025-01-09 19:00:00             3.0            nan  Prelimin  \n",
       "2025-01-09 20:00:00             3.0            nan  Prelimin  \n",
       "2025-01-09 21:00:00             3.0            nan  Prelimin  \n",
       "2025-01-09 22:00:00             3.0            nan  Prelimin  \n",
       "2025-01-09 23:00:00             3.0            nan  Prelimin  \n",
       "\n",
       "[349948 rows x 10 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg_funcs = {\n",
    "#     col:'sum' if col in met_vars else continue\n",
    "#     for col in df.columns\n",
    "# }\n",
    "\n",
    "# df_resampled = df.resample('D').agg(agg_funcs)\n",
    "\n",
    "# result = df.resample('1h',on='time').add_funcs()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try turning one particular column into a string\n",
    "qaqc_df[\"tas_eraqc\"] = qaqc_df[\"tas_eraqc\"].astype(str)\n",
    "# qaqc_df[\"tas_eraqc\"].astype(str) #this does NOT work\n",
    "print(qaqc_df[\"tas_eraqc\"].apply(type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2023-01-01\", periods=10, freq=\"D\"),\n",
    "        \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n",
    "        \"value\": [10, 15, 12, 18, 14, 20, 16, 22, 18, 24],\n",
    "    }\n",
    ")\n",
    "df.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Group by the variable and apply different resampling techniques\n",
    "resampled_df = (\n",
    "    df.groupby(\"category\").resample(\"W\").agg({\"value\": {\"A\": \"mean\", \"B\": \"first\"}})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### identify which precipitation vars are reported  #####\n",
    "# all_pr_vars = [var for var in df.columns if 'pr' in var] # can be variable length depending if there is a raw qc var\n",
    "# pr_vars = [var for var in all_pr_vars if not any(True for item in vars_to_remove if item in var)] # remove all qc variables so they do not also run through: raw, eraqc, qaqc_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_sum(x):\n",
    "    output = np.nan if x.isna().all() else x.sum()\n",
    "    return output\n",
    "\n",
    "\n",
    "# Use transform instead of agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resampler_sum(df, period, time_col):\n",
    "    groups = df.resample(period, on=time_col)\n",
    "    sums = [x[1].sum(skipna=False) for x in groups]\n",
    "    return pd.DataFrame(sums, groups.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resample(x):\n",
    "    if np.all(np.isnan(x)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if x in sum_vars:\n",
    "            return np.sum(x)\n",
    "        if x in instant_vars:\n",
    "            return np.first()\n",
    "        if x in qaqc_vars:\n",
    "            return np.apply(lambda x: \",\".join(x.unique()))\n",
    "        if x in constant_vars:\n",
    "            return np.first()\n",
    "        else:\n",
    "            printf(\n",
    "                \"custom resampling not successful\",\n",
    "                verbose=true,\n",
    "                log_file=log_file,\n",
    "                flush=True,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
