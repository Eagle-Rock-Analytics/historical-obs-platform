{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Merge Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Outline\n",
    "\n",
    "1. **Methdology**\n",
    "\n",
    "2. **Setup**: Change the intput dataset under \"Load data\"\n",
    "\n",
    "3. **Development section**: For developing parts of the final funciton and troubleshooting.\n",
    "\n",
    "4. **Final Script**: You can skip to this section to run the final function.\n",
    "\n",
    "5. **Code Sandbox**: Contains scraps of code that may be useful for future development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "- Decide on whether we should use \"reduce\" from \"functools\" to combine the susbets, or use repeated merges. The second option involves more code, but the first requires an additional module to be used. Unclear which is computatinoally more intensive.\n",
    "\n",
    "\n",
    "- If the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "- Could, in the future, make a library and use a custom resampler on the entire dataset, rather than splitting it inter subsets and performing resampling separately on each (as is done currently)\n",
    "\n",
    "- address the following warning: \n",
    "\n",
    "\n",
    "        SettingWithCopyWarning: \n",
    "        A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "        Try using .loc[row_indexer,col_indexer] = value instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break the input dataset into four subsets and then perform resampling on each one between combining them into a single result.\n",
    "\n",
    "\n",
    "**1. constant variables**\n",
    "\n",
    "\n",
    "<ins>Includes</ins>: columns like elevation and station name, which stay the same through time AND time variables that are the same within each hour\n",
    "\n",
    "\n",
    "<ins>Method</ins>: Take the first value to \"replace\" in the new time stamp. Use \"first()\" resampler. Only the \"time\" column will be modified, to be the top of the hour. All others (day, month, year, date) are constant within each hour. Use \"first()\" resampler.\n",
    "\n",
    "\n",
    "\n",
    "**2. QA/QC flags**\n",
    "\n",
    "\n",
    "<ins>Method</ins>: start off with treating them like strings, and concatenanting, with a comma\n",
    "\n",
    "\n",
    "<ins>Notes</ins>: There's an exception here: if the variable is flagged by _eraqc it should be ignored prior to standardization. So for example if precip at :01 is flagged, but precip at :06 is not, the obs at :06 is the one kept etc. We potentially might need to think on if there are qaqc flags that \"are okay to keep\" and one's that are not.\n",
    "\n",
    "\n",
    "**3. summed variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to sum across the hour\n",
    "- precipitation\n",
    "- solar radiation\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".sum() resampler\"). \n",
    "\n",
    "\n",
    "**4. instantaneous variables**\n",
    "\n",
    "<ins>Includes</ins>: variables where standard is to take one instantaneous value, find observation closest to top of the hour\n",
    "- temperature\n",
    "- dewpoint\n",
    "- wind speed\n",
    "- direction\n",
    "- relative humidity\n",
    "- air pressure\n",
    "\n",
    "<ins>Method</ins>: summed within each hour (using \".first() resampler\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: could use this as an alternative to multiple merges, for combining the subsets into the final output dataframe\n",
    "from functools import reduce\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) # Warning is raised when creating Point object from coords. Can't figure out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client('s3') # for lower-level processes\n",
    "\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary \n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir): \n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_log_file_merge(file):\n",
    "    global log_file\n",
    "    log_file = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str \n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    station_data: xr.Dataset \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file \n",
    "    I'd like to see us use a zarr workflow if possible to avoid this. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir = temp_dir, \n",
    "        prefix = \"\", \n",
    "        suffix = \".nc\",\n",
    "        delete = True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system \n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = 's3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc'.format(network_name, station_id)\n",
    "\n",
    "    # Read in the data using xarray \n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine='h5netcdf').load()\n",
    "\n",
    "    # Close temporary file \n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation; \n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key,val in ds.variables.items():\n",
    "        if val.dtype==object:\n",
    "            if key=='station':\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "                \n",
    "    exclude_qaqc = [\"time\", \"station\", \"lat\", \"lon\", \n",
    "                    \"qaqc_process\", \"sfcWind_method\", \n",
    "                    \"pr_duration\", \"pr_depth\", \"PREC_flag\",\n",
    "                    \"rsds_duration\", \"rsds_flag\", \n",
    "                    \"anemometer_height_m\",\n",
    "                    \"thermometer_height_m\"\n",
    "                   ] # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = [] # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = [] # our ERA qc variable\n",
    "    old_era_qc_vars = [] # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if 'q_code' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variable, need to keep for comparison, then drop\n",
    "        if '_qc' in var: \n",
    "            raw_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "        if '_eraqc' in var:\n",
    "            era_qc_vars.append(var) # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\" # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var])*np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "    \n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars)-len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars)!=n_qc:    \n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "    \n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var:ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if 'anemometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['anemometer_height_m'] = np.ones(ds['time'].shape)*ds.anemometer_height_m\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df['anemometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if 'thermometer_height_m' not in df.columns:\n",
    "        try:\n",
    "            df['thermometer_height_m'] = np.ones(ds['time'].shape)*ds.thermometer_height_m\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df['thermometer_height_m'] = np.ones(len(df))*np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "           \n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df['station'] = station\n",
    "    \n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "    \n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df['hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "    df['day'] = pd.to_datetime(df['time']).dt.day \n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month \n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year \n",
    "    df['date']  = pd.to_datetime(df['time']).dt.date\n",
    "    \n",
    "    return df#, MultiIndex, attrs, var_attrs, era_qc_vars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printf(*args, verbose=True, log_file=None, **kwargs):\n",
    "    import datetime\n",
    "    \n",
    "    tLog = lambda : datetime.datetime.utcnow().strftime(\"%m-%d-%Y %H:%M:%S\") + \" : \\t\"\n",
    "    args = [str(a) for a in args]\n",
    "    \n",
    "    if verbose:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs) or \\\n",
    "            print(\" \".join([tLog(),*args]), file=log_file, **kwargs)\n",
    "        else:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs)   \n",
    "    else:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), file=log_file, **kwargs)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "['tas_eraqc', 'pr_5min_eraqc', 'pr_1h_eraqc', 'elevation_eraqc']\n",
      "0 created era_qc variables\n"
     ]
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3('CRN', 'CRN_AGPC2', temp_dir) \n",
    "#ds = read_nc_from_s3('CRN', 'CRN_CETC2', temp_dir) \n",
    "#ds = read_nc_from_s3('ASOSAWOS', 'ASOSAWOS_72494023234', temp_dir) \n",
    "\n",
    "#convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n",
    "\n",
    "# check if precipitation data present\n",
    "all_pr_vars = [var for var in df.columns if 'pr' in var]\n",
    "#print(all_pr_vars)\n",
    "#print(df.columns)\n",
    "\n",
    "# datasets that have been checked\n",
    "# - CRN_AGPC2\n",
    "# - CRN_CETC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break down into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupings of columns that fall into each category\n",
    "\n",
    "constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration',\n",
    "                    \"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "#precipitatino and solar radiation\n",
    "sum_vars = [\"time\",\"tas\",\n",
    "                \"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"rsds\"]\n",
    "\n",
    "#emperature, dewpoint, wind speed, wind direction, relative humidity, air pressure\n",
    "instant_vars = [\"time\",\n",
    "                \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\",\n",
    "                \"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/kbcbbl557b96fgdc4pc5k8mh0000gn/T/ipykernel_7347/3426711088.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into four subsets\n",
    "\n",
    "constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "constant_df.name = 'constant_df'\n",
    "#print(constant_df.columns)\n",
    "\n",
    "#TODO: this seems clunky, but was made so to avoid a warning I kept getting if I did the following:\n",
    "# qaqc_df[[col for col in qaqc_vars if col in df.columns]] = qaqc_df[[col for col in qaqc_vars if col in df.columns]].astype(str)\n",
    "# where \"qaqc_vars\" does NOT contain \"time\"\n",
    "\n",
    "#ensure that qaqc flags are all set as strings\n",
    "qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "qaqc_vars_subset.remove(\"time\")\n",
    "qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n",
    "qaqc_df.name = 'qaqc_df'\n",
    "#print(qaqc_df.columns)\n",
    "\n",
    "sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "sum_df.name = 'sum_df'\n",
    "#print(sum_df.columns)\n",
    "\n",
    "instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "instant_df.name = 'instant_df'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: generate a subset of the data containing only qaqc columns, with all but\n",
    "#       the time column entries set as strings, a requirement for string concatenation\n",
    "#       down the line\n",
    "\n",
    "# create list of qaqc variables\n",
    "test_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "\n",
    "# subset the dataframe \n",
    "test_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "\n",
    "# remove \"time\"\n",
    "test_vars.remove(\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['pr_eraqc', 'pr_depth_qc', 'ps_qc', 'ps_altimeter_qc', 'ps_eraqc', 'ps_altimeter_eraqc', 'psl_qc', 'psl_eraqc', 'tdps_qc', 'tdps_eraqc', 'sfcWind_qc', 'sfcWind_dir_qc', 'sfcWind_eraqc', 'sfcWind_dir_eraqcelevation_eraqc', 'qaqc_process'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# and then set all other entries to strings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mqaqc_vars\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1020\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1020\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hist-obs/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['pr_eraqc', 'pr_depth_qc', 'ps_qc', 'ps_altimeter_qc', 'ps_eraqc', 'ps_altimeter_eraqc', 'psl_qc', 'psl_eraqc', 'tdps_qc', 'tdps_eraqc', 'sfcWind_qc', 'sfcWind_dir_qc', 'sfcWind_eraqc', 'sfcWind_dir_eraqcelevation_eraqc', 'qaqc_process'] not in index\""
     ]
    }
   ],
   "source": [
    "# and then set all other entries to strings\n",
    "test_df.loc[:,qaqc_vars].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_result = sum_df.resample('1h',on='time').apply(lambda x: np.nan if x.isna().all() else x.sum(skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_result.plot(y=['pr_5min','pr_1h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = sum_result['pr_5min']-sum_result['pr_1h']\n",
    "diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_result_counts =  sum_df.resample('1h',on='time').count()\n",
    "#sum_result_counts.columns = sum_result_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\")\n",
    "print(sum_result_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantaneous aggregation\n",
    "\n",
    "instant_result =  instant_df.resample('1h',on='time').first()\n",
    "print(instant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate qaqc flags within each hour\n",
    "# takes 1.5 minutes to run!\n",
    "qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x.unique()))\n",
    "print(qaqc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the first value in each hour\n",
    "constant_result =  constant_df.resample('1h',on='time').first()\n",
    "print(constant_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_list = [sum_result,instant_result,constant_result,qaqc_result]\n",
    "result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "                                            how='outer'), result_list)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative method for combinding the subsets that does not require a new module\n",
    "result_merge = sum_result.merge(instant_result, on='time', how='outer').merge(constant_result, on='time', how='outer').merge(qaqc_result, on='time', how='outer')\n",
    "print(result_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not include log file and verbose statements, which are included in the utils scripts so that messages printed to log file\n",
    "\n",
    "def hourly_standardization(df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions. \n",
    "    \n",
    "    Rules\n",
    "    ------\n",
    "        1.) top of the hour: take the first value in each hour\n",
    "            - standard convention for temperature, dewpoint, wind speed, direction, relative humidity, air pressure\n",
    "        2.) summation across hour: sum observations within each hour\n",
    "            - standard convention for precipitation and solar radiation\n",
    "        3.) constant across the hour: take the first value in each hour\n",
    "            - this applies to variables, like station name and location, that do not change within each hour\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame \n",
    "            station dataset converted to dataframe through QAQC pipeline\n",
    "        verbose: boolean\n",
    "            input for printf() to print to log file - set in script initialization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            df [pd.DataFrame]\n",
    "                QAQC dataframe with all columns resampled to one hour (column name retained) and\n",
    "                columns with hourly observation counts for each variables\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running: hourly_standardization\", flush=True)\n",
    "\n",
    "    ##### define the variables for each sub-dataframe #####\n",
    "\n",
    "    # Variables that remain constant within each hour\n",
    "    constant_vars = [\"time\",\"station\", \"lat\", \"lon\", \"elevation\",\n",
    "                    \"anemometer_height_m\",\"thermometer_height_m\",\n",
    "                    'sfcWind_method',\n",
    "                    'pr_duration',\n",
    "                    \"hour\",\"day\",\"month\",\"year\",\"date\"]\n",
    "\n",
    "    # Aggregation across hour variables, standard meteorological convention: precipitation and solar radiation\n",
    "    sum_vars = [\"time\",\n",
    "                \"pr\",\"pr_localmid\",\"pr_24h\",\"pr_5min\",\"pr_1h\",\n",
    "                \"rsds\"]\n",
    "\n",
    "    # Top of the hour variables, standard meteorological convention: temperature, dewpoint temperature, pressure, humidity, winds\n",
    "    instant_vars = [\"time\",\n",
    "                \"tas\", \"tdps\",\"tdps_derived\",\n",
    "                \"ps\",\"psl\",\"ps_altimeter\",\"ps_derived\",  \n",
    "                \"hurs\",\n",
    "                \"sfcwind\",\"sfcwind_dir\"]\n",
    "    \n",
    "    # QAQC flags, which remain constants within each hour\n",
    "    qaqc_vars = [\"time\",\"tas_qc\", \"tas_eraqc\",\n",
    "                \"pr_5min_eraqc\",\"pr_1h_eraqc\",\"pr_5min_qc\",'pr_eraqc','pr_depth_qc', \n",
    "                \"ps_qc\",'ps_altimeter_qc','ps_eraqc','ps_altimeter_eraqc', \n",
    "                'psl_qc','psl_eraqc',\n",
    "                'tdps_qc','tdps_eraqc',\n",
    "                'sfcWind_qc','sfcWind_dir_qc','sfcWind_eraqc','sfcWind_dir_eraqc'\n",
    "                \"elevation_eraqc\", \"qaqc_process\"]\n",
    "    \n",
    "    # All variables, necessary for producing columns with hourly counts for each variable\n",
    "    all_vars = constant_vars+sum_vars+instant_vars+qaqc_vars\n",
    "    \n",
    "    ##### Subset the dataframe according to rules\n",
    "    constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "    qaqc_df = df[[col for col in qaqc_vars if col in df.columns]]\n",
    "    qaqc_vars_subset = qaqc_df.columns.tolist()\n",
    "    qaqc_vars_subset.remove(\"time\")\n",
    "    qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str) \n",
    "\n",
    "    sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "\n",
    "    instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "\n",
    "    all_vars_df = df[[col for col in all_vars if col in df.columns]]\n",
    "    \n",
    "    ##### \n",
    "    \n",
    "    try:        \n",
    "        # if station does not report any variable, bypass\n",
    "        if len(df.columns) == 0:\n",
    "            print('Empty dataset - bypassing hourly aggregation', flush=True)\n",
    "            return df\n",
    "        else: \n",
    "            # Performing hourly aggregation\n",
    "            constant_result =  constant_df.resample('1h',on='time').first()\n",
    "            instant_result =  instant_df.resample('1h',on='time').first()\n",
    "            sum_result =  sum_df.resample('1h',on='time').apply(lambda x: np.nan if x.isna().all() else x.sum(skipna=True))\n",
    "            qaqc_result = qaqc_df.resample('1h',on='time').apply(lambda x: ','.join(x.unique())) # adding unique flags\n",
    "\n",
    "            # # Generating variable counts per hour\n",
    "            # all_vars_counts =  all_vars_df.resample('1h',on='time').count()\n",
    "            # all_vars_counts.columns = all_vars_counts.columns.map(lambda x: \"nobs_\" + x + \"_hourstd\") # adding obs count per hour\n",
    "\n",
    "            # # Aggregating and outputting reduced dataframe\n",
    "            # result_list = [sum_result,instant_result,constant_result,qaqc_result,all_vars_counts]\n",
    "            # result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "            #                                 how='outer'), result_list)\n",
    "            # return result\n",
    "\n",
    "            # Aggregating and outputting reduced dataframe\n",
    "            result_list = [sum_result,instant_result,constant_result,qaqc_result]\n",
    "            result = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
    "                                            how='outer'), result_list)\n",
    "            return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"hourly_standardization failed with Exception: {0}\".format(e), flush=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: hourly_standardization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/kbcbbl557b96fgdc4pc5k8mh0000gn/T/ipykernel_7347/413472883.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  qaqc_df[qaqc_vars_subset] = qaqc_df[qaqc_vars_subset].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     pr_5min  pr_1h      tas    station     lat        lon  \\\n",
      "time                                                                         \n",
      "2013-03-16 22:00:00      NaN    NaN  282.090  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-16 23:00:00      NaN    NaN  281.590  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 00:00:00      NaN    NaN  281.150  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 01:00:00      NaN    NaN  278.480  CRN_AGPC2  40.155 -103.14167   \n",
      "2013-03-17 02:00:00      NaN    NaN  276.650  CRN_AGPC2  40.155 -103.14167   \n",
      "...                      ...    ...      ...        ...     ...        ...   \n",
      "2022-08-31 19:00:00      0.0    NaN  305.378  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 20:00:00      0.0    0.0  306.189  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 21:00:00      0.0    0.0  306.578  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 22:00:00      0.0    0.0  305.200  CRN_AGPC2  40.155 -103.14167   \n",
      "2022-08-31 23:00:00      0.0    0.0  303.350  CRN_AGPC2  40.155 -103.14167   \n",
      "\n",
      "                     elevation  anemometer_height_m  thermometer_height_m  \\\n",
      "time                                                                        \n",
      "2013-03-16 22:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-16 23:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 00:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 01:00:00   1383.792                  NaN                   NaN   \n",
      "2013-03-17 02:00:00   1383.792                  NaN                   NaN   \n",
      "...                        ...                  ...                   ...   \n",
      "2022-08-31 19:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 20:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 21:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 22:00:00   1383.792                  NaN                   NaN   \n",
      "2022-08-31 23:00:00   1383.792                  NaN                   NaN   \n",
      "\n",
      "                     hour   day  month    year        date tas_qc tas_eraqc  \\\n",
      "time                                                                          \n",
      "2013-03-16 22:00:00  22.0  16.0    3.0  2013.0  2013-03-16    nan       nan   \n",
      "2013-03-16 23:00:00  23.0  16.0    3.0  2013.0  2013-03-16    nan       nan   \n",
      "2013-03-17 00:00:00   0.0  17.0    3.0  2013.0  2013-03-17    nan       nan   \n",
      "2013-03-17 01:00:00   1.0  17.0    3.0  2013.0  2013-03-17    nan       nan   \n",
      "2013-03-17 02:00:00   2.0  17.0    3.0  2013.0  2013-03-17    nan       nan   \n",
      "...                   ...   ...    ...     ...         ...    ...       ...   \n",
      "2022-08-31 19:00:00  19.0  31.0    8.0  2022.0  2022-08-31    nan       nan   \n",
      "2022-08-31 20:00:00  20.0  31.0    8.0  2022.0  2022-08-31    nan       nan   \n",
      "2022-08-31 21:00:00  21.0  31.0    8.0  2022.0  2022-08-31    nan       nan   \n",
      "2022-08-31 22:00:00  22.0  31.0    8.0  2022.0  2022-08-31    nan       nan   \n",
      "2022-08-31 23:00:00  23.0  31.0    8.0  2022.0  2022-08-31    nan       nan   \n",
      "\n",
      "                    pr_5min_eraqc pr_1h_eraqc pr_5min_qc  \n",
      "time                                                      \n",
      "2013-03-16 22:00:00          16.0         nan        nan  \n",
      "2013-03-16 23:00:00          16.0         nan        nan  \n",
      "2013-03-17 00:00:00          16.0         nan        nan  \n",
      "2013-03-17 01:00:00          16.0         nan        nan  \n",
      "2013-03-17 02:00:00          16.0         nan        nan  \n",
      "...                           ...         ...        ...  \n",
      "2022-08-31 19:00:00          16.0         nan        nan  \n",
      "2022-08-31 20:00:00          16.0         nan        nan  \n",
      "2022-08-31 21:00:00          16.0         nan        nan  \n",
      "2022-08-31 22:00:00          16.0         nan        nan  \n",
      "2022-08-31 23:00:00          16.0         nan        nan  \n",
      "\n",
      "[82922 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "result = hourly_standardization(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# agg_funcs = {\n",
    "#     col:'sum' if col in met_vars else continue\n",
    "#     for col in df.columns\n",
    "# }\n",
    "\n",
    "#df_resampled = df.resample('D').agg(agg_funcs)\n",
    "\n",
    "# result = df.resample('1h',on='time').add_funcs()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try turning one particular column into a string\n",
    "qaqc_df[\"tas_eraqc\"] = qaqc_df[\"tas_eraqc\"].astype(str)\n",
    "# qaqc_df[\"tas_eraqc\"].astype(str) #this does NOT work\n",
    "print(qaqc_df['tas_eraqc'].apply(type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=10, freq='D'),\n",
    "    'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
    "    'value': [10, 15, 12, 18, 14, 20, 16, 22, 18, 24]\n",
    "})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Group by the variable and apply different resampling techniques\n",
    "resampled_df = df.groupby('category').resample('W').agg({\n",
    "    'value': {'A': 'mean', 'B': 'first'}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##### identify which precipitation vars are reported  #####\n",
    "    # all_pr_vars = [var for var in df.columns if 'pr' in var] # can be variable length depending if there is a raw qc var\n",
    "    #pr_vars = [var for var in all_pr_vars if not any(True for item in vars_to_remove if item in var)] # remove all qc variables so they do not also run through: raw, eraqc, qaqc_process\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_sum(x):\n",
    "    output = np.nan if x.isna().all() else x.sum()\n",
    "    return output\n",
    "\n",
    "   # Use transform instead of agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resampler_sum(df,period,time_col):\n",
    "    groups = df.resample(period,on=time_col)\n",
    "    sums = [x[1].sum(skipna=False) for x in groups]\n",
    "    return pd.DataFrame(sums, groups.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resample(x):\n",
    "    if np.all(np.isnan(x)):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if x in sum_vars:\n",
    "            return np.sum(x)\n",
    "        if x in instant_vars:\n",
    "            return np.first()\n",
    "        if x in qaqc_vars:\n",
    "            return np.apply(lambda x: ','.join(x.unique()))\n",
    "        if x in constant_vars:\n",
    "            return np.first()\n",
    "        else:\n",
    "            printf(\"custom resampling not successful\", verbose=true, log_file=log_file, flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
