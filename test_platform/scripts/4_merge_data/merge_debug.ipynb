{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d4e80f",
   "metadata": {},
   "source": [
    "# Debugging MERGE_pipeline.py \n",
    "Break down the script step by step to debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96265ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "import inspect\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import logging\n",
    "\n",
    "from merge_log_config import setup_logger, upload_log_to_s3\n",
    "from merge_hourly_standardization import merge_hourly_standardization\n",
    "from merge_derive_missing import merge_derive_missing_vars\n",
    "from merge_clean_vars import merge_reorder_vars, merge_drop_vars\n",
    "from merge_eraqc_counts import eraqc_counts_native_timestep\n",
    "\n",
    "from MERGE_pipeline import read_station_metadata, validate_station, read_zarr_dataset, get_var_attrs, convert_xr_to_df, convert_df_to_xr, write_zarr_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490246d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"CDEC_BLB\"\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"wecc-historical-wx\"\n",
    "stations_csv_path = f\"s3://{bucket_name}/2_clean_wx/temp_clean_all_station_list.csv\"\n",
    "qaqc_dir = \"3_qaqc_wx\"\n",
    "merge_dir = \"4_merge_wx\"\n",
    "\n",
    "# Log start time\n",
    "start_time = time.time()\n",
    "\n",
    "## ======== SETUP ========\n",
    "\n",
    "# Set up logger\n",
    "logger, log_filepath = setup_logger(station, verbose=verbose)\n",
    "\n",
    "# Load station metadata\n",
    "stations_df = read_station_metadata(stations_csv_path, logger)\n",
    "\n",
    "# Validate station and get network name\n",
    "network_name = validate_station(station, stations_df, logger)\n",
    "\n",
    "## ======== READ IN AND REFORMAT DATA ========\n",
    "\n",
    "# Load Zarr dataset from S3\n",
    "ds = read_zarr_dataset(bucket_name, qaqc_dir, network_name, station, logger)\n",
    "\n",
    "# Get variable attributes from dataset\n",
    "var_attrs = get_var_attrs(ds, network_name, logger)\n",
    "\n",
    "# Convert dataset to DataFrame\n",
    "df = convert_xr_to_df(ds, logger)\n",
    "\n",
    "# ======== MERGE FUNCTIONS ========\n",
    "\n",
    "# Part 1: Construct and export table of raw QAQC counts per variable\n",
    "# For success report\n",
    "eraqc_counts_native_timestep(df, network_name, station, logger)\n",
    "df0 = df.copy()\n",
    "# Part 2: Derive any missing variables\n",
    "df, var_attrs = merge_derive_missing_vars(df, var_attrs, logger)\n",
    "\n",
    "df1 = df.copy()\n",
    "\n",
    "# Part 3: Standardize sub-hourly observations to hourly\n",
    "df, var_attrs = merge_hourly_standardization(df, var_attrs, logger)\n",
    "\n",
    "# Part 3b: Construct and export table of raw QAQC counts per variable post-hourly standardization\n",
    "# For HDP project documentation and final report\n",
    "# ----- INCOMPLETE -----\n",
    "\n",
    "# Part 4: Drops raw _qc variables (DECISION TO MAKE) or provide code to filter\n",
    "df2 = df.copy()\n",
    "df, var_attrs = merge_drop_vars(df, var_attrs, logger)\n",
    "\n",
    "# Part 5: Re-orders variables into final preferred order\n",
    "df = merge_reorder_vars(df, logger)\n",
    "\n",
    "# ======== CLEANUP & UPLOAD DATA TO S3 ========\n",
    "\n",
    "# Convert the cleaned DataFrame to an xarray.Dataset and assign global + variable-level metadata\n",
    "ds_merged = convert_df_to_xr(df, ds.attrs, var_attrs, logger)\n",
    "\n",
    "# # Write the xarray Dataset as a Zarr file to the specified S3 path\n",
    "# write_zarr_to_s3(\n",
    "#     ds_merged, bucket_name, merge_dir, network_name, station, logger\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f517eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df0.columns)\n",
    "print(df1.columns)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_attrs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
