{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d0a3ef",
   "metadata": {},
   "source": [
    "# QAQC flag counts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe6225",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14260284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "from functools import reduce\n",
    "\n",
    "import inspect\n",
    "\n",
    "import logging\n",
    "# Create a simple logger that just prints to the console\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e983a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ds_to_df(ds):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for processing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xr.Dataset\n",
    "        Data object with information about each network and station\n",
    "    verbose: boolean\n",
    "        Flag as to whether to print runtime statements to terminal. Default is False. Set in ALLNETWORKS_merge.py run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Table object with information about each network and station\n",
    "    MultiIndex: pd.DataFrame (I think)\n",
    "        Original multi-index of station and time, to be used on conversion back to ds\n",
    "    attrs:\n",
    "        Save ds attributes to inherent to the final merged file\n",
    "    var_attrs:\n",
    "        Save variable attributes to inherent to the final merged file\n",
    "    \"\"\"\n",
    "\n",
    "    # Save attributes to inherent them to the final merged file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    df = ds.to_dataframe()\n",
    "\n",
    "    # Save instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            logger.info(\"Filling anemometer_height_m with NaN.\")\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            logger.info(\"Filling thermometer_height_m with NaN.\")\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    return df, MultiIndex, attrs, var_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74b64f",
   "metadata": {},
   "source": [
    "## Set the stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28220b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_6001.zarr\"\n",
    "url = \"s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/ASOSAWOS_72493023230.zarr\"\n",
    "ds = xr.open_zarr(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, MultiIndex, attrs, var_attrs = merge_ds_to_df(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f02584",
   "metadata": {},
   "source": [
    "### Perform hourly standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def qaqc_flag_fcn(flags: str) -> str:\n",
    "    \"\"\"\n",
    "    Used for resampling QAQC flag columns. Ensures that the final standardized dataframe\n",
    "    does not contain any empty strings by returning 'nan' when given an empty input (i.e. in time gaps).\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    flags : array_like\n",
    "        sub-hourly timestep data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str : final flag value\n",
    "\n",
    "    \"\"\"\n",
    "    if len(flags) == 0:\n",
    "        return \"nan\"\n",
    "    else:\n",
    "        return \",\".join(flags.unique())\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "def _modify_infill(df: pd.DataFrame, constant_vars: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function does two things:\n",
    "    1. Flags rows that were infilled by resampling in the hourly standardization process, where\n",
    "        there were time gaps in the input dataframe. These infilled rows will NOT count towards\n",
    "        the total observations count when calculating flag rates for the success report\n",
    "    2. Infills constant variables (ie those in \"constant_vars\") observations that were left empty because\n",
    "        they were in a time gap. They are infilled with the first non-nan value of each column, and set to\n",
    "        np.nan if there are no non-nan values.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : pd.Dataframe\n",
    "        hourly standardized dataframe\n",
    "    constant_vars: list\n",
    "        variables that are constant throughout time\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.Dataframe\n",
    "        dataframe with updates added to rows infilled by hourly standardization\n",
    "\n",
    "    \"\"\"\n",
    "    # Mask for rows where station is None (or np.nan)\n",
    "    mask = df[\"station\"].isnull()\n",
    "\n",
    "    # Initialize dict to hold first non-NaN values\n",
    "    first_valids = {}\n",
    "\n",
    "    # Populate first_valids only for existing columns\n",
    "    for col in constant_vars:\n",
    "        if col not in df.columns or col == \"time\":\n",
    "            # skip if constant var not in df cols or if var == \"time\"\n",
    "            continue\n",
    "\n",
    "        first_valids[col] = (\n",
    "            df[col].dropna().iloc[0] if df[col].notna().any() else np.nan\n",
    "        )\n",
    "\n",
    "    # Update values in masked rows for existing columns\n",
    "    for col, val in first_valids.items():\n",
    "        df.loc[mask, col] = val\n",
    "\n",
    "    # Add or update 'standardized_infill' column\n",
    "    df[\"standardized_infill\"] = np.where(mask, \"y\", \"n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "def merge_hourly_standardization(\n",
    "    df: pd.DataFrame, var_attrs: dict, logger: logging.Logger\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        station dataset converted to dataframe through QAQC pipeline\n",
    "    var_attrs: library\n",
    "        attributes for sub-hourly variables\n",
    "    logger : logging.Logger\n",
    "        Logger instance for recording messages during processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame | None\n",
    "        returns a dataframe with all columns resampled to one hour (column name retained)\n",
    "    var_attrs : dict | None\n",
    "        returns variable attributes dictionary updated to note that sub-hourly variables are now hourly\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Rules:\n",
    "    1. Top of the hour: take the first value in each hour. Standard convention for temperature, dewpoint, wind speed, direction, relative humidity, air pressure.\n",
    "    2. Summation across the hour: sum observations within each hour. Standard convention for precipitation and solar radiation.\n",
    "    3. Constant across the hour: take the first value in each hour. This applied to variables that do not change.\n",
    "    \"\"\"\n",
    "\n",
    "    #logger.info(f\"{inspect.currentframe().f_code.co_name}: Starting...\")\n",
    "\n",
    "    # Variables that remain constant within each hour\n",
    "    constant_vars = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"elevation\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]\n",
    "\n",
    "    # Aggregation across hour variables, standard meteorological convention: precipitation and solar radiation\n",
    "    sum_vars = [\n",
    "        \"time\",\n",
    "        \"pr\",\n",
    "        \"pr_localmid\",\n",
    "        \"pr_24h\",\n",
    "        \"pr_1h\",\n",
    "        \"pr_15min\",\n",
    "        \"pr_5min\",\n",
    "        \"rsds\",\n",
    "    ]\n",
    "\n",
    "    # Top of the hour variables, standard meteorological convention: temperature, dewpoint temperature, pressure, humidity, winds\n",
    "    instant_vars = [\n",
    "        \"hurs_derived\",\n",
    "        \"time\",\n",
    "        \"tas\",\n",
    "        \"tas_derived\",\n",
    "        \"tdps\",\n",
    "        \"tdps_derived\",\n",
    "        \"ps\",\n",
    "        \"psl\",\n",
    "        \"ps_altimeter\",\n",
    "        \"ps_derived\",\n",
    "        \"hurs\",\n",
    "        \"sfcWind\",\n",
    "        \"sfcWind_dir\",\n",
    "    ]\n",
    "\n",
    "    # QAQC variable suffixes\n",
    "    # the QAQC variables contain these words (ex: ps_eraqc)\n",
    "    qaqc_var_suffixes = [\n",
    "        \"qc\",\n",
    "        \"eraqc\",\n",
    "        \"duration\",\n",
    "        \"method\",\n",
    "        \"flag\",\n",
    "        \"depth\",\n",
    "        \"process\",\n",
    "    ]\n",
    "\n",
    "    # QAQC variables, which we will concatenate within each hour\n",
    "    qaqc_vars = [\n",
    "        var for var in df.columns if any(item in var for item in qaqc_var_suffixes)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Subset the dataframe according to rules\n",
    "        constant_df = df[[col for col in constant_vars if col in df.columns]]\n",
    "\n",
    "        qaqc_df = df[[col for col in qaqc_vars if col in df.columns if col != \"time\"]]\n",
    "        qaqc_df = qaqc_df.astype(str)\n",
    "        qaqc_df.insert(0, \"time\", df[\"time\"])\n",
    "\n",
    "        sum_df = df[[col for col in sum_vars if col in df.columns]]\n",
    "\n",
    "        instant_df = df[[col for col in instant_vars if col in df.columns]]\n",
    "\n",
    "        # Performing hourly aggregation, only if subset contains more than one (ie more than the 'time' time) column\n",
    "        # This is to account for input dataframes that do not contain ALL subsets of variables defined above - just a subset of them.\n",
    "        result_list = []\n",
    "        if len(constant_df.columns) > 1:\n",
    "            constant_result = constant_df.resample(\"1h\", on=\"time\").first()\n",
    "            result_list.append(constant_result)\n",
    "\n",
    "        if len(instant_df.columns) > 1:\n",
    "            instant_result = instant_df.resample(\"1h\", on=\"time\").first()\n",
    "            result_list.append(instant_result)\n",
    "\n",
    "        if len(sum_df.columns) > 1:\n",
    "            sum_result = sum_df.resample(\"1h\", on=\"time\").apply(\n",
    "                lambda x: np.nan if x.isna().all() else x.sum(skipna=True)\n",
    "            )\n",
    "            result_list.append(sum_result)\n",
    "\n",
    "        if len(qaqc_df.columns) > 1:\n",
    "            qaqc_result = qaqc_df.resample(\"1h\", on=\"time\").apply(\n",
    "                lambda x: qaqc_flag_fcn(x)\n",
    "            )  # concatenating unique flags\n",
    "            result_list.append(qaqc_result)\n",
    "\n",
    "        # Aggregate and output reduced dataframe - this merges all dataframes defined\n",
    "        # This function sets \"time\" to the index; reset index to return to original index\n",
    "        result = reduce(\n",
    "            lambda left, right: pd.merge(left, right, on=[\"time\"], how=\"outer\"),\n",
    "            result_list,\n",
    "        )\n",
    "        result.reset_index(inplace=True)  # Convert time index --> column\n",
    "\n",
    "        # Infill constant values and flag rows added through resampling\n",
    "        result = _modify_infill(result, constant_vars)\n",
    "\n",
    "        # Update attributes for sub-hourly variables\n",
    "        sub_hourly_vars = [i for i in df.columns if \"min\" in i and \"qc\" not in i]\n",
    "        for var in sub_hourly_vars:\n",
    "            var_attrs[var][\"standardization\"] = (\n",
    "                \"{} has been standardized to an hourly timestep, but will retain its original name\".format(\n",
    "                    var\n",
    "                )\n",
    "            )\n",
    "        #logger.info(f\"{inspect.currentframe().f_code.co_name}: Completed successfully\")\n",
    "        return result, var_attrs\n",
    "\n",
    "    except Exception as e:\n",
    "        #logger.error(f\"{inspect.currentframe().f_code.co_name}: Failed\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting data to speed things up\n",
    "df_sub = df[['time','ps_eraqc','pr_eraqc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st, var_attrs_st = merge_hourly_standardization(df, var_attrs, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ba641",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed0f64",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "def eraqc_counts_original_timestep(\n",
    "    df: pd.DataFrame, network: str, station: str, logger: logging.Logger\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a dataframe of raw qaqc flag value counts for every variable,\n",
    "    in their native timestep, before hourly standardization.\n",
    "    Exports the dataframe as a csv to AWS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        station dataset converted to dataframe through QAQC pipeline\n",
    "    network: str\n",
    "        network name\n",
    "    station: str\n",
    "        station name\n",
    "    logger : logging.Logger\n",
    "        Logger instance for recording messages during processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{inspect.currentframe().f_code.co_name}: Starting...\")\n",
    "\n",
    "    try:\n",
    "        # identify _eraqc variables\n",
    "        eraqc_vars = [var for var in df.columns if \"_eraqc\" in var]\n",
    "\n",
    "        # filter df for only qaqc columns\n",
    "        # also replace Nan values with 'no_flag' for two reasons:\n",
    "        #   1. to enable us to count total observations for the success report\n",
    "        #   2. to clarify what the Nan value indicates\n",
    "        df = df[eraqc_vars].fillna(\"no_flag\")\n",
    "\n",
    "        # generate df of counts of each unique flag for each variable\n",
    "        # fill all Nan values with 0, since Nan = no observations counted\n",
    "        flag_counts = df.apply(pd.Series.value_counts).fillna(0)\n",
    "\n",
    "        # rename columns\n",
    "        flag_counts.columns = flag_counts.columns.str.replace(\"_eraqc\", \"\", regex=True)\n",
    "\n",
    "        # rename index (i.e. eraqc values) and then reset index\n",
    "        flag_counts = flag_counts.rename_axis(\"eraqc_flag_values\")\n",
    "\n",
    "        # set all counts to integers, for readability\n",
    "        flag_counts = flag_counts.astype(int)\n",
    "\n",
    "        # send file to AWS\n",
    "        csv_s3_filepath = f\"s3://wecc-historical-wx/4_merge_wx/{network}/eraqc_counts/{station}_flag_counts_native_timestep.csv\"\n",
    "        flag_counts.to_csv(csv_s3_filepath, index=True)\n",
    "\n",
    "        # Update logger\n",
    "        logger.info(f\"Uploaded file to: {csv_s3_filepath}\")\n",
    "        logger.info(f\"{inspect.currentframe().f_code.co_name}: Completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_code.co_name}: Failed\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def eraqc_counts_hourly_timestep(\n",
    "    df: pd.DataFrame, network: str, station: str, logger: logging.Logger\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a dataframe of raw qaqc flag value counts for every variable, for the hourly\n",
    "    timestep, after hourly standardization. Includes the total observation count.\n",
    "    Exports the dataframe as a CSV to AWS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        station dataset converted to dataframe through QAQC pipeline\n",
    "    network: str\n",
    "        network name\n",
    "    station: str\n",
    "        station name\n",
    "    logger : logging.Logger\n",
    "        Logger instance for recording messages during processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    logger.info(f\"{inspect.currentframe().f_code.co_name}: Starting...\")\n",
    "\n",
    "    try:\n",
    "        # filter out rows that were infilled during hourly standardization\n",
    "        df = df[df[\"standardized_infill\"] == \"n\"]\n",
    "\n",
    "        # identify _eraqc variables\n",
    "        eraqc_vars = [var for var in df.columns if \"_eraqc\" in var]\n",
    "\n",
    "        # filter df for only qaqc columns\n",
    "        # also replace Nan values with 'no_flag' for two reasons:\n",
    "        #   1. to enable us to count total observations for the success report\n",
    "        #   2. to clarify what the Nan value indicates\n",
    "        df_qaqc = df[eraqc_vars]\n",
    "\n",
    "        # generate df of counts of each unique flag for each variable\n",
    "        # fill all Nan values with 0, since Nan = no observations counted\n",
    "        flag_counts = df_qaqc.apply(\n",
    "            lambda x: x.str.split(\",\", expand=True).stack().value_counts()\n",
    "        ).fillna(0)\n",
    "\n",
    "        # rename columns\n",
    "        flag_counts.columns = flag_counts.columns.str.replace(\"_eraqc\", \"\", regex=True)\n",
    "\n",
    "        # rename index (i.e. eraqc values) and then reset index\n",
    "        flag_counts = flag_counts.rename_axis(\"eraqc_flag_values\")\n",
    "\n",
    "        # replace 'nan' (a string) with 'no_flag', for clarity\n",
    "        flag_counts = flag_counts.rename(index={\"nan\": \"no_flag\"})\n",
    "\n",
    "        # add row with total observation count\n",
    "        total_obs_count = len(df)\n",
    "        flag_counts.loc[\"total_obs_count\"] = [total_obs_count] * flag_counts.shape[1]\n",
    "\n",
    "        # set all counts to integers, for readability\n",
    "        flag_counts = flag_counts.astype(int)\n",
    "\n",
    "        # send file to AWS\n",
    "        csv_s3_filepath = f\"s3://wecc-historical-wx/4_merge_wx/{network}/eraqc_counts/{station}_flag_counts_hourly_standardized.csv\"\n",
    "        flag_counts.to_csv(csv_s3_filepath, index=True)\n",
    "\n",
    "        # Update logger\n",
    "        logger.info(f\"Uploaded file to: {csv_s3_filepath}\")\n",
    "        logger.info(f\"{inspect.currentframe().f_code.co_name}: Completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{inspect.currentframe().f_code.co_name}: Failed\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29689d6",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \"ASOSAWOS\"\n",
    "station = \"ASOSAWOS_72493023230\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a173b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eraqc_counts_hourly_timestep(df_st, network, station, logger)\n",
    "eraqc_counts_original_timestep(df, network, station, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07729803",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f\"4_merge_wx/{network}/eraqc_counts/{station}_flag_counts_native_timestep.csv\"\n",
    "\n",
    "list_import = s3_cl.get_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=key,\n",
    ")\n",
    "\n",
    "flag_counts_table = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "flag_counts_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = f\"4_merge_wx/{network}/eraqc_counts/{station}_flag_counts_hourly_standardized.csv\"\n",
    "\n",
    "list_import = s3_cl.get_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=key,\n",
    ")\n",
    "\n",
    "flag_counts_table = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "flag_counts_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
