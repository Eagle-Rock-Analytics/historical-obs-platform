"""
This script scrapes CWOP network data for ingestion into the Historical Observations Platform via API.
Approach:
(1) get_wecc_poly generates a bounding box from WECC shapefiles.
(2) get_meso_metadata produces a list of station IDs filtered by bounding box and network (CWOP in this case)
(3) get cwop_station_csv saves a csv for each station ID provided, with the option to select a start date for downloads (defaults to station start date).
Inputs: API key, paths to WECC shapefiles, path to save directory, start date (optional).
Outputs:
(1) Raw data for the CWOP network, all variables, all times. Organized by station.
(2) An error csv noting all station IDs where downloads failed.
"""

# Step 0: Environment set-up
import requests
import geopandas as gp
import pandas as pd
from datetime import datetime
import os
import re
import csv
import boto3
from io import BytesIO, StringIO
import calc_pull

## Set AWS credentials
s3 = boto3.resource("s3")
bucket_name = "wecc-historical-wx"
directory = "1_raw_wx/CWOP/"

try:
    import config # Import API keys.
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()

# Set envr variables

# Set path to head of git repository.
homedir = os.getcwd() # Get current working directory.
if "historical-obs-platform" in homedir: # If git folder in path
    homedir = homedir[0:homedir.index("historical-obs-platform")]+"historical-obs-platform" # Set path to top folder.
    os.chdir(homedir) # Change directory.
else:
    print("Error: Set current working directory to the git repository or a subfolder, and then rerun script.")
    exit()

# Set paths to WECC shapefiles in AWS bucket.
wecc_terr = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
wecc_mar = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"

# Function: return metadata for stations from synoptic API, filtering stations based on bounding box generated by get_wecc_poly.
# Inputs: Synoptic API public token (imported from config.py), path to terrestrial and marine WECC shapefiles relative to home directory.
# Outputs: Dataframe with list of station IDs and start date of station.
def get_meso_metadata(token, terrpath, marpath):
    try:
        t,m,bbox = calc_pull.get_wecc_poly(terrpath, marpath)
        bbox_api = bbox.loc[0,:].tolist() # [lonmin,latmin,lonmax,latmax]
        bbox_api = ','.join([str(elem) for elem in bbox_api])
        # Access station metadata to get list of IDs in bbox and network
        # Using: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        # CWOP data network ID in synoptic is 65 (see below)
        url = "https://api.synopticdata.com/v2/stations/metadata?token={}&network=65&bbox={}&recent=20&output=json".format(token, bbox_api)
        request = requests.get(url).json()
        # print(request)
        ids = []
        for each in request['STATION']:
            ids.append([each['STID'], each['PERIOD_OF_RECORD']['start']]) # Keep station ID and start date

        ids = pd.DataFrame(ids, columns = ['STID', 'start']).sort_values('start') # Sort by start date (note some stations return 'None' here)
        # print(ids)
        # Reformat date to match API format
        ids['start'] = pd.to_datetime(ids['start'], format='%Y-%m-%dT%H:%M:%SZ')
        ids['start'] = ids['start'].dt.strftime('%Y%m%d%H%M')
        return ids
    except Exception as e:
        print("Error: {}".format(e))

# Function: download CWOP station data from Synoptic API.
# Inputs:
# (1) token: Synoptic API token, stored in config.py file
# (2) ids: Takes dataframe with two columns, station ID and startdate. These are the stations to be downloaded, generated from get_meso_metadata.
# (3) savedir: path to directory where files should be saved.
# (4) start_date: If none, download data starting on 1980-01-01. Otherwise, download data after start date ("YYYYMMDDHHMM").
# Outputs: CSV for each station saved to savedir, starting at start date and ending at current date.
def get_cwop_station_csv(token, ids, bucket_name, directory, start_date = None, **options):

    # This is somewhat unneccesary because AWS buckets don't actually have directories (its more user-friendly organizational structures)
    # https://stackoverflow.com/questions/1939743/amazon-s3-boto-how-to-create-a-folder
    try:
        if directory != "1_raw_wx/CWOP/":
            directory = "1_raw_wx/CWOP/" # Make the directory to save data in. Except used to pass through code if folder already exists.
    except:
        pass

    # Set up error handling df.
    errors = {'Station ID':[], 'Time':[], 'Error':[]}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime('%Y%m%d%H%M')

    for index, id in ids.iterrows(): # For each station
        # Set start date
        if start_date is None:
            if id["start"] == "NaN" or pd.isna(id['start']): # Adding error handling for NaN dates.
                start_api = "198001010000" # If any of the stations in the chunk have a start time of NA, pull data from 01-01-1980 OH:OM and on.
                # To check: all stations seen so far with NaN return empty data. If true universally, can skip these instead of saving.
            else:
                start_api = id["start"]
        else:
            start_api = start_date

        # Generate URL
        # Note: decision here to use full flag suite of MesoWest and Synoptic data.
        # See Data Checks section here for more information: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&stid={}&start={}&end={}&output=csv&qc=on&qc_remove_data=off&qc_flags=on&qc_checks=synopticlabs,mesowest".format(token, id['STID'], start_api, end_api)
        # print(url) # For testing.

        # Try to get station csv.
        try:
            #request = requests.get(url)
            s3_obj = s3.Object(bucket_name, directory+"{}.csv".format(id["STID"]))

            # If **options timeout = True, save file as STID_2.csv
            if options.get("timeout") == True:
                s3_obj = s3.Object(bucket_name, directory+"{}_2.csv".format(id["STID"]))

            with requests.get(url, stream=True) as r:
                if r.status_code == 200: # If API call returns a response
                    if "RESPONSE_MESSAGE" in r.text: # If error response returned. Note that this is formatted differently depending on error type.
                        # Get error message and clean.
                        error_text = str(re.search("(RESPONSE_MESSAGE.*)",r.text).group(0)) # Get response message.
                        error_text = re.sub("RESPONSE_MESSAGE.: ", "", error_text)
                        error_text = re.sub(",.*", "", error_text)
                        error_text = re.sub('"', '', error_text)

                        # Append rows to dictionary
                        errors['Station ID'].append(id['STID'])
                        errors['Time'].append(end_api)
                        errors['Error'].append(error_text)
                        next
                    else:
                        # s3_obj = s3.Object(bucket_name, directory+"{}.csv".format(id["STID"]))
                        s3_obj.put(Body=r.content)
                        print("Saving data for station {}".format(id["STID"])) # Nice for testing, remove for full run.

                else:
                    errors['Station ID'].append(id['STID'])
                    errors['Time'].append(end_api)
                    errors['Error'].append(r.status_code)
                    print("Error: {}".format(r.status_code))

                    # If needed, split into smaller chunk? But csv seems to not kick so many errors.
            # if request['SUMMARY']['RESPONSE_CODE'] == -1 & request['SUMMARY']['RESPONSE_MESSAGE'].startswith("Querying too many station hours"):
            # # Response when the API call is too large. Split into smaller chunks.
            #     print("API request too large. Splitting into smaller chunks.")
            #     chunks = np.array_split(chunk, 3)
            #     pass # Write code to rerun w/ smaller chunks here.

            # Parse response into dataframe
            #### TO DO: clean and parse at the same time here?

        except Exception as e:
            print("Error: {}".format(e))

    # Write errors to csv for AWS
    csv_buffer_err = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer_err)
    content = csv_buffer_err.getvalue()
    s3.put_object(Bucket=bucket_name, Body=content, Key=directory+"errors_cwop_{}.csv".format(end_api))

# Run script.
ids = get_meso_metadata(token = config.token, terrpath = wecc_terr, marpath = wecc_mar)
get_cwop_station_csv(token = config.token, bucket_name = bucket_name, directory = directory, ids = ids.sample(40)) # .Sample() subset is for testing, remove for full run.


# Quality control: if any files return status 408 error, split request into smaller requests and re-run.
# Note: this approach assumes no file will need more than 2 splits. Test this when fuller data downloaded.
def get_cwop_station_timeout_csv(token, bucket_name, directory):
    ids_split = []
    res = s3.list_objects(Bucket=bucket_name, Prefix=directory)
    files = res("Contents")
    files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names

    for file in files:
        with open(bucket_name + directory + file,'r') as file:
            data = file.readlines()
            lastRow = data[-1]
            if 'Timeout' in lastRow: # If timeout recorded
                lastrealrow = data[-2] # Get last row of recorded data
                station = lastrealrow.split(",")[0] # Get station ID
                time = lastrealrow.split(",")[1] # Get last completed timestamp
                ids_split.append([station, time]) # Add to dataframe

    ids_split = pd.DataFrame(ids_split, columns = ['STID', 'start'])
    ids_split['start'] = pd.to_datetime(ids_split['start'], format='%Y-%m-%dT%H:%M:%SZ')
    ids_split['start'] = ids_split['start'].dt.strftime('%Y%m%d%H%M')
    if ids_split.empty is False:
        print(ids_split)
        get_cwop_station_csv(token, bucket_name, directory, ids = ids_split, timeout = True)

        # Check to see if any of the split files needs to be split again.
        res = s3.list_objects(Bucket=bucket_name, Prefix=directory)
        files = res("Contents")
        files = list(filter(lambda f: f.endswith("_2.csv"), files)) # Get list of file names

        for file in files:
            with open(bucket_name + directory + file,'r') as file:
                data = file.readlines()
                lastRow = data[-1]
                if 'Timeout' in lastRow: # If timeout recorded
                    lastrealrow = data[-2] # Get last row of recorded data
                    station = lastrealrow.split(",")[0] # Get station ID
                    time = lastrealrow.split(",")[1] # Get last completed timestamp
                    ids_split.append([station, time]) # Add to dataframe
                    if ids_split.empty is False:
                        print("Attention!: Run this script again on _2.csv files.")

    elif ids_split.empty is True:
        return

get_cwop_station_timeout_csv(token = config.token, bucket_name = bucket_name, directory = directory)


# Test: run on subset!
# # Get 3 real rows (or more as desired.)
#ids = get_meso_metadata(token = config.token, terrpath = wecc_terr, marpath = wecc_mar)
# ids = ids.sample(3)
# # And make 3 fake rows that might break our code - wrong station id, wrong time, and NaN in time format.
# test = pd.DataFrame({'STID': ["0ier", "E2082", "F2382"],
#                      'start': ["200001010000", "2", "NaN"]})
# ids = ids.append(test, ignore_index = True)
# print(ids)
#get_cwop_station_csv(token = config.token, ids = ids, savedir = savedir) # Run this with our test data.


### NOTES/SCRAPS FROM ALONG THE WAY

# with open("test.csv", 'wb') as f, \
#                 requests.get("https://api.synopticdata.com/v2/stations/timeseries?token=34e62da9b7c74f15a02ca172e6206bc3&stid=irgt&start=201905132239&end=202207261229&output=csv&qc=on&qc_remove_data=off&qc_flags=on", stream=True) as r:
#                 id = "AFtest"
#                 end_api = "200000000000"
#                 if r.status_code == 200: # If API call returns a response
#                     #print("Saving data for station {}".format(id["STID"])) # Nice for testing, remove for full run.
#                     #print(r.text())
#                     if "# RESPONSE_MESSAGE" in r.text:
#                        error_text = str(re.search("(RESPONSE_MESSAGE:.*)",r.text).group(0))
#                        error_text = error_text.replace("RESPONSE_MESSAGE:", "")
#                        error = {'Station ID': id, 'Time': end_api, 'Error': error_text} # Add error to error log.
#                        print(error)
#                     #for line in r.iter_lines():

#                         #f.write(line+'\n'.encode())
#                         #next




# # Option 2: generate JSONS for sets of stations. Still to figure out - how to join etc.
# ### Not finished!
# start_date = None
# def get_cwop_stations(token, ids, chunk_size, start_date = None):
# # Takes dataframe with two columns, station ID and startdate. Optional parameter to specify later start date ("YYYYMMDDHHMM")
# # no_chunk specifies the number of groups the ID list is split into. Play with this if API starts to break.
#     no_chunk = int(len(ids)/chunk_size) # Calculate the number of groups needed to have "chunk_size" stations in each grouo.
#     id_chunks = np.array_split(ids, no_chunk)
#     for chunk in id_chunks: # For a group of stations
#         # Get list of station IDs
#         idlist = ",".join(chunk["STID"])

#         # Get first start date in chunk
#         if start_date is None:
#             if chunk["start"].isnull().any()==True: # Adding error handling for NaN dates.
#                 start_api = "198001010000" # If any of the stations in the chunk have a start time of NA, pull data from 01-01-1980 and on.
#             else:
#                 start_api = chunk["start"].min()
#         else:
#             start_api = start_date

#         end_api = datetime.now().strftime('%Y%m%d%H%M')
#         url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&stid={}&start={}&end={}&output=json&qc_remove_data=off&qc_flags=on".format(token, idlist, start_api, end_api)
#         print(url) # For testing.
#         try:
#             request = requests.get(url).json()
#             if request.status_code == 200:
#                 pass
#                 # WRITE CODE TO SAVE / APPEND.
#             if request['SUMMARY']['RESPONSE_CODE'] == -1 & request['SUMMARY']['RESPONSE_MESSAGE'].startswith("Querying too many station hours"):
#             # Response when the API call is too large. Split into smaller chunks.
#                 print("API request too large. Splitting into smaller chunks.")
#                 chunks = np.array_split(chunk, 3)
#                 pass # Write code to rerun w/ smaller chunks here.

#             # Parse response into dataframe
#             #### TO DO: clean and parse at the same time here?

#         except Exception as e:
#             print("Error: {}".format(e))
#         # get_cwop_stations(token = token, ids = ids, chunk_size = 5, start_date = "202001010000")


# t,m,bbox = get_wecc_poly(wecc_terr, wecc_mar)
# bbox_api = bbox.loc[0,:].tolist() # [lonmin,latmin,lonmax,latmax]
# bbox_api = ','.join([str(elem) for elem in bbox_api])

# url = "https://api.synopticdata.com/v2/stations/metadata?token={}&network=65&bbox={}&stid=F1632&recent=20&output=json".format(token, bbox_api)
# request = requests.get(url).json()
# print(request)

# For testing, grab last 20 minutes of data.
#url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&network=65&bbox={}&recent=20&output=json&qc_remove_data=off&qc_flags=on".format(token, bbox_api)
#request = requests.get(url).json()
#print(request)

#print(bbox_api) For testing

# Get info about CWOP network
#url = "https://api.synopticdata.com/v2/networks?token={}&shortname=APRSWXNET/CWOP".format(token)
#request = requests.get(url).json()
#print(request) # Print information about CWOP network.
