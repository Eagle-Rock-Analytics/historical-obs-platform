"""
DEPRACATED AS OF NOV 8 2022. Use MADIS_pull.py instead.
This script scrapes CWOP network data for ingestion into the Historical Observations Platform via API.
Approach:
(1) get_wecc_poly generates a bounding box from WECC shapefiles.
(2) get_meso_metadata produces a list of station IDs filtered by bounding box and network (CWOP in this case)
(3) get cwop_station_csv saves a csv for each station ID provided, with the option to select a start date for downloads (defaults to station start date).
Inputs: API key, paths to WECC shapefiles, path to save directory, start date (optional).
Outputs:
(1) Raw data for the CWOP network, all variables, all times. Organized by station.
(2) An error csv noting all station IDs where downloads failed.
"""

# Step 0: Environment set-up
import requests
import pandas as pd
from datetime import datetime
import os
import re
import boto3
from io import BytesIO, StringIO
import calc_pull

## Set AWS credentials
s3 = boto3.resource("s3")
s3_cl = boto3.client('s3') # for lower-level processes
bucket_name = "wecc-historical-wx"
directory = "1_raw_wx/CWOP/"

try:
    import config # Import API keys.
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()

# Set envr variables
# Set paths to WECC shapefiles in AWS bucket.
wecc_terr = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
wecc_mar = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"

# Function: return metadata for stations from synoptic API, filtering stations based on bounding box generated by get_wecc_poly.
# Inputs: Synoptic API public token (imported from config.py), path to terrestrial and marine WECC shapefiles relative to home directory.
# Outputs: Dataframe with list of station IDs and start date of station.
def get_meso_metadata(token, terrpath, marpath):
    try:
        t,m,bbox = calc_pull.get_wecc_poly(terrpath, marpath)
        bbox_api = bbox.loc[0,:].tolist() # [lonmin,latmin,lonmax,latmax]
        bbox_api = ','.join([str(elem) for elem in bbox_api])
        # Access station metadata to get list of IDs in bbox and network
        # Using: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        # CWOP data network ID in synoptic is 65 (see below)
        url = "https://api.synopticdata.com/v2/stations/metadata?token={}&network=65&bbox={}&recent=20&output=json".format(token, bbox_api)
        request = requests.get(url).json()
        ids = []
        for each in request['STATION']:
            ids.append([each['STID'], each['PERIOD_OF_RECORD']['start']]) # Keep station ID and start date

        ids = pd.DataFrame(ids, columns = ['STID', 'start']).sort_values('start') # Sort by start date (note some stations return 'None' here)
        # Reformat date to match API format
        ids['start'] = pd.to_datetime(ids['start'], format='%Y-%m-%dT%H:%M:%SZ')
        ids['start'] = ids['start'].dt.strftime('%Y%m%d%H%M')
        return ids
    except Exception as e:
        print("Error: {}".format(e))

# Function: download CWOP station data from Synoptic API.
# Inputs:
# (1) token: Synoptic API token, stored in config.py file
# (2) ids: Takes dataframe with two columns, station ID and startdate. These are the stations to be downloaded, generated from get_meso_metadata.
# (3) bucket_name: name of AWS bucket
# (4) directory: folder path in AWS bucket
# (5) start_date: If none, download data starting on 1980-01-01. Otherwise, download data after start date ("YYYYMMDDHHMM").
# (6) options: timeout = True will identify and download any station data that timed out the API request.
# Outputs: CSV for each station saved to savedir, starting at start date and ending at current date.
def get_cwop_station_csv(token, ids, bucket_name, directory, start_date = None, **options):

    # This is somewhat unneccesary because AWS buckets don't actually have directories (its more user-friendly organizational structures)
    # https://stackoverflow.com/questions/1939743/amazon-s3-boto-how-to-create-a-folder
    try:
        if directory != "1_raw_wx/CWOP/":
            directory = "1_raw_wx/CWOP/" # Make the directory to save data in. Except used to pass through code if folder already exists.
    except:
        pass

    # Set up error handling df.
    errors = {'Station ID':[], 'Time':[], 'Error':[]}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime('%Y%m%d%H%M')

    for index, id in ids.iterrows(): # For each station
        # Set start date
        if start_date is None:
            if id["start"] == "NaN" or pd.isna(id['start']): # Adding error handling for NaN dates.
                start_api = "198001010000" # If any of the stations in the chunk have a start time of NA, pull data from 01-01-1980 OH:OM and on.
                # To check: all stations seen so far with NaN return empty data. If true universally, can skip these instead of saving.
            else:
                start_api = id["start"]
        else:
            start_api = start_date

        # Generate URL
        # Note: decision here to use full flag suite of MesoWest and Synoptic data.
        # See Data Checks section here for more information: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&stid={}&start={}&end={}&output=csv&qc=on&qc_remove_data=off&qc_flags=on&qc_checks=synopticlabs,mesowest".format(token, id['STID'], start_api, end_api)

        # Try to get station csv.
        try:
            #request = requests.get(url)
            s3_obj = s3.Object(bucket_name, directory+"{}.csv".format(id["STID"]))

            # If **options timeout = True, save file as STID_2.csv
            if options.get("timeout") == True:
                s3_obj = s3.Object(bucket_name, directory+"{}_2.csv".format(id["STID"]))

            with requests.get(url, stream=True) as r:
                if r.status_code == 200: # If API call returns a response
                    if "RESPONSE_MESSAGE" in r.text: # If error response returned. Note that this is formatted differently depending on error type.
                        # Get error message and clean.
                        error_text = str(re.search("(RESPONSE_MESSAGE.*)",r.text).group(0)) # Get response message.
                        error_text = re.sub("RESPONSE_MESSAGE.: ", "", error_text)
                        error_text = re.sub(",.*", "", error_text)
                        error_text = re.sub('"', '', error_text)

                        # Append rows to dictionary
                        errors['Station ID'].append(id['STID'])
                        errors['Time'].append(end_api)
                        errors['Error'].append(error_text)
                        next
                    else:
                        s3_obj.put(Body=r.content)
                        print("Saving data for station {}".format(id["STID"])) # Nice for testing, remove for full run.

                else:
                    errors['Station ID'].append(id['STID'])
                    errors['Time'].append(end_api)
                    errors['Error'].append(r.status_code)
                    print("Error: {}".format(r.status_code))

        except Exception as e:
            print("Error: {}".format(e))

    # Write errors to csv for AWS
    csv_buffer_err = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer_err)
    content = csv_buffer_err.getvalue()
    s3_cl.put_object(Bucket=bucket_name, Body=content, Key=directory+"errors_cwop_{}.csv".format(end_api))

# Quality control: if any files return status 408 error, split request into smaller requests and re-run.
# Note: this approach assumes no file will need more than 2 splits. Test this when fuller data downloaded.
def get_cwop_station_timeout_csv(token, bucket_name, directory):
    files = []
    for item in s3.Bucket(bucket_name).objects.filter(Prefix = directory): 
        file = str(item.key)
        files += [file]
    files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names
    
    files = [file for file in files if "errors" not in file]
    files = [file for file in files if "station" not in file] # Remove error and station list files

    for file in files:
            print(file)
            file = s3_cl.get_object(Bucket= bucket_name, Key= file) # Open file
            data = file['Body'].read().split(b"\n") # Read in file
            data = data[-10:] # Keep last ten rows.
            # Convert to strings.
            for i, val in enumerate(data):
                val = val.decode()
                print(val)
                if "Timeout" in val:
                    print("Timeout found in file {}. Queuing for redownload.".format(file))
                    lastrealrow = data[i-1] # Get last row of recorded data
                    station = lastrealrow.split(",")[0] # Get station ID
                    time = lastrealrow.split(",")[1] # Get last completed timestamp
                    ids_split.append([station, time]) # Add to dataframe

    ids_split = pd.DataFrame(ids_split, columns = ['STID', 'start'])
    ids_split['start'] = pd.to_datetime(ids_split['start'], format='%Y-%m-%dT%H:%M:%SZ')
    ids_split['start'] = ids_split['start'].dt.strftime('%Y%m%d%H%M')
    if ids_split.empty is False:
        print(ids_split)
        get_cwop_station_csv(token, bucket_name, directory, ids = ids_split, timeout = True)

        # Check to see if any of the split files needs to be split again.
        for item in s3.Bucket(bucket_name).objects.all():
            files = item.key
        files = list(filter(lambda f: f.endswith("_2.csv"), files)) # Get list of file names

        for file in files:
            with open(bucket_name + directory + file,'r') as file:
                data = file.readlines()
                lastRow = data[-1]
                if 'Timeout' in lastRow: # If timeout recorded
                    lastrealrow = data[-2] # Get last row of recorded data
                    station = lastrealrow.split(",")[0] # Get station ID
                    time = lastrealrow.split(",")[1] # Get last completed timestamp
                    ids_split.append([station, time]) # Add to dataframe
                    if ids_split.empty is False:
                        print("Attention!: Run this script again on _2.csv files.")

    elif ids_split.empty is True:
        return

# Run script.
ids = get_meso_metadata(token = config.token, terrpath = wecc_terr, marpath = wecc_mar)
get_cwop_station_csv(token = config.token, bucket_name = bucket_name, directory = directory, ids = ids) 
get_cwop_station_timeout_csv(token = config.token, bucket_name = bucket_name, directory = directory)


# Note: set ids = ids.sample(2) in get_cwop_station_csv for small subset for testing, remove for full run.

# Test: run on subset!
# # Get 3 real rows (or more as desired.)
#ids = get_meso_metadata(token = config.token, terrpath = wecc_terr, marpath = wecc_mar)
# ids = ids.sample(3)
# # And make 3 fake rows that might break our code - wrong station id, wrong time, and NaN in time format.
# test = pd.DataFrame({'STID': ["0ier", "E2082", "F2382"],
#                      'start': ["200001010000", "2", "NaN"]})
# ids = ids.append(test, ignore_index = True)
# print(ids)
#get_cwop_station_csv(token = config.token, ids = ids, savedir = savedir) # Run this with our test data.
