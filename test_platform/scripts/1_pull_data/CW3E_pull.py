"""
CW3E_pull.py

This script runs a full pull C3WE data from UCSD using ftp.
Approach:
(1) For each station, download STID_HourlyData_Full.txt file (with exception of 3 stations, which have alternate formats).

Functions
---------
- get_cw3e_metadata: Return metadata for CW3E stations from Synoptic API
- get_cw3e: Pull CW3E data via FTP
- get_cw3e_update: Pull updated CW3E data via FTP

Intended Use
------------ 
Retrieves raw data for an individual network, all variables, all times. Organized by station, with 1 file per station per year.

Notes
-----
1. Subsequent updates may want to just pull the most recent files in the 2022 subfolder by station, 
which are numbered by day and saved as .20m, .21m, .22m, .23m (in bytes). The data is typically at a native resolution of 3minutes, 
resulting in very large raw files.
2. This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.
See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.
"""

from ftplib import FTP
from datetime import datetime
import pandas as pd
import boto3
from io import BytesIO, StringIO
import calc_pull
import requests

try:
    from calc_pull import ftp_to_aws
except RuntimeError as e:
    print(f"Error importing calc_pull: {e}")

# Synoptic API keys (obsolete)
try:
    import config
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()

s3 = boto3.client("s3")
s3_cl = boto3.client("s3")  # for lower-level processes
BUCKET_NAME = "wecc-historical-wx"
DIRECTORY = "1_raw_wx/CW3E/"
WECC_TERR = (
    "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
)
WECC_MAR = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"


def get_cw3e_metadata(
    token: str, terrpath: str, marpath: str, directory: str
) -> pd.DataFrame | None:
    """
    Return metadata for CW3E stations from Synoptic API, filtering stations based on bounding box generated by get_wecc_poly.
    CW3E data does not have a station list, so we use MADIS as a station metadata source, but discard start and end dates (they're incorrect).

    Parameters
    ----------
    token : str
        Synoptic API token (private)
    terrpath : str
        shapefiles for maritime and terrestrial WECC boundaries
    marpath : str
        shapefiles for maritime and terrestrial WECC boundaries
    directory : str
        name of network directory in AWS

    Returns
    -------
    station_list : pd.DataFrame
        station list for metadata

    Notes
    -----
    Function uses the obsolete Synoptic API, API public token (imported from config.py).
    """
    try:
        t, m, bbox = calc_pull.get_wecc_poly(terrpath, marpath)
        bbox_api = bbox.loc[0, :].tolist()  # [lonmin,latmin,lonmax,latmax]
        bbox_api = ",".join([str(elem) for elem in bbox_api])

        # Access station metadata to get list of IDs in bbox and network
        # Using: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = f"https://api.synopticdata.com/v2/stations/metadata?token={token}&network=263&bbox={bbox_api}&recent=20&output=json"
        request = requests.get(url).json()

        station_list = pd.DataFrame(request["STATION"])

        # Split Period of Record column
        station_list = pd.concat(
            [station_list, station_list["PERIOD_OF_RECORD"].apply(pd.Series)], axis=1
        )
        station_list = station_list.drop("PERIOD_OF_RECORD", axis=1)

        # Remove start and end dates
        station_list = station_list.drop("start", axis=1)
        station_list = station_list.drop("end", axis=1)

        # Save station list to AWS
        csv_buffer_err = StringIO()
        station_list.to_csv(csv_buffer_err)
        content = csv_buffer_err.getvalue()
        s3_cl.put_object(
            Bucket=BUCKET_NAME, Body=content, Key=directory + "stationlist_CW3E.csv"
        )
        return station_list

    except Exception as e:
        print(f"Error: {e}")
        return None


def get_cw3e(directory: str, station: list[str] | None = None):
    """
    Pull CW3E data via FTP. HourlyData_Full files do not include as many variables
    as the individual bytes files, so we download the entire dataset in byte format.

    Parameters
    ----------
    directory : str
        path to folder in AWS for network
    station : list[str], optional
        specific station to download

    Returns
    -------
    None
    """

    # Login using ftplib
    ftp = FTP("sioftp.ucsd.edu")
    ftp.login()  # user anonymous, password anonymous
    ftp.cwd("CW3E_DataShare/CW3E_SurfaceMetObs/")
    ftp_to_aws(ftp, "HourlyData_README.txt", directory)
    pwd = ftp.pwd()

    # Set up error handling df
    errors = {"File": [], "Time": [], "Error": []}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime("%Y%m%d%H%M")

    # Get station list
    stations = ftp.nlst()
    stations = [k for k in stations if ".txt" not in k]

    if station:
        # Subset by station list provided
        stations = [sta for sta in stations if sta in station]

    for i in stations:
        try:
            ftp.cwd(pwd)
            dir = i + "/"
            if i == "LBH":
                # For LowerBathHouse, Table1-NewObs and TwoMin file appear to span same dates. Grab TwoMin.
                ftp.cwd(dir)
                ftp_to_aws(ftp, "LowerBathHouse_TwoMin.dat", directory)

            else:
                # For all other files, each year has a folder containing a subfolder for each day.
                ftp.cwd(dir)
                files = ftp.nlst()
                # Filter out other files in folder
                years = [x for x in files if len(x) == 4]

                # Get station readme
                ftp_to_aws(ftp, f"{i}_README.txt", directory)

                # Get station data format file
                if "DataFormat.txt" in files:
                    # Not all stations have this
                    ftp_to_aws(
                        ftp,
                        "DataFormat.txt",
                        directory,
                        rename=f"{i}_DataFormat.txt",
                    )

                for k in years:
                    ftp.cwd(k + "/")
                    days = ftp.nlst()
                    # Filter out other files in folder
                    days = [x for x in days if len(x) <= 3]

                    for l in days:
                        try:
                            ftp.cwd(l)
                            files = ftp.nlst()
                            for file in files:
                                ftp_to_aws(ftp, file, directory)
                            ftp.cwd("../")

                        except:
                            try:
                                # attempt to pull the file itself, rename according to CW3E naming convention
                                # setting to .23m as default here
                                ftp_to_aws(
                                    ftp,
                                    l,
                                    directory,
                                    rename=f"{i.lower()}{k[-2:]}{l}.23m",
                                )

                            except Exception as e:
                                # if the file cannot be saved to AWS, just skip and move onto next connection
                                # Useful if this occurs for other files
                                print(
                                    f"File for {l} day in {k} for {i} cannot be retrieved, please check with CW3E FTP server"
                                )
                                errors["File"].append(i)
                                errors["Time"].append(end_api)
                                errors["Error"].append(
                                    f"Error in pulling non-directory file: {e}"
                                )
                                continue
                            continue

                    ftp.cwd("../")

        except Exception as e:
            errors["File"].append(i)
            errors["Time"].append(end_api)
            errors["Error"].append(e)

    # Write errors to csv
    csv_buffer = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer)
    content = csv_buffer.getvalue()
    s3.put_object(
        Bucket=BUCKET_NAME,
        Body=content,
        Key=directory + f"errors_cw3e_{end_api}.csv",
    )

    # close connection
    ftp.quit()

    return None


def get_cw3e_update(
    directory: str,
    station: list[str] | None = None,
    start_date: str | None = None,
    end_date: str | None = None,
):
    """Pull updated CW3E data using FTP. HourlyData_Full files do not include as many
    variables as the individual bytes files, so we download the entire dataset in byte format.

    Parameters
    ----------
    directory : str
        path to save to for network
    station : list[str], optional
        specific station names to return
    start_date : str, optional
        Start date to pull specific subset of data
    end_date : str, optional
        End date to pull specific subset of data

    Returns
    -------
    None
    """

    # Login using ftplib
    ftp = FTP("sioftp.ucsd.edu")
    ftp.login()  # user anonymous, password anonymous
    ftp.cwd("CW3E_DataShare/CW3E_SurfaceMetObs/")
    # Get hourly read me file.
    ftp_to_aws(ftp, "HourlyData_README.txt", directory)
    pwd = ftp.pwd()

    # Set up error handling df
    errors = {"File": [], "Time": [], "Error": []}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime("%Y%m%d%H%M")

    # Set up filtering by time.
    if start_date is None:
        if end_date is None:
            years = range(2018, int(datetime.now().year) + 1)
        else:
            years = range(2018, int(end_date[0:4]) + 1)
            end_day = datetime.strptime(end_date, "%Y-%m-%d").strftime("%j")
    else:
        start_year = int(start_date[0:4])
        start_day = datetime.strptime(start_date, "%Y-%m-%d").strftime("%j")
        if end_date is None:
            years = range(start_year, int(datetime.now().year) + 1)
        else:
            years = range(start_year, int(end_date[0:4]) + 1)
            end_day = datetime.strptime(end_date, "%Y-%m-%d").strftime("%j")

    # Get station list
    stations = ftp.nlst()
    stations = [k for k in stations if ".txt" not in k]

    if station:
        # Subset by station list provided
        stations = [sta for sta in stations if sta in station]

    for i in stations:
        try:
            ftp.cwd(pwd)
            dir = i + "/"
            if i == "LBH":
                # For LowerBathHouse, Table1-NewObs and TwoMin file appear to span same dates. Grab TwoMin.
                ftp.cwd(dir)
                ftp_to_aws(ftp, "LowerBathHouse_TwoMin.dat", directory)
            else:
                # For all other files, each year has a folder containing a subfolder for each day.
                ftp.cwd(dir)
                files = ftp.nlst()
                # Filter out other files in folder
                data_years = [x for x in files if len(x) == 4]

                # Get station readme
                ftp_to_aws(ftp, f"{i}_README.txt", directory)
                # Get station data format file, Not all stations have this
                if "DataFormat.txt" in files:
                    ftp_to_aws(
                        ftp,
                        "DataFormat.txt",
                        directory,
                        rename=f"{i}_DataFormat.txt",
                    )

                for k in data_years:
                    # Filter by years
                    if int(k) in years:
                        # 366 to include leap years
                        days_to_download = list(range(0, 367))
                        if start_date is not None and int(k) == start_year:
                            # Get rid of links before start date.
                            days_to_download = [
                                x for x in days_to_download if x >= int(start_day)
                            ]

                        if end_date is not None and int(k) == int(end_date[0:4]):
                            # Get rid of links after end date.
                            days_to_download = [
                                x for x in days_to_download if x <= int(end_day)
                            ]

                        ftp.cwd(k + "/")
                        days = ftp.nlst()
                        # Filter out other files in folder
                        days = [x for x in days if len(x) <= 3]

                        for l in days:
                            if int(l) in days_to_download:
                                try:
                                    ftp.cwd(l)
                                    files = ftp.nlst()
                                    for file in files:
                                        ftp_to_aws(ftp, file, directory)
                                    ftp.cwd("../")

                                except:
                                    try:
                                        # attempt to pull the file itself, rename according to CW3E naming convention
                                        ftp_to_aws(
                                            ftp,
                                            l,
                                            directory,
                                            rename=f"{i.lower()}{k[-2:]}{l}.23m",
                                        )
                                        # setting to .23m as default here
                                    except Exception as e:
                                        # if the file cannot be saved to AWS, just skip and move onto next connection
                                        print(
                                            f"File for {l} day in {k} for {i} cannot be retrieved, please check with CW3E FTP server"
                                        )
                                        # Useful if this occurs for other files
                                        errors["File"].append(i)
                                        errors["Time"].append(end_api)
                                        errors["Error"].append(
                                            f"Error in pulling non-directory file: {e}"
                                        )
                                        continue
                                    continue

                        # Return to working directory
                        ftp.cwd("../")

        except Exception as e:
            errors["File"].append(i)
            errors["Time"].append(end_api)
            errors["Error"].append(e)

    # Write errors to csv
    csv_buffer = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer)
    content = csv_buffer.getvalue()
    s3.put_object(
        Bucket=BUCKET_NAME,
        Body=content,
        Key=directory + f"errors_cw3e_{end_api}.csv",
    )

    # close connection
    ftp.quit()

    return None


if __name__ == "__main__":
    # To get station list, run:
    get_cw3e_metadata(
        token=config.token,
        terrpath=WECC_TERR,
        marpath=WECC_MAR,
        directory=DIRECTORY,
    )
    # To download all data, run:
    get_cw3e(DIRECTORY)

    # Specific station in all data download
    # ge_cw3e(DIRECTORY, station=["PVN"])
