"""
HADS_pull.py

This script downloads HADS data directly from https://www.ncei.noaa.gov/data/nws-hydrometeorological-automated-data-system/archive/.
Approach:
(1) Get station list (does not need to be re-run constantly)
(2) Download data using station list.

Functions
---------
- get_hads_stations: Pulls in WECC shapefile, returns list of all stations and IDs.
- get_file_links: Takes URL of given website and returns all file download links contained on it.
- link_to_aws: Takes input list of links generated by get_file_links() and saves files to AWS bucket
- get_hads_dat: Query NCEI server for HADS data and download zipped files.
- get_hads_update: Query NCEI server for updated HADS data and download zipped files.

Intended Use
------------ 
Retrieves raw data for an individual network, all variables, all times. Organized by station, with 1 file per day per year.

Notes
-----
1. This function assumes users have configured the AWS CLI such that their access key / secret key pair are stored in ~/.aws/credentials.
See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html for guidance.
"""

import requests
import pandas as pd
from datetime import datetime, timezone
import re
import boto3
from io import StringIO
import geopandas as gp
from shapely.geometry import Point
from geopandas.tools import sjoin
import requests
from bs4 import BeautifulSoup

try:
    from calc_pull import get_wecc_poly
except RuntimeError as e:
    print(f"Error importing calc_pull: {e}")

# Synoptic API keys (obsolete)
try:
    import config  # Import API keys.
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()

s3 = boto3.resource("s3")
s3_cl = boto3.client("s3")  # for lower-level processes
BUCKET_NAME = "wecc-historical-wx"
DIRECTORY = "1_raw_wx/HADS/"
WECC_TERR = (
    "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
)
WECC_MAR = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"


def get_hads_stations(terrpath: str, marpath: str, directory: str) -> pd.DataFrame:
    """Pulls in WECC shapefile, returns list of all stations and IDs.

    Parameters
    ----------
    terrpath : str
        shapefiles for maritime and terrestrial WECC boundaries
    marpath : str
        shapefiles for maritime and terrestrial WECC boundaries
    directory : str
        AWS bucket to save to

    Returns
    -------
    weccstations : pd.DataFrame
        HADS stations within WECC
    """

    # Get list of HADS stations as CSV
    url = "https://hads.ncep.noaa.gov/compressed_defs/all_dcp_defs.txt"
    r = requests.get(url)
    lines = r.content.split(b"\n")
    df = []

    for line in lines:
        row = line.split(b"|")
        row = row[0:10]  # Only keep first 10 columns
        row = [x.decode("utf-8") for x in row]  # Convert to string
        row = list(map(str.strip, row))
        df.append(row)

    # Create pandas dataframe
    stations = pd.DataFrame(
        columns=[
            "GOES NESDIS ID",
            "NWSLI",
            "DCP Owner",
            "State Location",
            "Hydrologic Service Area",
            "Latitude",
            "Longitude",
            "Initial Daily Transmission Time (UTC)",
            "DCP Transmission Interval (Minutes)",
            "DCP Location Name",
        ],
        data=df,
    )
    stations = stations.dropna()

    # Use spatial geometry to only keep points in wecc marine / terrestrial areas.
    stations["Longitude"] = [calc_pull._lon_dms_to_dd(i) for i in stations["Longitude"]]
    stations["Latitude"] = [calc_pull._lat_dms_to_dd(i) for i in stations["Latitude"]]

    # Zip lat lon coords and convert to geodataframe
    geometry = [Point(xy) for xy in zip(stations["Longitude"], stations["Latitude"])]
    weccgeo = gp.GeoDataFrame(stations, crs="EPSG:4326", geometry=geometry)

    # Get bbox of WECC to use to filter stations against
    t, m, bbox = get_wecc_poly(terrpath, marpath)

    # Get terrestrial stations
    # Convert to CRS of terrestrial stations
    weccgeo = weccgeo.to_crs(t.crs)
    # Only keep stations in terrestrial WECC region
    terwecc = sjoin(weccgeo.dropna(), t, how="left")
    terwecc = terwecc.dropna()

    # Get marine stations
    # Only keep stations in marine WECC region
    marwecc = sjoin(weccgeo.dropna(), m, how="left")
    marwecc = marwecc.dropna()

    # Join and remove duplicates using GOES NESDIS ID as combined unique identifier
    weccstations = pd.concat(
        [terwecc, marwecc], ignore_index=True, sort=False
    ).drop_duplicates(["GOES NESDIS ID"], keep="first")

    # Drop columns
    weccstations.drop(
        [
            "OBJECTID_1",
            "OBJECTID",
            "Shape_Leng",
            "geometry",
            "FID_WECC_B",
            "BUFF_DIST",
            "index_right",
        ],
        axis=1,
        inplace=True,
    )

    # Rename in_wecc to in_terr
    weccstations.rename(
        columns={"in_WECC": "in_terr_wecc", "in_marine": "in_mar_wecc"}, inplace=True
    )

    # Write stations to AWS bucket
    wecc_buffer = StringIO()
    weccstations.to_csv(wecc_buffer)
    content = wecc_buffer.getvalue()
    s3_cl.put_object(
        Bucket=BUCKET_NAME, Body=content, Key=directory + "stationlist_HADS.csv"
    )

    return weccstations


def get_file_links(url: str) -> list[tuple[str, str]]:
    """
    Takes URL of given website and returns all file download links contained on it.

    Parameters
    ----------
    url : str
        url path to download files

    Returns
    -------
    links : list[str, str]
        list of download urls and date last modified
    """

    # create response object
    r = requests.get(url)

    # create beautiful-soup object
    soup = BeautifulSoup(r.content, "html.parser")

    # find all links on web-page
    links = soup.findAll("a")

    # filter the link sending with dat.gz
    file_links = [
        url + link["href"] for link in links if link["href"].endswith("dat.gz")
    ]

    # get all 'last modified' dates
    dates = soup.find_all(string=re.compile(r"\d{4}-\d{2}-\d{2}"))
    dates = [i.strip() for i in dates]
    links = list(zip(file_links, dates))

    return links


def link_to_aws(links: list[tuple[str, str]], directory: str):
    """
    Takes input list of links generated by get_file_links() and saves files to AWS bucket

    Parameters
    ----------
    links : list[tuple[str, str]]
        zip list of file links and last modified dates
    directory : str
        path to AWS folder

    Returns
    -------
    None

    Notes
    -----
    # TODO: Update function reflect zipped list. Obtain filename by splitting url and getting last string.
    """

    # iterate through all links and download
    for i, (file_link, dates) in enumerate(links):
        file_name = file_link.split("/")[-1]
        s3_obj = s3.Object(BUCKET_NAME, directory + file_name)

        # create response object
        with requests.get(file_link, stream=True) as r:
            # If API call returns a response
            if r.status_code == 200:
                s3_obj.put(Body=r.content)
                print(f"Saving file {file_name}")

    return None


def get_hads_dat(directory: str, start_date: str | None = None, get_all: bool = True):
    """Query NCEI server for HADS data and download zipped files.

    Parameters
    ----------
    directory : str
        AWS bucket to save files into
    start_date : str, optional
        Optional start date (year) to subset filtering
    get_all : bool
        If false, only download files whose last edit date is newer than the most recent files downloaded in the save folder
        Only use to update a complete set of files

    Returns
    -------
    None
    """

    # Set up error handling
    errors = {"Date": [], "Time": [], "Error": []}
    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime("%Y%m%d%H%M")

    try:
        objects = s3.list_objects(Bucket=BUCKET_NAME, Prefix=directory)
        all = objects["Contents"]

        # Get date of last edited file
        latest = max(all, key=lambda x: x["LastModified"])
        last_edit_time = latest["LastModified"]

        # Get list of all file names
        alreadysaved = []
        for item in all:
            files = item["Key"]
            alreadysaved.append(files)
        alreadysaved = [ele.replace(directory, "") for ele in alreadysaved]

    except:
        # If folder empty or there's an error with the "last downloaded" metadata, redownload all data
        get_all = True

    # Set up filtering by time
    if start_date is None:
        years = range(1997, 2023)

    else:
        # Grab year
        start_date = int(start_date[:4])
        years = range(start_date[:4], 2023)

    for i in years:
        try:
            yearurl = f"https://www.ncei.noaa.gov/data/nws-hydrometeorological-automated-data-system/archive/{str(i)}/"
            links = get_file_links(yearurl)

            if not get_all:
                try:
                    # Get files in AWS folder
                    objects = s3.list_objects(Bucket=BUCKET_NAME, Prefix=directory)
                    all = objects["Contents"]

                    # Get date of last edited file
                    latest = max(all, key=lambda x: x["LastModified"])
                    last_edit_time = latest["LastModified"]

                    # Get list of all file names
                    alreadysaved = []
                    for item in all:
                        files = item["Key"]
                        alreadysaved.append(files)
                    alreadysaved = [ele.replace(directory, "") for ele in alreadysaved]

                    # Filter links by file names already in folder, and files updated since last download
                    link_sub = []
                    date_sub = []
                    for k, (link, date) in enumerate(links):
                        if link.split("/")[-1] in alreadysaved:
                            date = datetime.strptime(date, "%Y-%m-%d %H:%M").replace(
                                tzinfo=timezone.utc
                            )
                            if date > last_edit_time:
                                link_sub.append(link)
                                date_sub.append(date)
                            else:
                                fname = link.split("/")[-1]
                                print(f"{fname} already saved.")
                                continue
                        else:
                            link_sub.append(link)
                            date_sub.append(date)
                    link_sub = list(zip(link_sub, date_sub))
                    link_to_aws(link_sub, directory)

                except:
                    # If folder empty or there's an error with the "last downloaded" metadata, redownload all data
                    get_all = True

            if get_all:
                link_to_aws(links, directory)

        except Exception as e:
            print(f"Error in downloading year {i}: {e}")
            errors["Date"].append(i)
            errors["Time"].append(end_api)
            errors["Error"].append(e)

    # Write errors to csv
    csv_buffer = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer)
    content = csv_buffer.getvalue()
    s3.put_object(
        Bucket=BUCKET_NAME,
        Body=content,
        Key=directory + f"errors_hads_{end_api}.csv",
    )

    return None


def get_hads_update(
    directory: str, start_date: str | None = None, end_date: str | None = None
):
    """Query NCEI server for updated HADS data and download zipped files.

    Parameters
    ----------
    directory : str
        folder path within bucket
    start_date : str, optional
        start date filtering, YYYY-MM-DD
    end_date : str, optional
        end date filtering, YYYY-MM-DD

    Returns
    -------
    None
    """

    # Set up error handling
    errors = {"Date": [], "Time": [], "Error": []}
    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime("%Y%m%d%H%M")

    # Set up filtering by time
    if start_date is None:
        if end_date is None:
            years = range(1997, int(datetime.now().year) + 1)
        else:
            years = range(1997, int(end_date[0:4]) + 1)
            end_day = datetime.strptime(end_date, "%Y-%m-%d").strftime("%j")

    else:
        start_year = int(start_date[0:4])
        start_day = datetime.strptime(start_date, "%Y-%m-%d").strftime("%j")

        if end_date is None:
            years = range(start_year, int(datetime.now().year) + 1)
        else:
            years = range(start_year, int(end_date[0:4]) + 1)
            end_day = datetime.strptime(end_date, "%Y-%m-%d").strftime("%j")

    for i in years:
        try:
            yearurl = f"https://www.ncei.noaa.gov/data/nws-hydrometeorological-automated-data-system/archive/{str(i)}/"
            links = get_file_links(yearurl)

            # Inclusive of leap days
            days_to_download = list(range(0, 367))
            if start_date is not None and i == start_year:
                # Get rid of links before start date
                days_to_download = [x for x in days_to_download if x >= int(start_day)]

            if end_date is not None and i == int(end_date[0:4]):
                # Get rid of links after end date
                days_to_download = [x for x in days_to_download if x <= int(end_day)]

            # Filter file names by day
            link_sub = []
            date_sub = []

            for k, (link, date) in enumerate(links):
                if (
                    int(link.split("/")[-1].replace(".dat.gz", "").split("-")[-1])
                    in days_to_download
                ):
                    link_sub.append(link)
                    date_sub.append(date)

            link_sub = list(zip(link_sub, date_sub))
            link_to_aws(link_sub, directory)

        except Exception as e:
            print(f"Error in downloading year {i}: {e}")
            errors["Date"].append(i)
            errors["Time"].append(end_api)
            errors["Error"].append(e)

    # Write errors to csv
    csv_buffer = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer)
    content = csv_buffer.getvalue()
    s3.put_object(
        Bucket=BUCKET_NAME,
        Body=content,
        Key=directory + f"errors_hads_{end_api}.csv",
    )

    return None


if __name__ == "__main__":
    get_hads_stations(WECC_TERR, WECC_MAR, DIRECTORY)
    get_hads_dat(DIRECTORY, start_date=None, get_all=True)

# Note, for first full data pull, set get_all = True
# For all subsequent data pulls/update with newer data, set get_all = False
