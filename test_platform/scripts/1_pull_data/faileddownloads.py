'''
# This function iterates through all networks, 
# 
# For station-based file systems (SCAN, SNOTEL, MADIS networks),
# it does the following:
# 1) compare station lists to all files and identify any missing station files, attempting to redownload them.
# 2) MADIS only: opens the last line of each file to locate timeout errors, and redownloads from the last timestamp if error found.

# For time-based station files (e.g. ASOSAWOS, OtherISD, MARITIME/NDBC),
# it does the following:
# 1) Read in station lists and file names from AWS
# 2) Compare station lists to 
# 3) ISD-only: using start and end dates, identifies any months of missing data for redownload.
'''

# Import packages
from MADIS_pull import get_madis_station_csv
from SCAN_pull import get_scan_station_data
import boto3
import pandas as pd
import config
from smart_open import open
import os

# Set environment variables
bucket_name = "wecc-historical-wx"
s3 = boto3.resource("s3")  
s3_cl = boto3.client("s3") 

## Define sub-functions
# For MADIS networks
def madis_retry_downloads(token, bucket_name, network):
    # Get list of files in folder
    prefix = "1_raw_wx/"+network
    files = []
    for item in s3.Bucket(bucket_name).objects.filter(Prefix = prefix): 
        file = str(item.key)
        files += [file]
    files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names

    station_file = [file for file in files if "station" in file] # ID station file
    station_file = str(station_file[0])
    files = [file for file in files if "errors" not in file]
    files = [file for file in files if "station" not in file] # Remove error and station list files

    # Get only station IDs from file names
    stations = [file.split("/")[-1] for file in files]
    stations = [file.replace(".csv", '') for file in stations]

    # Read in station list
    station_list = s3_cl.get_object(Bucket= bucket_name, Key= station_file)
    station_list = pd.read_csv(station_list['Body'])

    # Get list of IDs not in download folder
    missed_stations = [id for id in station_list['STID'] if id not in stations]
    missed_ids = station_list[['STID', 'start']] # Format list in way that MADIS_pull script wants it.
    missed_ids = missed_ids[missed_ids.STID.isin(missed_stations)]

    # Check for duplicate stations in station list
    dup_stations = station_list.loc[station_list.duplicated(subset='STID', keep=False)]
    dup_stations = dup_stations[['STID', 'start']]
    dup_todownload = dup_stations.sort_values(by='start', ascending=True).drop_duplicates( # Keep first start date.
    keep='first', subset=['STID'])

    # If any stations duplicated, take the earlier start data and add to the redownload queue.
    download_ids = pd.merge(missed_ids, dup_todownload, how='outer')
    download_ids = download_ids.sort_values(by='start', ascending=True).drop_duplicates(
    keep='first', subset=['STID']) # If download_ids has duplicate STIDs, take first record.

    # Reorganize start date to meet API specs.
    download_ids['start'] = pd.to_datetime(download_ids['start'], format='%Y-%m-%dT%H:%M:%SZ')
    download_ids['start'] = download_ids['start'].dt.strftime('%Y%m%d%H%M')

    # Print list of stations to download.
    if download_ids.empty is False:
        print("Downloading IDs:")
        print(download_ids)
        # Note here we ignore the end date of files, since we will be trimming the last 2 months of data anyways.
        # This could be changed down the road as these dates diverge.
        errors = get_madis_station_csv(token = token, bucket_name = bucket_name, directory = prefix+"/", ids = download_ids, timeout = False)
        
        # Manually print out errors for immediate verification of success. Will also save to AWS.
        print(errors)
    else:
        print("No missing station files. Checking for timeout errors.")
    

# Quality control: if any files return end of file errors, split request into smaller requests and re-run.

# Function: given a file object (e.g. from f.open, smart-open's open function)
# read byte by byte backwards to return the previous line from the current cursor position.
# Adapted so that this only takes data in the form of bytes.
# Inputs: self, a file object generated by an .open() function.
# Function adapted from: https://stackoverflow.com/questions/8721040/python-read-previous-line-and-compare-against-current-line
def before(self):
    # get the current cursor position, then
    # store it in memory.
    current_position = self.tell()
    _tmp = b"" # Specify data in byte form
    _rea = 0 # Set parameter

    while True:
        _rea += 1
        # Move back 1 byte
        self.seek(current_position - _rea)

        # Add the one byte to the string
        _tmp += self.read(1)

        # Move back one byte again
        self.seek(current_position - _rea)

        # If the string in memory contains the "\n"
        # more than once, we will return the string.
        
        # alternatively break if the current position
        # is zero (start of the string).
        if _tmp.count(b"\n") == 2 or self.tell() == 0:
            break

    # Because we're reading backwards, reverse the order and decode into a string.
    return _tmp[::-1].decode()


# Timeout function: Given a list of files, generate URL, read last lines and add to timeout list for re-download if error found.
def get_timeouts(files):
    ids_split = []
    for file in files:
        url = "s3://{}/{}".format(bucket_name, file)
        
        with open(url, 'rb') as f: # Use the open method from smart_open
            try:  # catch OSError in case of a one line file 
                f.seek(-2, os.SEEK_END) # Use seek here, much faster than reading entire file into memory, taken from: https://stackoverflow.com/questions/46258499/how-to-read-the-last-line-of-a-file-in-python
                while f.read(1) != b'\n': 
                    f.seek(-2, os.SEEK_CUR)
            except OSError:
                f.seek(0)
            last_line = f.readline().decode()
            
            #print(last_line) # For testing
            if 'Timeout' in last_line: # If last line is a timeout error, use f.seek to avoid reading entire file into memory.
                print("Timeout error in {}: processing for secondary download.".format(file)) # Useful for testing / progres, can delete.
                # Go backwards one line.
                bytelen = len(last_line) # Get length of last line in bytes
                f.seek(-bytelen, 1) # Move to start of last line.
                prev_line = before(f) # Use function to seek backwards byte by byte, saving file when next '\n' reached.
                
                time = prev_line.split(",")[1] # Get last completed timestamp
                #print(prev_line) # For testing
                station = file.split("/")[-1].replace(".csv", "") # Get station ID.
                ids_split.append([station, time]) # Add to dataframe
            elif 'status' in last_line: # Otherwise, if other errors found in last line (not seen yet)
                print("Timeout error in {}: processing for secondary download.".format(file)) # Useful for testing / progres, can delete.
                # Go backwards one line.
                bytelen = len(last_line) # Get length of last line in bytes
                f.seek(-bytelen, 1) # Move to start of last line.
                prev_line = before(f) # Use function to seek backwards byte by byte, saving file when next '\n' reached.
                #print(prev_line) # For testing.
                    
                # If station ID in previous line, treat as functional end point.
                stid = file.split("/")[-1]
                stid = stid.split("_")[-1] # If there are any prefixes in file name, remove.
                stid = stid.replace(".csv", "")
                #print(stid) # For testing
                
                if stid in prev_line:
                    time = prev_line.split(",")[1] # Get last completed timestamp
                    station = file.split("/")[-1].replace(".csv", "") # Get station ID.
                    ids_split.append([station, time]) # Add to dataframe

                else: # Otherwise, just print an error to console. The last rows will be cleaned in the next stage.
                    print("Error: last lines of {} not parseable.".format(file))
                    continue


    ids_split = pd.DataFrame(ids_split, columns = ['STID', 'start'])
    ids_split['start'] = pd.to_datetime(ids_split['start'], format='%Y-%m-%dT%H:%M:%SZ')
    ids_split['start'] = ids_split['start'].dt.strftime('%Y%m%d%H%M')
    #print(ids_split) # For testing
    return ids_split

# Timeout function part 2: read AWS directory, generate list of files, and iterate through the get_timeouts() script until no more timeout errors found.
def get_madis_station_timeout_csv(token, bucket_name, directory):
    round = 1
    files = []
    for item in s3.Bucket(bucket_name).objects.filter(Prefix = directory): 
        file = str(item.key)
        files += [file]
    files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names
    
    files = [file for file in files if "errors" not in file]
    files = [file for file in files if "station" not in file] # Remove error and station list files
    files = [file for file in files if "2_" not in file] # Only run on primary files first.
    
    # Get list of timeout IDs from primary downloads
    ids_split = get_timeouts(files)

    # Keep running this function as long as needed
    while ids_split.empty is False:
        round += 1 # Count round
        print("Round {} of downloading timeout errors".format(round))
        get_madis_station_csv(token, ids_split, bucket_name, directory,  timeout = True, round = str(round)) # Rerun pull script on timeout files from date of timeout.

        # Check to see if any of the split files needs to be split again.
        # Filter by the round, checking the files just downloaded for other status errors.
        files = []
        for item in s3.Bucket(bucket_name).objects.filter(Prefix = directory+"{}_".format(str(round))): 
            file = str(item.key)
            files += [file]
        timeout_files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names
        #print(timeout_files) # For testing
        ids_split = get_timeouts(timeout_files) # Run on timeout files

    print("No more timeout errors found in {} network".format(directory))


# For SCAN/SNOTEL
def scan_retry_downloads(bucket_name, network, terrpath, marpath):
    prefix = "1_raw_wx/"+network[0]
    files = []
    for item in s3.Bucket(bucket_name).objects.filter(Prefix = prefix): 
        file = str(item.key)
        files += [file]
    files = list(filter(lambda f: f.endswith(".csv"), files)) # Get list of file names

    station_file = [file for file in files if "station" in file] # ID station file
    station_file = str(station_file[0])
    files = [file for file in files if "errors" not in file]
    files = [file for file in files if "station" not in file] # Remove error and station list files

    # Get only station IDs from file names
    stations = [file.split("/")[-1] for file in files]
    stations = [file.replace(".csv", '') for file in stations]
    
    # Read in station list
    station_list = s3_cl.get_object(Bucket= bucket_name, Key= station_file)
    station_list = pd.read_csv(station_list['Body'])
    
    # # Get list of IDs not in download folder
    missed_ids = [id for id in station_list['stationTriplet'] if id not in stations]
    missed_stations = station_list.loc[station_list['stationTriplet'].isin(missed_ids)]
    
    # # Print list of stations to download.
    print(missed_stations)
    
    # # Note here we ignore the end date of files, since we will be trimming the last 2 months of data anyways.
    # # This could be changed down the road as these dates diverge.
    errors = get_scan_station_data(terrpath, marpath, bucket_name, start_date = None, stations = missed_stations, primary = True)

    # # Manually print out errors for immediate verification of success. Will also save to AWS.
    print(errors)


## Define overarching function
# Inputs: madis token, bucket name and network names (as list). If not specified, this runs through all networks.
def retry_downloads(token, bucket_name, networks = None):
    # Set paths to WECC shapefiles in AWS bucket.
    wecc_terr = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
    wecc_mar = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"

    # Define list of all MADIS networks.
    MADIS = ['CAHYDRO', 'CDEC', 'CNRFC', 'CRN', 'CWOP', 'HADS', 'HNXWFO', 'HOLFUY', 'HPWREN', 'LOXWFO', 'MAP', 'MTRWFO', 'NOS-NWLON', 'NOS-PORTS', 'RAWS', 
    'SGXWFO', 'SHASAVAL', 'VCAPCD']

    # Define list of SCAN/SNOTEL networks.
    SNTL = ['SNOTEL', 'SCAN']

    # If network not provided, get list of all networks from AWS bucket and iterate.
    if networks is None:
        print("All networks specified.")
        response = s3_cl.list_objects_v2(Bucket=bucket_name, Prefix = "1_raw_wx/", Delimiter = '/')
        networks = [prefix['Prefix'][:-1].replace("1_raw_wx/", "") for prefix in response['CommonPrefixes']]
        
    for network in networks:
        print("Attempting to download missing files for {} network".format(network))
        directory = "1_raw_wx/"+network+"/"
        if network in MADIS:
            madis_retry_downloads(token, bucket_name, network) # Retry any missing downloads
            # Get timeout CSVs.
            print("Identifying file timeouts for {} network".format(network))
            get_madis_station_timeout_csv(token = token, bucket_name = bucket_name, directory = directory) # Get timeout CSVs.
        elif network in SNTL:
            scan_retry_downloads(bucket_name = bucket_name, network = [network], terrpath = wecc_terr, marpath = wecc_mar)
        else:
            print("{} network not currently configured for download retry.".format(network))
            continue

retry_downloads(token = config.token, bucket_name= bucket_name, networks = ["RAWS"])
# If networks not specified, will attempt all networks (generating list from folders in raw bucket.)

# Helpful networks for testing: 
# HPWREN has many timeout errors
# MTRWFO has a missing file that won't download and throws a "internal error".
# Delete a file from SNOTEL and run on that network.
# Throw a junk name into networks, or run with no network specified.
