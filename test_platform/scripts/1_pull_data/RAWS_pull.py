"""
DEPRACATED as of Nov 8 2022: Use MADIS_pull.py instead.
This script downloads RAWS data from Synoptic.
Approach:
(1) Get station list (does not need to be re-run constantly)
(2) Download data using station list.
Inputs: bucket name in AWS, directory to save file to (folder path), station list (optional), start date of file pull (optional),
parameter to only download changed files (optional)
Outputs: Raw data for an individual network, all variables, all times. Organized by station, with 1 file per year.
"""

## Step 0: Environment set-up
# Import libraries
import requests
import pandas as pd
from datetime import datetime
import re
import boto3
from io import StringIO
import calc_pull

# Set envr variables
s3 = boto3.resource("s3")
s3_cl = boto3.client("s3")  # for lower level processes
bucket_name = "wecc-historical-wx"
directory = "1_raw_wx/RAWS/"

# Set paths to WECC shapefiles in AWS bucket.
wecc_terr = (
    "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
)
wecc_mar = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"

try:
    import config  # Import API keys.
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()


## Step 1: Download data
# Function: return metadata for stations from synoptic API, filtering stations based on bounding box generated by get_wecc_poly.
# Inputs: Synoptic API public token (imported from config.py), path to terrestrial and marine WECC shapefiles relative to home directory.
## bucket name and directory paths to AWS.
# Outputs: Dataframe with list of station IDs and start date of station.
def get_meso_metadata(token, terrpath, marpath, bucket_name, directory):
    try:
        t, m, bbox = calc_pull.get_wecc_poly(terrpath, marpath)
        bbox_api = bbox.loc[0, :].tolist()  # [lonmin,latmin,lonmax,latmax]
        bbox_api = ",".join([str(elem) for elem in bbox_api])
        # Access station metadata to get list of IDs in bbox and network
        # Using: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        # RAWS data network ID in synoptic is 2 (see below)
        url = "https://api.synopticdata.com/v2/stations/metadata?token={}&network=2&bbox={}&recent=20&output=json".format(
            token, bbox_api
        )
        request = requests.get(url).json()
        # print(request)
        ids = []
        station_list = pd.DataFrame(request["STATION"])
        station_list = pd.concat(
            [station_list, station_list["PERIOD_OF_RECORD"].apply(pd.Series)], axis=1
        )  # Split Period of Record column
        station_list = station_list.drop("PERIOD_OF_RECORD", axis=1)
        print(station_list)
        ids = station_list[["STID", "start"]].sort_values(
            "start"
        )  # Sort by start date (note some stations return 'None' here)
        # Reformat date to match API format
        ids["start"] = pd.to_datetime(ids["start"], format="%Y-%m-%dT%H:%M:%SZ")
        ids["start"] = ids["start"].dt.strftime("%Y%m%d%H%M")

        # Save station list to AWS
        csv_buffer_err = StringIO()
        station_list.to_csv(csv_buffer_err)
        content = csv_buffer_err.getvalue()
        networkname = directory.replace(
            "1_raw_wx/", ""
        )  # Get network name from directory name
        networkname = networkname.replace("/", "")
        s3_cl.put_object(
            Bucket=bucket_name,
            Body=content,
            Key=directory + "{}_stationlist.csv".format(networkname),
        )

        return ids
    except Exception as e:
        print("Error: {}".format(e))


# Function: download RAWS station data from Synoptic API.
# Inputs:
# (1) token: Synoptic API token, stored in config.py file
# (2) ids: Takes dataframe with two columns, station ID and startdate. These are the stations to be downloaded, generated from get_meso_metadata.
# (3) bucket_name: name of AWS bucket
# (4) directory: folder path in AWS bucket
# (5) start_date: If none, download data starting on 1980-01-01. Otherwise, download data after start date ("YYYYMMDDHHMM").
# (6) options: timeout = True will identify and download any station data that timed out the API request.
# Outputs: CSV for each station saved to savedir, starting at start date and ending at current date.
def get_raws_station_csv(
    token, ids, bucket_name, directory, start_date=None, **options
):
    # Set up error handling df.
    errors = {"Station ID": [], "Time": [], "Error": []}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime("%Y%m%d%H%M")

    for index, id in ids.iterrows():  # For each station
        # Set start date
        if start_date is None:
            if id["start"] == "NaN" or pd.isna(
                id["start"]
            ):  # Adding error handling for NaN dates.
                start_api = "198001010000"  # If any of the stations in the chunk have a start time of NA, pull data from 01-01-1980 OH:OM and on.
                # To check: all stations seen so far with NaN return empty data. If true universally, can skip these instead of saving.
            else:
                start_api = id["start"]
        else:
            start_api = start_date

        # Generate URL
        # Note: decision here to use full flag suite of MesoWest and Synoptic data.
        # See Data Checks section here for more information: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&stid={}&start={}&end={}&output=csv&qc=on&qc_remove_data=off&qc_flags=on&qc_checks=synopticlabs,mesowest".format(
            token, id["STID"], start_api, end_api
        )

        # Try to get station csv.
        try:
            # request = requests.get(url)
            s3_obj = s3.Object(bucket_name, directory + "{}.csv".format(id["STID"]))

            # If **options timeout = True, save file as STID_2.csv
            if options.get("timeout") == True:
                s3_obj = s3.Object(
                    bucket_name, directory + "{}_2.csv".format(id["STID"])
                )

            with requests.get(url, stream=True) as r:
                if r.status_code == 200:  # If API call returns a response
                    if (
                        "RESPONSE_MESSAGE" in r.text
                    ):  # If error response returned. Note that this is formatted differently depending on error type.
                        # Get error message and clean.
                        error_text = str(
                            re.search("(RESPONSE_MESSAGE.*)", r.text).group(0)
                        )  # Get response message.
                        error_text = re.sub("RESPONSE_MESSAGE.: ", "", error_text)
                        error_text = re.sub(",.*", "", error_text)
                        error_text = re.sub('"', "", error_text)

                        # Append rows to dictionary
                        errors["Station ID"].append(id["STID"])
                        errors["Time"].append(end_api)
                        errors["Error"].append(error_text)
                        next
                    else:
                        s3_obj.put(Body=r.content)
                        print("Saving data for station {}".format(id["STID"]))

                else:
                    errors["Station ID"].append(id["STID"])
                    errors["Time"].append(end_api)
                    errors["Error"].append(r.status_code)
                    print("Error: {}".format(r.status_code))

        except Exception as e:
            print("Error: {}".format(e))

    # Write errors to csv for AWS
    csv_buffer_err = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer_err)
    content = csv_buffer_err.getvalue()
    s3_cl.put_object(
        Bucket=bucket_name,
        Body=content,
        Key=directory + "errors_raws_{}.csv".format(end_api),
    )


# Quality control: if any files return status 408 error, split request into smaller requests and re-run.
# Note: this approach assumes no file will need more than 2 splits. Test this when fuller data downloaded.
def get_raws_station_timeout_csv(token, bucket_name, directory):
    files = []
    for item in s3.Bucket(bucket_name).objects.filter(Prefix=directory):
        file = str(item.key)
        files += [file]
    files = list(filter(lambda f: f.endswith(".csv"), files))  # Get list of file names

    files = [file for file in files if "errors" not in file]
    files = [
        file for file in files if "station" not in file
    ]  # Remove error and station list files

    ids_split = []
    for file in files:
        file = s3_cl.get_object(Bucket=bucket_name, Key=file)  # Open file
        data = file["Body"].read().split(b"\n")  # Read in file
        data = data[-10:]  # Keep last ten rows.
        # Convert to strings.
        for i, val in enumerate(data):
            val = val.decode()
            if "Timeout" in val:
                print("Timeout found in file {}. Queuing for redownload.".format(file))
                lastrealrow = data[i - 1]  # Get last row of recorded data
                station = lastrealrow.split(",")[0]  # Get station ID
                time = lastrealrow.split(",")[1]  # Get last completed timestamp
                ids_split.append([station, time])  # Add to dataframe

    ids_split = pd.DataFrame(ids_split, columns=["STID", "start"])
    ids_split["start"] = pd.to_datetime(ids_split["start"], format="%Y-%m-%dT%H:%M:%SZ")
    ids_split["start"] = ids_split["start"].dt.strftime("%Y%m%d%H%M")
    if ids_split.empty is False:
        get_raws_station_csv(token, bucket_name, directory, ids=ids_split, timeout=True)

        # Check to see if any of the split files needs to be split again.
        for item in s3.Bucket(bucket_name).objects.all():
            files = item.key
        files = list(
            filter(lambda f: f.endswith("_2.csv"), files)
        )  # Get list of file names

        for file in files:
            with open(bucket_name + directory + file, "r") as file:
                data = file.readlines()
                lastRow = data[-1]
                if "Timeout" in lastRow:  # If timeout recorded
                    lastrealrow = data[-2]  # Get last row of recorded data
                    station = lastrealrow.split(",")[0]  # Get station ID
                    time = lastrealrow.split(",")[1]  # Get last completed timestamp
                    ids_split.append([station, time])  # Add to dataframe
                    if ids_split.empty is False:
                        print("Attention!: Run this script again on 2_.csv files.")

    elif ids_split.empty is True:
        return


if __name__ == "__main__":
    # Run script.
    ids = get_meso_metadata(
        token=config.token,
        terrpath=wecc_terr,
        marpath=wecc_mar,
        bucket_name=bucket_name,
        directory=directory,
    )
    get_raws_station_csv(
        token=config.token, bucket_name=bucket_name, directory=directory, ids=ids
    )
    get_raws_station_timeout_csv(
        token=config.token, bucket_name=bucket_name, directory=directory
    )
