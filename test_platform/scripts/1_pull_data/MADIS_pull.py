"""
This script scrapes MADIS network data for ingestion into the Historical Observations Platform via API.
Approach:
Provided a list of network names, madis_pull() calls the following functions.
(1) get_wecc_poly generates a bounding box from WECC shapefiles.
(2) get_madis_metadata produces a list of station IDs filtered by bounding box and network
(3) get madis_station_csv saves a csv for each station ID provided, with the option to select a start date for downloads (defaults to station start date).
Inputs: API key, paths to WECC shapefiles, path to save directory, start date (optional).
Outputs:
(1) Raw data for the selected network, all variables, all times. Organized by station.
(2) An error csv noting all station IDs where downloads failed.
"""

# Step 0: Environment set-up
import requests
import pandas as pd
from datetime import datetime
import re
import boto3
from io import BytesIO, StringIO
import calc_pull
from smart_open import open

## Set AWS credentials
s3 = boto3.resource("s3")
s3_cl = boto3.client('s3') # for lower-level processes
bucket_name = "wecc-historical-wx"


try:
    import config # Import API keys.
except:
    print("Missing config.py file with API token. Make file if necessary.")
    exit()

# Set envr variables
# Set paths to WECC shapefiles in AWS bucket.
wecc_terr = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp"
wecc_mar = "s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp"
raw_path = "1_raw_wx/"

# Function calls Synoptic API to get network metadata and save to AWS.
# Takes Synoptic API token as input.
def get_network_metadata(token):
    
    # Produce table of station metadata from Synoptic API
    r = requests.get("https://api.synopticdata.com/v2/networks?token={}".format(token)).json()
    networks = pd.DataFrame(r['MNET'])
    
    # Sort by number of stations
    networks = networks.sort_values("REPORTING_STATIONS", ascending = False)

    # Save to AWS bucket.
    csv_buffer_err = StringIO()
    networks.to_csv(csv_buffer_err)
    content = csv_buffer_err.getvalue()
    s3_cl.put_object(Bucket=bucket_name, Body=content, Key=raw_path+"madis_network_metadata.csv")
    return networks

# Function: return metadata for stations from synoptic API, filtering stations based on bounding box generated by get_wecc_poly.
# Inputs: Synoptic API public token (imported from config.py), path to terrestrial and marine WECC shapefiles relative to home directory.
## bucket name and directory paths to AWS.
# Outputs: Dataframe with list of station IDs and start date of station.
def get_madis_metadata(token, terrpath, marpath, networkid, bucket_name, directory):
    try:
        t,m,bbox = calc_pull.get_wecc_poly(terrpath, marpath)
        bbox_api = bbox.loc[0,:].tolist() # [lonmin,latmin,lonmax,latmax]
        bbox_api = ','.join([str(elem) for elem in bbox_api])
        # Access station metadata to get list of IDs in bbox and network
        # Using: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = "https://api.synopticdata.com/v2/stations/metadata?token={}&network={}&bbox={}&recent=20&output=json".format(token, networkid, bbox_api)
        request = requests.get(url).json()
        # print(request)
        ids = []
        station_list = pd.DataFrame(request['STATION'])
        station_list = pd.concat([station_list, station_list["PERIOD_OF_RECORD"].apply(pd.Series)], axis=1) # Split Period of Record column
        station_list = station_list.drop("PERIOD_OF_RECORD", axis =1)
        
        ids = station_list[['STID', 'start']].sort_values('start') # Sort by start date (note some stations return 'None' here)
        # Reformat date to match API format
        ids['start'] = pd.to_datetime(ids['start'], format='%Y-%m-%dT%H:%M:%SZ')
        ids['start'] = ids['start'].dt.strftime('%Y%m%d%H%M')

        # Save station list to AWS
        csv_buffer_err = StringIO()
        station_list.to_csv(csv_buffer_err)
        content = csv_buffer_err.getvalue()
        networkname = directory.replace("1_raw_wx/", "") # Get network name from directory name
        networkname = networkname.replace("/", "")

        s3_cl.put_object(Bucket=bucket_name, Body=content, Key=directory+"{}_stationlist.csv".format(networkname))
        
        return ids
    except Exception as e:
        print("Error: {}".format(e))

# Function: download network station data from Synoptic API.
# Inputs:
# (1) token: Synoptic API token, stored in config.py file
# (2) ids: Takes dataframe with two columns, station ID and startdate. These are the stations to be downloaded, generated from get_meso_metadata.
# (3) bucket_name: name of AWS bucket
# (4) directory: folder path in AWS bucket
# (5) start_date: If none, download data starting on 1980-01-01. Otherwise, download data after start date ("YYYYMMDDHHMM").
# (6) options: timeout = True will identify and download any station data that timed out the API request.
# Outputs: CSV for each station saved to savedir, starting at start date and ending at current date.
def get_madis_station_csv(token, ids, bucket_name, directory, start_date = None, **options):

    # Set up error handling df.
    errors = {'Station ID':[], 'Time':[], 'Error':[]}

    # Set end time to be current time at beginning of download
    end_api = datetime.now().strftime('%Y%m%d%H%M')

    for index, id in ids.iterrows(): # For each station
        # Set start date
        if start_date is None:
            if id["start"] == "NaN" or pd.isna(id['start']): # Adding error handling for NaN dates.
                start_api = "198001010000" # If any of the stations in the chunk have a start time of NA, pull data from 01-01-1980 OH:OM and on.
                # To check: all stations seen so far with NaN return empty data. If true universally, can skip these instead of saving.
            else:
                start_api = id["start"] # Otherwise, set start date to start date of station.
                # If start_api starts prior to 1980, set manually to be 1980-01-01.
                if start_api[0:4] < "1980": # If start year prior to 1980
                    start_api = "198001010000"
        else:
            start_api = start_date

        # If station file is 2_/3_ etc., get station name
        if options.get("timeout") == True:
            id['STID'] = id['STID'].split("_")[-1]

        # Generate URL
        # Note: decision here to use full flag suite of MesoWest and Synoptic data.
        # See Data Checks section here for more information: https://developers.synopticdata.com/mesonet/v2/stations/timeseries/
        url = "https://api.synopticdata.com/v2/stations/timeseries?token={}&stid={}&start={}&end={}&output=csv&qc=on&qc_remove_data=off&qc_flags=on&qc_checks=synopticlabs,mesowest".format(token, id['STID'], start_api, end_api)
        # print(url) # For testing.

        # Try to get station csv.
        try:
            #request = requests.get(url)
            s3_obj = s3.Object(bucket_name, directory+"{}.csv".format(id["STID"]))

            # If **options timeout = True, save file as 2_STID.csv
            if options.get("timeout") == True:
<               prefix = options.get("round")
                s3_obj = s3.Object(bucket_name, directory+"{}_{}.csv".format(prefix, id["STID"]))

            with requests.get(url, stream=True) as r:
                if r.status_code == 200: # If API call returns a response
                    if "RESPONSE_MESSAGE" in r.text: # If error response returned. Note that this is formatted differently depending on error type.
                        # Get error message and clean.
                        error_text = str(re.search("(RESPONSE_MESSAGE.*)",r.text).group(0)) # Get response message.
                        error_text = re.sub("RESPONSE_MESSAGE.: ", "", error_text)
                        error_text = re.sub(",.*", "", error_text)
                        error_text = re.sub('"', '', error_text)

                        # Append rows to dictionary
                        errors['Station ID'].append(id['STID'])
                        errors['Time'].append(end_api)
                        errors['Error'].append(error_text)
                        next
                    else:
                        s3_obj.put(Body=r.content)
                        print("Saving data for station {}".format(id["STID"])) # Nice for testing, remove for full run.

                else:
                    errors['Station ID'].append(id['STID'])
                    errors['Time'].append(end_api)
                    errors['Error'].append(r.status_code)
                    print("Error: {}".format(r.status_code))

        except Exception as e:
            print("Error: {}".format(e))

    # Write errors to csv for AWS
    csv_buffer_err = StringIO()
    errors = pd.DataFrame(errors)
    errors.to_csv(csv_buffer_err)
    content = csv_buffer_err.getvalue()
    networkname = directory.replace("1_raw_wx/", "") # Get network name from directory name
    networkname = networkname.replace("/", "")
    s3_cl.put_object(Bucket=bucket_name, Body=content, Key=directory+"errors_{}_{}.csv".format(networkname, end_api))

# Token: Syntopic API token (stored in config.py)
# Networks: expects a list of network names (e.g. ["CWOP", "RAWS"]). If not set, downloads all non-restricted network data.
# Note here this will look for an exact word match in the shortnames (e.g. RAWS will not return NSRAWS but ASOS returns ASOS/AWOS and HF-ASOS)
# Pause: Optional. If True, prompts user to indicate yes to continue before downloading large networks.
## Automatically set to yes when networks is not specified.
def madis_pull(token, networks, pause = None):
    if networks is None: # If no networks provided, pull full list.
        networkdf = get_network_metadata(token)
        networkdf = networkdf[networkdf['TOTAL_RESTRICTED']==0] # Remove restricted networks.
        networkdf = networkdf[['ID', 'SHORTNAME', 'REPORTING_STATIONS']] # Only keep ID, shortname, expected station #s.
        pause = True # Set pause to be true (this will be a large runtime.)
        
    else:
        networkdf = get_network_metadata(token)
        mask = [any([re.search(r'\b' + kw + r'\b', r) for kw in networks]) for r in networkdf['SHORTNAME']]
        networkdf = networkdf[mask]
        resp = input("Networks to be downloaded:{}. Would you like to continue? (Y/N)".format(list(networkdf['SHORTNAME'])))
        if resp == "N":
            return
        if resp == "Y":
            pass
        else:
            print("Invalid response, exiting function.")
            return

    # By network, download data.
    for index, row in networkdf.iterrows(): # For each network.
        print("Downloading data for {} network".format(row['SHORTNAME']))
        if pause: # Set up pause function for large networks.
            if row['REPORTING_STATIONS']>=1000:
                resp = input("Warning: This network contains {} stations. Are you ready to download it? (Y/N)".format(row['REPORTING_STATIONS']))
                if resp == "N":
                    print("Skipping to next network.")
                    continue
                if resp == "Y":
                    pass
                else:
                    print("Invalid response. Skipping network.")
        dirname = row['SHORTNAME'].replace("/", "-") # Get rid of slashes for AWS
        directory = raw_path+dirname+"/" # Use name from syntopic to name folder
        
        # Except if the network is CWOP, then manually set to be CWOP.
        if row['SHORTNAME'] == "APRSWXNET/CWOP":
            dirname = "CWOP"
            directory = raw_path+"CWOP/"
            print(dirname, directory)
        
        # Get list of station IDs and start date.
        #ids = get_madis_metadata(token = config.token, terrpath = wecc_terr, marpath = wecc_mar, networkid = row['ID'], bucket_name = bucket_name, directory = directory)
        
        # Get station CSVs.
        #get_madis_station_csv(token = config.token, bucket_name = bucket_name, directory = directory, ids = ids) # .Sample() subset is for testing(!), remove for full run.

if __name__ == "__main__":    
    madis_pull(config.token, networks = ["CRN"])

