{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f8db7c",
   "metadata": {},
   "source": [
    "## 2_clean/station visualizations\n",
    "\n",
    "The following notebook produces several key items related to all stations that have **cleaned** data:\n",
    "- Updates the primary list of all stations with cleaned data, using station lists available for each network\n",
    "    - Includes cleaned variable coverage for the primary stationlist csv\n",
    "    - Ignores Valley Water stations (code modification on 04/10/25)\n",
    "- Figure 1: station chart showing **actual** station temporal distribution\n",
    "    - This figure represents information from the cleaned station lists, indicating actual station data availability (i.e., # of stations that are successfully cleaned)\n",
    "    - This figure is an update from the `1_pull_stationvisualizer` notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364289ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:18:16.580583Z",
     "iopub.status.busy": "2025-01-28T19:18:16.579868Z",
     "iopub.status.idle": "2025-01-28T19:18:17.461254Z",
     "shell.execute_reply": "2025-01-28T19:18:17.460795Z",
     "shell.execute_reply.started": "2025-01-28T19:18:16.580543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "from io import BytesIO, StringIO\n",
    "from datetime import datetime, timezone, date\n",
    "import geopandas as gpd\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9adb78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:19:13.888297Z",
     "iopub.status.busy": "2025-01-28T19:19:13.887263Z",
     "iopub.status.idle": "2025-01-28T19:19:13.984667Z",
     "shell.execute_reply": "2025-01-28T19:19:13.984203Z",
     "shell.execute_reply.started": "2025-01-28T19:19:13.888252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function: iterate through clean folder and get all station lists\n",
    "def get_station_list_paths(bucket_name, directory):\n",
    "    # Set up variables\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")\n",
    "    get_last_modified = lambda obj: int(\n",
    "        obj[\"LastModified\"].strftime(\"%s\")\n",
    "    )  #  Write method to get last modified file\n",
    "\n",
    "    # Read in all station lists\n",
    "    # Get list of folder prefixes\n",
    "    response = s3_cl.list_objects_v2(\n",
    "        Bucket=bucket_name, Prefix=directory, Delimiter=\"/\"\n",
    "    )\n",
    "\n",
    "    networks = {\"Network\": [], \"NetworkPath\": [], \"StationFile\": []}\n",
    "\n",
    "    for prefix in response[\"CommonPrefixes\"]:  # For each folder path\n",
    "        networkpath = prefix[\"Prefix\"][:-1]\n",
    "        networkname = networkpath.split(\"/\")[-1]\n",
    "        station_file = (\n",
    "            s3.Bucket(bucket_name)\n",
    "            .objects.filter(Prefix=networkpath + \"/\" + \"stationlist_\")\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "        if len(list(station_file)) == 1:  # If one item returned\n",
    "            for item in station_file:  # Get station file\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(item.key)\n",
    "                break  # If more than one file of this format found in folder, just take the most recent\n",
    "\n",
    "        elif len(list(station_file)) == 0:  # If no items found using search above\n",
    "            files = s3.Bucket(bucket_name).objects.filter(\n",
    "                Prefix=networkpath + \"/\"\n",
    "            )  # List all files in folder\n",
    "            file = [\n",
    "                file for file in files if \"station\" in file.key\n",
    "            ]  # More general search for 'station'\n",
    "\n",
    "            # Keep all files found here. These files may be different (e.g. ISD ASOS/AWOS vs ASOS/AWOS station lists)\n",
    "            for item in file:\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(item.key)\n",
    "\n",
    "        # If more than one identically formatted station list returned, take most recent\n",
    "        elif len(list(station_file)) > 1:\n",
    "            # sort station lists by last edit\n",
    "            file_all = [obj.key for obj in station_file]\n",
    "\n",
    "            # need to handle for subsetted CWOP stationfiles\n",
    "            if networkname == \"CWOP\":\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(\n",
    "                    file_all[0]\n",
    "                )  # Add path to first in list -- alphabetical\n",
    "\n",
    "            else:\n",
    "                file = [\n",
    "                    obj.key\n",
    "                    for obj in sorted(\n",
    "                        station_file, key=lambda x: x.last_modified, reverse=True\n",
    "                    )\n",
    "                ]\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(\n",
    "                    file[0]\n",
    "                )  # Add path to most recently changed file\n",
    "\n",
    "    # Note: currently doesn't have a method for dealing with more than one normally formatted station file\n",
    "    return networks\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "def get_station_list(bucket_name, directory):\n",
    "    # Set up variables\n",
    "    # Note these calls are reproduced in get_station_list_paths, but enables to run function separately\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")\n",
    "    dffull = pd.DataFrame()\n",
    "\n",
    "    networks = get_station_list_paths(bucket_name, directory)\n",
    "    networks = pd.DataFrame(networks)\n",
    "\n",
    "    # Remove ASOS/AWOS merge version -- need to confirm what this is\n",
    "    # If station has more than one type of station file (e.g. ASOSAWOS), manually remove the one you dont want to use\n",
    "    remove_list = [\"stationlist_ASOSAWOS_merge.csv\"]\n",
    "    networks = networks[~networks.StationFile.str.contains(\"|\".join(remove_list))]\n",
    "\n",
    "    # Remove all Valley Water stations from list\n",
    "    networks = networks[networks[\"Network\"] != \"VALLEYWATER\"]\n",
    "\n",
    "    # Check that no network has >1 station file and break code if it does.\n",
    "    boolean = networks[\"Network\"].is_unique\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    if boolean is False:  # Tested by commenting out two remove lines above, works.\n",
    "        dupes = networks[\"Network\"].duplicated()\n",
    "        doubles = list(networks[\"Network\"][dupes])\n",
    "        print(\n",
    "            \"Error: More than one station file found for the following networks: {}. \\\n",
    "              Inspect folder, add duplicated files to remove_list and re-run function.\".format(\n",
    "                doubles\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    for index, row in networks.iterrows():\n",
    "        #         try:\n",
    "        # Get data\n",
    "        df = pd.DataFrame()\n",
    "        obj = s3_cl.get_object(Bucket=bucket_name, Key=row[\"StationFile\"])  # get file\n",
    "        body = obj[\"Body\"].read()\n",
    "        temp = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "\n",
    "        try:\n",
    "            networkname = row[\"Network\"]\n",
    "\n",
    "            print(\"Checking for cleaned stations: \", networkname)\n",
    "            total += len(temp)\n",
    "\n",
    "            # Deal with distinct formatting of different station lists.\n",
    "            # Make column names to lower.\n",
    "            temp.columns = temp.columns.str.lower()\n",
    "\n",
    "            # Delete index column\n",
    "            remove = [col for col in temp.columns if \"unnamed\" in col]\n",
    "            if remove is not None:\n",
    "                temp = temp.drop(remove, axis=1)\n",
    "\n",
    "            # Each df should have at least the following columns:\n",
    "            # ERA-ID: specific cleaned station name of network_station\n",
    "            # LAT\n",
    "            # LON\n",
    "            # start-date: date station started running\n",
    "            # end-date: date station stopped running\n",
    "            # Cleaned: Y/N/NA, if station assessed as downloaded by stnlist_update_clean.py\n",
    "            # Time_Cleaned: time of clean check\n",
    "\n",
    "            # Cleaned variable coverage, number of valid observations\n",
    "            # tas_nobs\n",
    "            # tdps_nobs\n",
    "            # tdps_derived_obs\n",
    "            # ps_nobs\n",
    "            # psl_nobs\n",
    "            # ps_altimeter_nobs\n",
    "            # ps_derived_nobs\n",
    "            # pr_nobs\n",
    "            # pr_5min_nobs\n",
    "            # pr_1h_nobs\n",
    "            # pr_25h_nobs\n",
    "            # pr_localmid_nobs\n",
    "            # hurs_nobs\n",
    "            # sfcWind_nobs\n",
    "            # sfcWind_dir_nobs\n",
    "            # rsds_nobs\n",
    "            # total_nobs: total length of data record, including nans\n",
    "\n",
    "            # name: station name, station_name, name, dcp location name --> use name as filter\n",
    "            if any(\"era-id\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"era-id\" in col]\n",
    "                df[\"era-id\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )  # Assign column to df.\n",
    "\n",
    "            # latitude: lat or latitude\n",
    "            if any(\"lat\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"lat\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])\n",
    "                    colname = list(set(colname) - removelist)\n",
    "                    if len(colname) > 1:\n",
    "                        print(\n",
    "                            \"Too many options for lat cols. Add manually to removelist: {}\".format(\n",
    "                                colname\n",
    "                            )\n",
    "                        )\n",
    "                        break\n",
    "                df[\"latitude\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"latitude\"] = np.nan\n",
    "\n",
    "            # longitude: lat or latitude\n",
    "            if any(\"lon\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"lon\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])\n",
    "                    colname = list(set(colname) - removelist)\n",
    "                    if len(colname) > 1:\n",
    "                        print(\n",
    "                            \"Too many options for lon cols. Add manually to removelist: {}\".format(\n",
    "                                colname\n",
    "                            )\n",
    "                        )\n",
    "                        break\n",
    "                df[\"longitude\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"longitude\"] = np.nan\n",
    "\n",
    "            # elevation: elev or elevation (TO DO: convert to same unit!!)\n",
    "            if any(\"elev\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"elev\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set(\n",
    "                        [\"elev(m)\", \"barometer_elev\", \"anemometer_elev\"]\n",
    "                    )  # remove sensor heights\n",
    "                    colname = list(set(colname) - removelist)\n",
    "                    if len(colname) > 1:\n",
    "                        if \"elev_dem\" in colname:\n",
    "                            colname.remove(\"elev_dem\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for elev cols. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"elevation\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"elevation\"] = np.nan\n",
    "\n",
    "            # start-date: search for start, begin or connect\n",
    "            if any(y in x for x in temp.columns for y in [\"begin\", \"start\", \"connect\"]):\n",
    "                colname = [\n",
    "                    col\n",
    "                    for col in temp.columns\n",
    "                    if any(sub in col for sub in [\"begin\", \"start\", \"connect\"])\n",
    "                ]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set(\n",
    "                        [\"startdate\"]\n",
    "                    )  # Add any items to be manually removed here.\n",
    "                    colname = list(set(colname) - removelist)\n",
    "                    if len(colname) > 1:\n",
    "                        # If both start_time (parsed) and begin (not parsed) columns present, remove begin.\n",
    "                        if \"start_time\" in colname:\n",
    "                            if \"begin\" in colname:\n",
    "                                colname.remove(\"begin\")\n",
    "                        if \"disconnect\" in colname:\n",
    "                            colname.remove(\"disconnect\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for start cols. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"start-date\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:  # If no start date provided\n",
    "                df[\"start-date\"] = np.nan\n",
    "\n",
    "            # end-date: search for end or disconnect\n",
    "            if any(y in x for x in temp.columns for y in [\"end\", \"disconnect\"]):\n",
    "                colname = [\n",
    "                    col\n",
    "                    for col in temp.columns\n",
    "                    if any(sub in col for sub in [\"end\", \"disconnect\"])\n",
    "                ]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])  # Add any items to be manually removed here.\n",
    "                    colname = list(set(colname) - removelist)\n",
    "                    if len(colname) > 1:\n",
    "                        # If both start_time (parsed) and begin (not parsed) columns present, remove begin.\n",
    "                        if \"end_time\" in colname:\n",
    "                            if \"end\" in colname:\n",
    "                                colname.remove(\"end\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for end cols. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"end-date\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:  # If no start date provided\n",
    "                df[\"end-date\"] = np.nan\n",
    "\n",
    "            # Add pulled and date checked columns, if they exist\n",
    "            if any(\"cleaned\" in str for str in temp.columns):\n",
    "                df[\"cleaned\"] = temp[\"cleaned\"].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"cleaned\"] = np.nan\n",
    "\n",
    "            if any(\"time_cleaned\" in str for str in temp.columns):\n",
    "                df[\"time_cleaned\"] = temp[\"time_cleaned\"].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"time_cleaned\"] = np.nan\n",
    "\n",
    "            # Make network column\n",
    "            df[\"network\"] = networkname\n",
    "\n",
    "            ## Cleaned variable coverage\n",
    "            cleaned_vars = [\n",
    "                \"tas_nobs\",\n",
    "                \"tdps_nobs\",\n",
    "                \"tdps_derived_nobs\",\n",
    "                \"ps_nobs\",\n",
    "                \"ps_derived_nobs\",\n",
    "                \"psl_nobs\",\n",
    "                \"ps_altimeter_nobs\",\n",
    "                \"pr_nobs\",\n",
    "                \"pr_5min_nobs\",\n",
    "                \"pr_1h_nobs\",\n",
    "                \"pr_24h_nobs\",\n",
    "                \"pr_localmid_nobs\",\n",
    "                \"hurs_nobs\",\n",
    "                \"sfcwind_nobs\",\n",
    "                \"sfcwind_dir_nobs\",\n",
    "                \"rsds_nobs\",\n",
    "                \"total_nobs\",\n",
    "            ]\n",
    "\n",
    "            for clean_var in cleaned_vars:\n",
    "                if any(clean_var in str for str in temp.columns):\n",
    "                    df[clean_var] = temp[clean_var].values.reshape(\n",
    "                        -1,\n",
    "                    )\n",
    "                else:\n",
    "                    df[clean_var] = (\n",
    "                        np.nan\n",
    "                    )  # will be redundant once all networks have var coverage updated\n",
    "\n",
    "            dffull = pd.concat([dffull, df], sort=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Organize full dataframe\n",
    "    # If end date is \"active\", make this be today's date.\n",
    "    today = datetime.now()\n",
    "    dffull[\"end-date\"] = dffull[\"end-date\"].replace(\"Active\", today)\n",
    "\n",
    "    # Format dates in datetime format.\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # Drop empty rows - lat/lon as min criteria for inclusion\n",
    "    dffull.dropna(subset=[\"latitude\"], inplace=True)\n",
    "    dffull.dropna(subset=[\"longitude\"], inplace=True)\n",
    "\n",
    "    # Remove any duplicates (of network and ID)\n",
    "    dffull.sort_values(\n",
    "        by=[\"start-date\"], ascending=True, inplace=True\n",
    "    )  #  oldest network is always first\n",
    "    dffull.drop_duplicates(\n",
    "        subset=[\"era-id\", \"latitude\", \"longitude\", \"network\"], inplace=True\n",
    "    )\n",
    "\n",
    "    # Print some useful statistics\n",
    "    num_allstns = len(dffull)\n",
    "    num_cleanstns = dffull[\"cleaned\"].value_counts()[\"Y\"]\n",
    "    print(\"Number of cleaned stations: \", num_cleanstns)\n",
    "    print(\n",
    "        \"Successful clean station rate: {}%\".format((num_cleanstns / num_allstns) * 100)\n",
    "    )\n",
    "\n",
    "    # Resort by network\n",
    "    dffull.sort_values(by=[\"network\"], inplace=True)\n",
    "\n",
    "    # Reset index.\n",
    "    dffull = dffull.reset_index(drop=True)\n",
    "\n",
    "    # Save station chart to AWS\n",
    "    csv_buffer = StringIO()\n",
    "    dffull.to_csv(csv_buffer, na_rep=\"NaN\")\n",
    "    content = csv_buffer.getvalue()\n",
    "    s3_cl.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Body=content,\n",
    "        Key=\"2_clean_wx/temp_clean_all_station_list.csv\",\n",
    "    )\n",
    "\n",
    "    return dffull\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# Function: plot station chart\n",
    "# update = True, if you want to regenerate the primary (all) station list first\n",
    "# update = False, function pulls the existing file from AWS\n",
    "def get_station_chart(bucket_name, directory, update=False):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "    if update == False:\n",
    "        obj = s3_cl.get_object(\n",
    "            Bucket=bucket_name, Key=\"2_clean_wx/temp_clean_all_station_list.csv\"\n",
    "        )\n",
    "        body = obj[\"Body\"].read()\n",
    "        dffull = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "    elif update == True:\n",
    "        dffull = get_station_list(bucket_name, directory)\n",
    "\n",
    "    # Get period\n",
    "    # Format dates in datetime format (this gets lost in import).\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # Fix nas\n",
    "    # Filter out rows w/o start date\n",
    "    # Note here: we lose MARITIME and NDBC networks\n",
    "    # print(dffull[dffull['network']==\"MARITIME\"])\n",
    "    subdf = dffull.loc[~dffull[\"start-date\"].isnull()].copy()\n",
    "\n",
    "    # Filter out non-cleaned rows\n",
    "    subdf = subdf.loc[subdf[\"cleaned\"] != \"N\"].copy()\n",
    "\n",
    "    # Manually filter dates to >01-01-1980 and <today.\n",
    "    # Timezones so far ignored here but we presume on the scale of month we can safely ignore them for the moment\n",
    "    # Note!: This implicitly assumes stations w/o end date run until present\n",
    "    subdf[\"start-date\"] = subdf[\"start-date\"].apply(\n",
    "        lambda x: (\n",
    "            x\n",
    "            if x > datetime(1980, 1, 1, tzinfo=timezone.utc)\n",
    "            else datetime(1980, 1, 1, tzinfo=timezone.utc)\n",
    "        )\n",
    "    )\n",
    "    subdf[\"end-date\"] = subdf[\"end-date\"].apply(\n",
    "        lambda x: (\n",
    "            x\n",
    "            if x < datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "            else datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get period of months for range of dates for each station\n",
    "    subdf[\"period\"] = [\n",
    "        pd.period_range(*v, freq=\"M\")\n",
    "        for v in zip(subdf[\"start-date\"], subdf[\"end-date\"])\n",
    "    ]\n",
    "\n",
    "    subdf = subdf[subdf.period.str.len() > 0]\n",
    "    subdf = subdf.reset_index(drop=True)\n",
    "\n",
    "    out = subdf.explode(\"period\").pivot_table(\n",
    "        values=\"era-id\",\n",
    "        index=\"network\",\n",
    "        columns=\"period\",\n",
    "        aggfunc=\"count\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    # out.columns = out.columns.strftime('%b-%y')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42ef69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:19:20.897258Z",
     "iopub.status.busy": "2025-01-28T19:19:20.896534Z",
     "iopub.status.idle": "2025-01-28T19:20:03.083585Z",
     "shell.execute_reply": "2025-01-28T19:20:03.083110Z",
     "shell.execute_reply.started": "2025-01-28T19:19:20.897217Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate station chart\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "directory = \"2_clean_wx/\"\n",
    "\n",
    "station_list = get_station_list(bucket_name, directory)  # Get combined station list\n",
    "out = get_station_chart(\n",
    "    bucket_name, directory, update=True\n",
    ")  # set update=True to regenerate all stations list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1aa52",
   "metadata": {},
   "source": [
    "## Figure: cleaned stations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f765b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:23:55.073530Z",
     "iopub.status.busy": "2025-01-28T19:23:55.072642Z",
     "iopub.status.idle": "2025-01-28T19:23:57.200902Z",
     "shell.execute_reply": "2025-01-28T19:23:57.200501Z",
     "shell.execute_reply.started": "2025-01-28T19:23:55.073479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "outt = out.T.reset_index()\n",
    "\n",
    "# Fix time component\n",
    "outt[\"date\"] = outt[\"period\"].astype(str)\n",
    "outt[\"date\"] = pd.to_datetime(outt[\"date\"])\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "# Subplot parameters\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "outt.plot.area(\n",
    "    x=\"date\",\n",
    "    title=\"Cleaned stations by network over time\",\n",
    "    ax=ax,\n",
    "    x_compat=True,\n",
    "    cmap=\"tab20c_r\",\n",
    ")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.0, 0.5))  # Fix legend\n",
    "ax.tick_params(labelcolor=\"black\", labelsize=\"medium\", width=3)\n",
    "ax.set_facecolor(\"w\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of stations\")\n",
    "\n",
    "# Change axis bounds\n",
    "ax.set_xlim([date(1980, 1, 1), date(2022, 8, 1)])\n",
    "\n",
    "# Change tick marks\n",
    "ax.minorticks_on()\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.YearLocator(3))\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_minor_locator(matplotlib.dates.YearLocator(1))\n",
    "\n",
    "# Change y ticks\n",
    "plt.locator_params(axis=\"y\", nbins=12)\n",
    "ax.yaxis.get_ticklocs(minor=True)\n",
    "\n",
    "# Set x axis labels\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, top=0.8, right=0.8)\n",
    "\n",
    "# Annotate text for total number\n",
    "# hard coding the number for now, come back to this\n",
    "plt.annotate(\n",
    "    \"Total # of cleaned stations: 15,991\", xy=(0.025, 0.95), xycoords=\"axes fraction\"\n",
    ")\n",
    "\n",
    "# Save to AWS\n",
    "img_data = BytesIO()\n",
    "plt.savefig(img_data, format=\"png\")\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.put_object(\n",
    "    Body=img_data,\n",
    "    ContentType=\"image/png\",\n",
    "    Key=\"2_clean_wx/clean_stations_over_time.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac158d0",
   "metadata": {},
   "source": [
    "#### Fig 2-3 Update from spatial maps of pulled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b8768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-28T19:24:04.414457Z",
     "iopub.status.busy": "2025-01-28T19:24:04.413589Z",
     "iopub.status.idle": "2025-01-28T19:24:26.599636Z",
     "shell.execute_reply": "2025-01-28T19:24:26.599196Z",
     "shell.execute_reply.started": "2025-01-28T19:24:04.414411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run function - generate station map\n",
    "def get_station_map(bucket_name, directory, shapepath, update=False):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")\n",
    "    if update == False:\n",
    "        obj = s3_cl.get_object(\n",
    "            Bucket=bucket_name, Key=\"2_clean_wx/temp_clean_all_station_list.csv\"\n",
    "        )\n",
    "        body = obj[\"Body\"].read()\n",
    "        dffull = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "    elif update == True:\n",
    "        dffull = get_station_list(bucket_name, directory)\n",
    "\n",
    "    # Format dates in datetime format (this gets lost in import).\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    #     # Quality control (optional -- uncomment next 3 lines of code if desired)\n",
    "    #     # Filter out rows w/o start date - this will remove NDBC, MARITIME, CW3E networks (no date coverage in stn list)\n",
    "    #     subdf = dffull.loc[~dffull['start-date'].isnull()].copy()\n",
    "    #     # Filter out rows without data between 1980 and now.\n",
    "    #     subdf = subdf.loc[(subdf['start-date']<=datetime.utcnow().replace(tzinfo=timezone.utc))\n",
    "    #                       & (subdf['end-date']>='1980-01-01')]\n",
    "    subdf = dffull\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    # Make a geodataframe.\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        subdf, geometry=gpd.points_from_xy(subdf.longitude, subdf.latitude)\n",
    "    )\n",
    "    gdf.set_crs(epsg=4326, inplace=True)  # Set CRS\n",
    "\n",
    "    # Project data to match base tiles.\n",
    "    gdf_wm = gdf.to_crs(epsg=3857)  # Web mercator\n",
    "\n",
    "    # Read in geometry of continental US.\n",
    "    us = gpd.read_file(shapepath)\n",
    "\n",
    "    # Remove territories, AK, HI\n",
    "    rem_list = [\"HI\", \"AK\", \"MP\", \"GU\", \"AS\", \"PR\", \"VI\"]\n",
    "    us = us.loc[us.STUSPS.isin(rem_list) == False]\n",
    "\n",
    "    # Use to clip stations\n",
    "    us = us.to_crs(epsg=3857)\n",
    "    gdf_us = gdf_wm.clip(us)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    # Version 1 - full map\n",
    "    ax = gdf_us.plot(\n",
    "        \"network\",\n",
    "        figsize=(15, 15),\n",
    "        alpha=1,\n",
    "        markersize=3,\n",
    "        legend=True,\n",
    "        cmap=\"nipy_spectral\",\n",
    "    )\n",
    "    cx.add_basemap(ax, source=cx.providers.CartoDB.Positron)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Save to AWS\n",
    "    img_data = BytesIO()\n",
    "    plt.savefig(img_data, format=\"png\")\n",
    "    img_data.seek(0)\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.put_object(\n",
    "        Body=img_data, ContentType=\"image/png\", Key=\"2_clean_wx/clean_station_map.png\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------\n",
    "    # Version 2 - only big networks\n",
    "    # Sort stations by number of networks\n",
    "    gdf_us[\"network_count\"] = gdf_us.groupby(\"network\")[\"network\"].transform(\n",
    "        \"count\"\n",
    "    )  # Add network count column.\n",
    "\n",
    "    # If <100 stations, change to \"misc\"\n",
    "    gdf_us.loc[gdf_us[\"network_count\"] < 100, \"network\"] = \"Misc\"\n",
    "\n",
    "    # Plot\n",
    "    ax = gdf_us.plot(\n",
    "        \"network\",\n",
    "        figsize=(15, 15),\n",
    "        alpha=1,\n",
    "        markersize=3,\n",
    "        legend=True,\n",
    "        cmap=\"nipy_spectral\",\n",
    "    )\n",
    "    cx.add_basemap(ax, source=cx.providers.CartoDB.Positron)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Save to AWS\n",
    "    img_data = BytesIO()\n",
    "    plt.savefig(img_data, format=\"png\")\n",
    "    img_data.seek(0)\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.put_object(\n",
    "        Body=img_data,\n",
    "        ContentType=\"image/png\",\n",
    "        Key=\"2_clean_wx/clean_station_map_min.png\",\n",
    "    )\n",
    "\n",
    "shapepath = \"s3://wecc-historical-wx/0_maps/tl_2021_us_state\"\n",
    "get_station_map(bucket_name, directory, shapepath=shapepath, update=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
