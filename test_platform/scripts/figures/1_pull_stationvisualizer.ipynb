{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script updates the primary list of all stations with raw data, using the station lists available for each network. Then, this data is used to make a map and a chart showing station distribution over time. Note that this reflects the information provided in station lists, and not actual station data availability (i.e., this figure should be re-made following the cleaning stage to reflect dropped stations and the actual temporal availability of data). Stations without time information in their station lists are not reflected in the time-based chart, but are included in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries, \n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib\n",
    "from io import BytesIO, StringIO\n",
    "from datetime import datetime, timezone, date\n",
    "import geopandas as gpd\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: iterate through raw folder and get all station lists\n",
    "def get_station_list_paths(bucket_name, directory):\n",
    "    # Set up variables\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "    get_last_modified = lambda obj: int(\n",
    "        obj[\"LastModified\"].strftime(\"%s\")\n",
    "    )  #  Write method to get last modified file\n",
    "\n",
    "    # Read in all station lists.\n",
    "    # Get list of folder prefixes\n",
    "    response = s3_cl.list_objects_v2(\n",
    "        Bucket=bucket_name, Prefix=directory, Delimiter=\"/\"\n",
    "    )\n",
    "\n",
    "    networks = {\"Network\": [], \"NetworkPath\": [], \"StationFile\": []}\n",
    "\n",
    "    for prefix in response[\"CommonPrefixes\"]:  # For each folder path\n",
    "        networkpath = prefix[\"Prefix\"][:-1]\n",
    "        networkname = networkpath.split(\"/\")[-1]\n",
    "        station_file = (\n",
    "            s3.Bucket(bucket_name)\n",
    "            .objects.filter(Prefix=networkpath + \"/\" + \"stationlist_\")\n",
    "            .all()\n",
    "        )\n",
    "        if len(list(station_file)) == 1:  # If one item returned\n",
    "            for item in station_file:  # Get station file\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(item.key)\n",
    "                break  # If more than one file of this format found in folder, just take the most recent.\n",
    "        elif len(list(station_file)) == 0:  # If no items found using search above\n",
    "            files = s3.Bucket(bucket_name).objects.filter(\n",
    "                Prefix=networkpath + \"/\"\n",
    "            )  # List all files in folder\n",
    "            file = [\n",
    "                file for file in files if \"station\" in file.key\n",
    "            ]  # More general search for 'station'\n",
    "            for (\n",
    "                item\n",
    "            ) in (\n",
    "                file\n",
    "            ):  # Keep all files found here. These files may be different (e.g. ISD ASOS/AWOS vs ASOS/AWOS station lists)\n",
    "                networks[\"Network\"].append(networkname)\n",
    "                networks[\"NetworkPath\"].append(networkpath)\n",
    "                networks[\"StationFile\"].append(item.key)\n",
    "        elif (\n",
    "            len(list(station_file)) > 1\n",
    "        ):  # If more than one identically formatted station list returned (shouldn't happen), take most recent\n",
    "            file = [\n",
    "                obj.key\n",
    "                for obj in sorted(\n",
    "                    station_file, key=lambda x: x.last_modified, reverse=True\n",
    "                )\n",
    "            ]  # Sort station files by most recent edit\n",
    "            networks[\"Network\"].append(networkname)\n",
    "            networks[\"NetworkPath\"].append(networkpath)\n",
    "            networks[\"StationFile\"].append(\n",
    "                file[0]\n",
    "            )  # Add path to most recently changed file. (Tested and works)\n",
    "        # Note method currently doesn't have a method for dealing with more than one normally formatted station file\n",
    "    return networks\n",
    "\n",
    "\n",
    "def get_station_list(bucket_name, directory):\n",
    "    # Set up variables\n",
    "    s3 = boto3.resource(\n",
    "        \"s3\"\n",
    "    )  # Note these calls are reproduced in get_station_list_paths, but this enables us to run that function separately.\n",
    "    s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "    dffull = pd.DataFrame()\n",
    "\n",
    "    networks = get_station_list_paths(bucket_name, directory)\n",
    "    networks = pd.DataFrame(networks)\n",
    "\n",
    "    # Remove ASOS/AWOS, madis and isd lists to prevent duplicates\n",
    "    # note: for station list (all stations), you'll probably want to KEEP the asosawos_stations and remove isd_asosawos or merge both!\n",
    "    # we do this here because the ISD station list has start/end dates, and asosawos does not.\n",
    "    # If station has more than one type of station file (e.g. ASOSAWOS), manually remove the one you don't want to use here.\n",
    "    remove_list = [\n",
    "        \"/stationlist_ASOSAWOS.csv\",\n",
    "        \"/isd_stations.csv\",\n",
    "        \"madis_stations.csv\",\n",
    "    ]  # / used to keep otherisd_stations.csv\n",
    "    networks = networks[~networks.StationFile.str.contains(\"|\".join(remove_list))]\n",
    "\n",
    "    # Check that no network has >1 station file and break code if it does.\n",
    "    boolean = networks[\"Network\"].is_unique\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    # print(boolean)\n",
    "    if boolean is False:  # Tested by commenting out two remove lines above, works.\n",
    "        dupes = networks[\"Network\"].duplicated()\n",
    "        doubles = list(networks[\"Network\"][dupes])\n",
    "        print(\n",
    "            \"Error: More than one station file found for the following networks: {}. Inspect folder, add duplicated files to remove_list and re-run function.\".format(\n",
    "                doubles\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    for index, row in networks.iterrows():\n",
    "        try:\n",
    "            # print(row['StationFile'])\n",
    "            # Get data\n",
    "            df = pd.DataFrame()\n",
    "            obj = s3_cl.get_object(\n",
    "                Bucket=bucket_name, Key=row[\"StationFile\"]\n",
    "            )  # Get file using StationFile as path.\n",
    "            body = obj[\"Body\"].read()\n",
    "            temp = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "\n",
    "        except Exception as e:  # If there's an encoding error when reading in file\n",
    "            # print(\"Error parsing station file for {}: {}\".format(row['Network'], e))\n",
    "            try:\n",
    "                if \"xlsx\" in row[\"StationFile\"]:  # If file is .xlsx file (CIMIS)\n",
    "                    temp = pd.read_excel(\n",
    "                        BytesIO(body), engine=\"openpyxl\"\n",
    "                    )  # Use different pandas function to read in.\n",
    "            except Exception as e:  # If there's an encoding error when reading in file\n",
    "                print(\"Error parsing station file for {}: {}\".format(row[\"Network\"], e))\n",
    "\n",
    "        try:\n",
    "            networkname = row[\"Network\"]\n",
    "\n",
    "            print(networkname, len(temp))\n",
    "            total += len(temp)\n",
    "            print(total)\n",
    "\n",
    "            # Deal with distinct formatting of different station lists.\n",
    "            # Make column names to lower.\n",
    "            temp.columns = temp.columns.str.lower()\n",
    "\n",
    "            # Delete index column\n",
    "            remove = [col for col in temp.columns if \"unnamed\" in col]\n",
    "            if remove is not None:\n",
    "                temp = temp.drop(remove, axis=1)\n",
    "\n",
    "            # Each df should have 6 columns.\n",
    "            # network: network name.\n",
    "            # name: station name\n",
    "            # latitude\n",
    "            # longitude\n",
    "            # start-date: date station started running\n",
    "            # end-date: date station stopped running\n",
    "            # pulled: Y/N/NA, if station assessed as downloaded by pull_qa.py\n",
    "            # time_checked: time of pull check.\n",
    "\n",
    "            # # name: station name, station_name, name, dcp location name --> use name as filter.\n",
    "            if any(\"name\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"name\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([\"countyname\"])\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        print(\n",
    "                            \"Too many options for station name columns. Add manually to removelist: {}\".format(\n",
    "                                colname\n",
    "                            )\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "                # print(temp[colname])\n",
    "                df[\"name\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )  # Assign column to df.\n",
    "                # print(df)\n",
    "\n",
    "            # latitude: lat or latitude\n",
    "            if any(\"lat\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"lat\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        print(\n",
    "                            \"Too many options for latitude columns. Add manually to removelist: {}\".format(\n",
    "                                colname\n",
    "                            )\n",
    "                        )\n",
    "                        break\n",
    "                df[\"latitude\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"latitude\"] = np.nan\n",
    "\n",
    "            # longitude: lat or latitude\n",
    "            if any(\"lon\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"lon\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        print(\n",
    "                            \"Too many options for longitude columns. Add manually to removelist: {}\".format(\n",
    "                                colname\n",
    "                            )\n",
    "                        )\n",
    "                        break\n",
    "                df[\"longitude\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"longitude\"] = np.nan\n",
    "\n",
    "            # elevation: elev or elevation (TO DO: convert to same unit!!)\n",
    "            if any(\"elev\" in str for str in temp.columns):\n",
    "                colname = [col for col in temp.columns if \"elev\" in col]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        if \"elev_dem\" in colname:\n",
    "                            colname.remove(\"elev_dem\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for elevation columns. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"elevation\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"elevation\"] = np.nan\n",
    "\n",
    "            # # start-date: search for start, begin or connect\n",
    "            if any(y in x for x in temp.columns for y in [\"begin\", \"start\", \"connect\"]):\n",
    "                colname = [\n",
    "                    col\n",
    "                    for col in temp.columns\n",
    "                    if any(sub in col for sub in [\"begin\", \"start\", \"connect\"])\n",
    "                ]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])  # Add any items to be manually removed here.\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        if (\n",
    "                            \"start_time\" in colname\n",
    "                        ):  # If both start_time (parsed) and begin (not parsed) columns present, remove begin.\n",
    "                            if \"begin\" in colname:\n",
    "                                colname.remove(\"begin\")\n",
    "                        if \"disconnect\" in colname:\n",
    "                            colname.remove(\"disconnect\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for start date columns. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"start-date\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:  # If no start date provided\n",
    "                df[\"start-date\"] = np.nan\n",
    "\n",
    "            # # end-date: search for end or disconnect\n",
    "            if any(y in x for x in temp.columns for y in [\"end\", \"disconnect\"]):\n",
    "                colname = [\n",
    "                    col\n",
    "                    for col in temp.columns\n",
    "                    if any(sub in col for sub in [\"end\", \"disconnect\"])\n",
    "                ]\n",
    "                if len(colname) > 1:  # If more than one col returned\n",
    "                    removelist = set([])  # Add any items to be manually removed here.\n",
    "                    colname = list(\n",
    "                        set(colname) - removelist\n",
    "                    )  # Use sets to exclude partial matches (e.g. 'name' in 'countyname')\n",
    "                    if len(colname) > 1:\n",
    "                        if (\n",
    "                            \"end_time\" in colname\n",
    "                        ):  # If both start_time (parsed) and begin (not parsed) columns present, remove begin.\n",
    "                            if \"end\" in colname:\n",
    "                                colname.remove(\"end\")\n",
    "                        if len(colname) > 1:\n",
    "                            print(\n",
    "                                \"Too many options for end date columns. Add manually to removelist: {}\".format(\n",
    "                                    colname\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "                df[\"end-date\"] = temp[colname].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:  # If no start date provided\n",
    "                df[\"end-date\"] = np.nan\n",
    "\n",
    "            # Add pulled and date checked columns, if they exist\n",
    "            if any(\"pulled\" in str for str in temp.columns):\n",
    "                df[\"pulled\"] = temp[\"pulled\"].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"pulled\"] = np.nan\n",
    "\n",
    "            if any(\"time_checked\" in str for str in temp.columns):\n",
    "                df[\"time_checked\"] = temp[\"time_checked\"].values.reshape(\n",
    "                    -1,\n",
    "                )\n",
    "            else:\n",
    "                df[\"time_checked\"] = np.nan\n",
    "\n",
    "            # Make network column\n",
    "            df[\"network\"] = networkname\n",
    "\n",
    "            dffull = pd.concat([dffull, df], sort=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Organize full dataframe.\n",
    "    # If end date is \"active\", make this be today's date.\n",
    "    today = datetime.now()\n",
    "\n",
    "    print(len(dffull))\n",
    "    dffull[\"end-date\"] = dffull[\"end-date\"].replace(\"Active\", today)\n",
    "\n",
    "    # Format dates in datetime format.\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # Drop empty rows - lat/lon as min criteria for inclusion\n",
    "    dffull.dropna(subset=[\"latitude\"], inplace=True)\n",
    "    print(len(dffull))\n",
    "    dffull.dropna(subset=[\"longitude\"], inplace=True)\n",
    "    print(len(dffull))\n",
    "    # Remove any duplicates (of network and ID)\n",
    "    dffull.sort_values(\n",
    "        by=[\"start-date\"], ascending=True, inplace=True\n",
    "    )  # Sort by network and time so oldest network is always first\n",
    "    dffull.drop_duplicates(\n",
    "        subset=[\"name\", \"latitude\", \"longitude\", \"network\"], inplace=True\n",
    "    )\n",
    "    print(len(dffull))\n",
    "    # Resort by network\n",
    "    dffull.sort_values(by=[\"network\"], inplace=True)\n",
    "\n",
    "    # Reset index.\n",
    "    dffull = dffull.reset_index(drop=True)\n",
    "\n",
    "    # Save station chart to AWS\n",
    "    csv_buffer = StringIO()\n",
    "    dffull.to_csv(csv_buffer, na_rep=\"NaN\")\n",
    "    content = csv_buffer.getvalue()\n",
    "    s3_cl.put_object(\n",
    "        Bucket=bucket_name, Body=content, Key=\"1_raw_wx/temp_pull_all_station_list.csv\"\n",
    "    )\n",
    "\n",
    "    return dffull\n",
    "\n",
    "\n",
    "# Function: plot station chart\n",
    "# Set update = True if you want to regenerate the primary station list, otherwise function pulls the existing file from AWS.\n",
    "def get_station_chart(bucket_name, directory, update=False):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "    if update == False:\n",
    "        obj = s3_cl.get_object(\n",
    "            Bucket=bucket_name, Key=\"1_raw_wx/temp_pull_all_station_list.csv\"\n",
    "        )\n",
    "        body = obj[\"Body\"].read()\n",
    "        dffull = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "    elif update == True:\n",
    "        dffull = get_station_list(bucket_name, directory)\n",
    "\n",
    "    # Get period\n",
    "\n",
    "    # Format dates in datetime format (this gets lost in import).\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # Fix nas\n",
    "    ## Filter out rows w/o start date\n",
    "    ## Note here: we lose MARITIME and NDBC networks.\n",
    "    # print(dffull[dffull['network']==\"MARITIME\"])\n",
    "    subdf = dffull.loc[~dffull[\"start-date\"].isnull()].copy()\n",
    "\n",
    "    ## Filter out non-downloaded rows\n",
    "    subdf = subdf.loc[subdf[\"pulled\"] != \"N\"].copy()\n",
    "\n",
    "    # manually filter dates to >01-01-1980 and <today.\n",
    "    # Timezones so far ignored here but we presume on the scale of month we can safely ignore them for the moment.\n",
    "    # Note!: This implicitly assumes stations w/o end date run until present.\n",
    "    subdf[\"start-date\"] = subdf[\"start-date\"].apply(\n",
    "        lambda x: (\n",
    "            x\n",
    "            if x > datetime(1980, 1, 1, tzinfo=timezone.utc)\n",
    "            else datetime(1980, 1, 1, tzinfo=timezone.utc)\n",
    "        )\n",
    "    )\n",
    "    subdf[\"end-date\"] = subdf[\"end-date\"].apply(\n",
    "        lambda x: (\n",
    "            x\n",
    "            if x < datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "            else datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get period of months for range of dates for each station\n",
    "    subdf[\"period\"] = [\n",
    "        pd.period_range(*v, freq=\"M\")\n",
    "        for v in zip(subdf[\"start-date\"], subdf[\"end-date\"])\n",
    "    ]\n",
    "\n",
    "    subdf = subdf[subdf.period.str.len() > 0]\n",
    "    subdf = subdf.reset_index(drop=True)\n",
    "\n",
    "    out = subdf.explode(\"period\").pivot_table(\n",
    "        values=\"name\", index=\"network\", columns=\"period\", aggfunc=\"count\", fill_value=0\n",
    "    )\n",
    "    # out.columns = out.columns.strftime('%b-%y')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run functions - generate station chart\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "directory = \"1_raw_wx/\"\n",
    "\n",
    "station_list = get_station_list(bucket_name, directory)  # Get combined station list.\n",
    "out = get_station_chart(bucket_name, directory, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "outt = out.T.reset_index()\n",
    "\n",
    "# Fix time component\n",
    "outt[\"date\"] = outt[\"period\"].astype(str)\n",
    "outt[\"date\"] = pd.to_datetime(outt[\"date\"])\n",
    "\n",
    "# Plot parameters\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "# Subplot parameters\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "outt.plot.area(\n",
    "    x=\"date\",\n",
    "    title=\"Pulled stations by network over time\",\n",
    "    ax=ax,\n",
    "    x_compat=True,\n",
    "    cmap=\"tab20c_r\",\n",
    ")  # Get area plot\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.0, 0.5))  # Fix legend\n",
    "ax.tick_params(labelcolor=\"black\", labelsize=\"medium\", width=3)\n",
    "ax.set_facecolor(\"w\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of stations\")\n",
    "\n",
    "# Change axis bounds\n",
    "ax.set_xlim([date(1980, 1, 1), date(2022, 8, 1)])\n",
    "\n",
    "# Change tick marks\n",
    "ax.minorticks_on()\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.YearLocator(3))\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y\"))\n",
    "ax.xaxis.set_minor_locator(matplotlib.dates.YearLocator(1))\n",
    "\n",
    "# Change y ticks\n",
    "plt.locator_params(axis=\"y\", nbins=12)\n",
    "ax.yaxis.get_ticklocs(minor=True)\n",
    "\n",
    "# Set x axis labels\n",
    "# #plt.title(\"Stations Over Time By Network\")\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2, top=0.8, right=0.8)\n",
    "\n",
    "# Annotate with number of stations\n",
    "plt.annotate(\n",
    "    \"Total # of pulled stations: 16,079\", xy=(0.025, 0.95), xycoords=\"axes fraction\"\n",
    ")\n",
    "\n",
    "# Save to AWS\n",
    "img_data = BytesIO()\n",
    "plt.savefig(img_data, format=\"png\")\n",
    "img_data.seek(0)\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.put_object(\n",
    "    Body=img_data, ContentType=\"image/png\", Key=\"1_raw_wx/pull_stations_over_time.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run function - generate station map\n",
    "\n",
    "\n",
    "def get_station_map(bucket_name, directory, shapepath, update=False):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "    if update == False:\n",
    "        obj = s3_cl.get_object(\n",
    "            Bucket=bucket_name, Key=\"1_raw_wx/temp_pull_all_station_list.csv\"\n",
    "        )\n",
    "        body = obj[\"Body\"].read()\n",
    "        dffull = pd.read_csv(BytesIO(body), encoding=\"utf8\")\n",
    "    elif update == True:\n",
    "        dffull = get_station_list(bucket_name, directory)\n",
    "\n",
    "    # Get period\n",
    "\n",
    "    # Format dates in datetime format (this gets lost in import).\n",
    "    dffull[\"start-date\"] = pd.to_datetime(dffull[\"start-date\"], utc=True)\n",
    "    dffull[\"end-date\"] = pd.to_datetime(dffull[\"end-date\"], utc=True)\n",
    "\n",
    "    # Quality control.\n",
    "    # Fix nas\n",
    "    ## Filter out rows w/o start date\n",
    "    #     subdf = dffull.loc[~dffull['start-date'].isnull()].copy()\n",
    "    # Filter out rows without data between 1980 and now.\n",
    "    #     subdf = subdf.loc[(subdf['start-date']<=datetime.utcnow().replace(tzinfo=timezone.utc)) & (subdf['end-date']>='1980-01-01')]\n",
    "\n",
    "    subdf = dffull\n",
    "\n",
    "    # Make a geodataframe.\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        subdf, geometry=gpd.points_from_xy(subdf.longitude, subdf.latitude)\n",
    "    )\n",
    "    gdf.set_crs(epsg=4326, inplace=True)  # Set CRS\n",
    "\n",
    "    # Project data to match base tiles.\n",
    "    gdf_wm = gdf.to_crs(epsg=3857)  # Web mercator\n",
    "\n",
    "    # Read in geometry of continental US.\n",
    "    us = gpd.read_file(shapepath)\n",
    "\n",
    "    # Remove territories, AK, HI\n",
    "    rem_list = [\"HI\", \"AK\", \"MP\", \"GU\", \"AS\", \"PR\", \"VI\"]\n",
    "    us = us.loc[us.STUSPS.isin(rem_list) == False]\n",
    "\n",
    "    # Use to clip stations\n",
    "    us = us.to_crs(epsg=3857)\n",
    "    gdf_us = gdf_wm.clip(us)\n",
    "\n",
    "    # Version 1 - full map\n",
    "    ax = gdf_us.plot(\n",
    "        \"network\",\n",
    "        figsize=(15, 15),\n",
    "        alpha=1,\n",
    "        markersize=3,\n",
    "        legend=True,\n",
    "        cmap=\"nipy_spectral\",\n",
    "    )\n",
    "    cx.add_basemap(ax, source=cx.providers.Stamen.TonerLite)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Save to AWS\n",
    "    img_data = BytesIO()\n",
    "    plt.savefig(img_data, format=\"png\")\n",
    "    img_data.seek(0)\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.put_object(\n",
    "        Body=img_data, ContentType=\"image/png\", Key=\"1_raw_wx/pull_station_map.png\"\n",
    "    )\n",
    "\n",
    "\n",
    "#     # Version 2\n",
    "#     # If <100 stations, change to misc\n",
    "#     # Sort stations by number of networks\n",
    "#     gdf_us['network_count'] = gdf_us.groupby('network')['network'].transform('count') # Add network count column.\n",
    "\n",
    "#     gdf_us.loc[gdf_us['network_count'] < 100, 'network'] = \"Misc\"\n",
    "\n",
    "#     # Plot\n",
    "#     ax = gdf_us.plot(\"network\", figsize=(15, 15), alpha=1, markersize = 3, legend = True, cmap = 'nipy_spectral')\n",
    "#     cx.add_basemap(ax, source=cx.providers.Stamen.TonerLite)\n",
    "#     ax.set_axis_off()\n",
    "\n",
    "#     # Save to AWS\n",
    "#     img_data = BytesIO()\n",
    "#     plt.savefig(img_data, format='png')\n",
    "#     img_data.seek(0)\n",
    "\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     bucket = s3.Bucket(bucket_name)\n",
    "#     bucket.put_object(Body=img_data, ContentType='image/png', Key=\"1_raw_wx/pull_station_map_min.png\")\n",
    "\n",
    "shapepath = \"s3://wecc-historical-wx/0_maps/tl_2021_us_state.shp\"\n",
    "get_station_map(bucket_name, directory, shapepath=shapepath, update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
