{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24263655-e41b-4a21-89a4-c257218966b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T17:48:40.598634Z",
     "iopub.status.busy": "2024-10-08T17:48:40.597905Z",
     "iopub.status.idle": "2024-10-08T17:48:40.610572Z",
     "shell.execute_reply": "2024-10-08T17:48:40.609081Z",
     "shell.execute_reply.started": "2024-10-08T17:48:40.598586Z"
    }
   },
   "source": [
    "# Find weather stations that set flags\n",
    "From the AWS data catalog of station data, find the subset of weather stations that set flags \n",
    "<br>Output a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf3a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import s3fs\n",
    "import tempfile # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "import time # Used for progress bar \n",
    "import sys # Used for progress bar \n",
    "\n",
    "# Silence runtime warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ded0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout): # Python3.6+\n",
    "    \"\"\"\n",
    "    Print a progress bar to console \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://stackoverflow.com/questions/3160699/python-progress-bar\n",
    "    \n",
    "    \"\"\"\n",
    "    count = len(it)\n",
    "    start = time.time() # time estimate start\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        # time estimate calculation and string\n",
    "        remaining = ((time.time() - start) / j) * (count - j)        \n",
    "        mins, sec = divmod(remaining, 60) # limited to minutes\n",
    "        time_str = f\"{int(mins):02}:{sec:03.1f}\"\n",
    "        print(f\"{prefix}[{u'â–ˆ'*x}{('.'*(size-x))}] {j}/{count} Est wait {time_str}\", end='\\r', file=out, flush=True)\n",
    "    show(0.1) # avoid div/0 \n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i+1)\n",
    "    print(\"\\n\", flush=True, file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2009d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we used zarr, this wouldn't be neccessary \n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir): \n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a list of weather stations \n",
    "# We just use the names of the stations to filter \n",
    "# Ideally I'd like to see this moved to the AWS bucket \n",
    "train_stns = pd.read_csv('../qaqc_training_station_list_events.csv')\n",
    "train_stns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fa761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str \n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    station_data: xr.Dataset \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file \n",
    "    I'd like to see us use a zarr workflow if possible to avoid this. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir = temp_dir, \n",
    "        prefix = \"\", \n",
    "        suffix = \".nc\",\n",
    "        delete = True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system \n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = 's3://wecc-historical-wx/3_qaqc_wx_dev/{}/{}.nc'.format(network_name, station_id)\n",
    "\n",
    "    # Read in the data using xarray \n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine='h5netcdf').load()\n",
    "\n",
    "    # Close temporary file \n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37908e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_flags(station_ds): \n",
    "    \"\"\"Find unique flags in a Dataset.\n",
    "    Filters through flag variables; assumes flag variables contain the substring '_eraqc'\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    station_ds: xr.Dataset \n",
    "        Dataset containing station data, with each variable as a unique data variable \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    unique_flags: list or None\n",
    "        List of unique flag values found in station_ds \n",
    "        Returns None if no flags found for any variable \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the string names of the eraqc flag variables \n",
    "    era_flag_var_names = [var for var in station_ds.data_vars if '_eraqc' in var]\n",
    "\n",
    "    # Subset Dataset to just contain flag variables \n",
    "    station_flags_ds = station_ds[era_flag_var_names]\n",
    "\n",
    "    # Check for nulls in the flag variables \n",
    "    # If all are null, that means the station set no flags for this event! \n",
    "    all_null = station_flags_ds.to_array().isnull().all().item()\n",
    "    unique_flags = None\n",
    "\n",
    "    # If there are some flags set, find out what they are \n",
    "    if not all_null: \n",
    "        # Stack all the variables, since we don't care which variables the flags belong to \n",
    "        # Makes it easier to search the array (fewer lines of code :)\n",
    "        # Works like np.flatten, but on an xarray object \n",
    "        stacked = station_flags_ds.to_array().stack({\"everything\":[\"variable\",\"time\"]})\n",
    "\n",
    "        # Drop all non-null values \n",
    "        # i.e. [nan, nan, 23, nan, 17] --> [23, 17]\n",
    "        all_flags = stacked.where(~stacked.isnull(), drop=True)\n",
    "\n",
    "        # Get unique flag values as integers\n",
    "        # nan is treated as a float so the .isnull() step converted floats to ints (I think)\n",
    "        # i.e. [23.0, 23.0, 23.0, 17.0, 23.0, 19.0, 19.0] --> [23, 17, 19]\n",
    "        unique_flags = list(np.unique(all_flags.values).astype(int))\n",
    "\n",
    "    return unique_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc48585",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_start_date = \"2007-10-20\"\n",
    "event_end_date = \"2007-10-24\"\n",
    "stn_subset = train_stns[:5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stations_with_flags = {\"network\":[],\"era-id\":[],\"flags\":[]}\n",
    "\n",
    "# Loop through each station to look for flags \n",
    "for i in progressbar(range(len(stn_subset))):\n",
    "    # Get info for one station \n",
    "    network_name, station_id = stn_subset.iloc[i][[\"network\",\"era-id\"]]\n",
    "\n",
    "    # Read in the data from AWS as an xarray Dataset \n",
    "    station_ds = read_nc_from_s3(\n",
    "        network_name=network_name, \n",
    "        station_id=station_id, \n",
    "        temp_dir=temp_dir\n",
    "        )\n",
    "\n",
    "    # Reduce dimension of object \n",
    "    # \"station\" is a singleton dimension\n",
    "    station_ds = station_ds.squeeze()\n",
    "\n",
    "    # Subset Dataset to event time period \n",
    "    station_ds = station_ds.sel(time=slice(event_start_date, event_end_date))\n",
    "\n",
    "    unique_flags = find_flags(station_ds)\n",
    "\n",
    "    if unique_flags is not None: \n",
    "        stations_with_flags[\"network\"].append(network_name) \n",
    "        stations_with_flags[\"era-id\"].append(station_id)\n",
    "        stations_with_flags[\"flags\"].append(unique_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stations_with_flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
