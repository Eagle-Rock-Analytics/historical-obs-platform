{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c073d99",
   "metadata": {},
   "source": [
    "# Generate summed flag count tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb392abe",
   "metadata": {},
   "source": [
    "This notebook creates QAQC flag counts csv files per network from the corresponding eraqc_counts_timestep files that were generated as a part of the final processing step for stations within the Historical Data Pipeline. These tables are used to then generate statistics for the QAQC success report.\n",
    "\n",
    "This is carried out in two steps:\n",
    "\n",
    "1. Generate the per-network QAQC flag count tables, at native and hourly timesteps\n",
    "\n",
    "2. Generates one flag count table that sums all per-network tables, at native and hourly timesteps\n",
    "\n",
    "\n",
    "Using the following functions:\n",
    "\n",
    "\n",
    "- _pairwise_sum(): helper function that merges two input flag tables, used by network_sum_flag_counts() and total_sum_flag_counts().\n",
    "\n",
    "- network_sum_flag_counts(): sums all station flag count tables for a given network, creating one flag count table for that network\n",
    "\n",
    "- generate_station_tables(): runs network_sum_flag_counts() for every network\n",
    "\n",
    "- total_sum_flag_counts(): sums all network flag count tables, creating one final flag count table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce425205",
   "metadata": {},
   "source": [
    "## Step 0: Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e2b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "from functools import reduce\n",
    "\n",
    "import inspect\n",
    "\n",
    "import logging\n",
    "# Create a simple logger that just prints to the console\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7320ab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "# Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "stations_csv_path = f\"s3://{bucket_name}/2_clean_wx/temp_clean_all_station_list.csv\"\n",
    "qaqc_dir = \"3_qaqc_wx\"\n",
    "merge_dir = \"4_merge_wx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7594f1f",
   "metadata": {},
   "source": [
    "### The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8c45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pairwise_sum(flag_df_1, flag_df_2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sums two input flag count dataframes. This is a helper function for sum_flag_counts(). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flag_df_1: pd.DataFrame\n",
    "        dataframe of previously summed station flag counts\n",
    "    flag_df_2: pd.DataFrame\n",
    "        flag counts dataframes for next station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summed_df: pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    if len(flag_df_1) == 0:\n",
    "        return flag_df_2\n",
    "    else:\n",
    "        total_df = pd.concat([flag_df_1, flag_df_2])\n",
    "\n",
    "        summed_df = total_df.groupby('eraqc_flag_values', as_index=False).sum()\n",
    "        return summed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a695a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_sum_flag_counts(network: str, timestep: str) -> None:\n",
    "    \"\"\"\n",
    "    Sums all station QAQC flag counts in a network for a given timestep (hourly or native) and sends to AWS. \n",
    "    These counts are used to generate statistics for the QAQC success report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network: str\n",
    "        network name\n",
    "    timestep: str\n",
    "        if set to 'hourly', merge all hourly QAQC flag count tables\n",
    "        if set to 'native', merge all native timestep QAQC flag count tables\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    ## Setup\n",
    "\n",
    "    # read in flag meanings CSV\n",
    "\n",
    "    flag_meanings = pd.read_csv(\"era_qaqc_flag_meanings.csv\")\n",
    "\n",
    "    # only run for a valid \"timestep\" input\n",
    "    if timestep not in (\"hourly\", \"native\"):\n",
    "        print(\"invalid timestep: \", timestep)\n",
    "        return None\n",
    "\n",
    "    # the function iteratively adds in flag counts to this dataframe\n",
    "    summed_counts_df = []\n",
    "\n",
    "    # point to folder containing station flag count CSVs\n",
    "    flags_prefix = f\"{merge_dir}/{network}/eraqc_counts_{timestep}_timestep\"\n",
    "\n",
    "    ## Merge flag counts\n",
    "\n",
    "    # loop through all CSVs are the given level\n",
    "    for item in s3.Bucket(bucket_name).objects.filter(Prefix=flags_prefix):\n",
    "        obj = s3_cl.get_object(Bucket=bucket_name, Key=item.key)\n",
    "        flags = pd.read_csv(obj[\"Body\"])\n",
    "        # the CSV is empty\n",
    "        if flags.empty:\n",
    "            continue\n",
    "        # the CSV is not empty\n",
    "        else:\n",
    "            # send current dataframe and dataframe of previously summed counts to helper function\n",
    "            summed_counts_df = _pairwise_sum(summed_counts_df, flags)\n",
    "\n",
    "    ## Format final dataframe\n",
    "\n",
    "    # merge with flag meaning dataframe\n",
    "    # - to include all flag values, and their meanings\n",
    "    \n",
    "\n",
    "    # Reorder flag values in numerical order\n",
    "    summed_counts_df = summed_counts_df.sort_values(by=\"eraqc_flag_values\")\n",
    "\n",
    "    ## Send final counts file to AWS as CSV\n",
    "\n",
    "    csv_s3_filepath = f\"s3://wecc-historical-wx/4_merge_wx/per_network_flag_counts/{network}_flag_counts_{timestep}_timestep.csv\"\n",
    "    # summed_counts_df.to_csv(csv_s3_filepath, index=False)\n",
    "    print(\n",
    "        f\"Sending summed counts dataframe for {network} to: {csv_s3_filepath}\"\n",
    "    )\n",
    "\n",
    "    return summed_counts_df  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594cf721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_flag_counts(timestep: str) -> None:\n",
    "    \"\"\"\n",
    "    Sums all network-level QAQC flag counts for a given timestep (hourly or native) and sends to AWS. \n",
    "    These counts are used to generate statistics for the QAQC success report.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestep: str\n",
    "        if set to 'hourly', merge all hourly QAQC flag count tables\n",
    "        if set to 'native', merge all native timestep QAQC flag count tables\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    ## Setup\n",
    "\n",
    "    # only run for a valid \"timestep\" input\n",
    "    if timestep not in (\"hourly\", \"native\"):\n",
    "        print(\"invalid timestep: \", timestep)\n",
    "        return None\n",
    "\n",
    "    # the function iteratively adds in flag counts to this dataframe\n",
    "    summed_counts_df = []\n",
    "\n",
    "    # point to folder containing network-level flag count CSVs\n",
    "    flags_prefix = f\"{merge_dir}/per_network_flag_counts\"\n",
    "\n",
    "    ## Merge flag counts\n",
    "\n",
    "    # loop through all networks CSVs\n",
    "    for item in s3.Bucket(bucket_name).objects.filter(Prefix=flags_prefix):\n",
    "        obj = s3_cl.get_object(Bucket=bucket_name, Key=item.key)\n",
    "        flags = pd.read_csv(obj[\"Body\"])\n",
    "        # the CSV is empty\n",
    "        if flags.empty:\n",
    "            continue\n",
    "        # the CSV is not empty\n",
    "        else:\n",
    "            # send current dataframe and dataframe of previously summed counts to helper function\n",
    "            summed_counts_df = _pairwise_sum(summed_counts_df, flags)\n",
    "\n",
    "    ## Send final counts file to AWS as CSV\n",
    "    if len(summed_counts_df) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        csv_s3_filepath = (\n",
    "            f\"s3://wecc-historical-wx/4_merge_wx/total_flag_counts_{timestep}_timestep.csv\"\n",
    "        )\n",
    "        # summed_counts_df.to_csv(csv_s3_filepath, index=False)\n",
    "        print(f\"Sending final summed counts dataframe for to: {csv_s3_filepath}\")\n",
    "\n",
    "        return summed_counts_df  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd33d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_station_tables(timestep: str) -> None:\n",
    "    \"\"\"\n",
    "    Runs network_sum_flag_counts() for every network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestep: str\n",
    "        if set to 'hourly', merge all hourly QAQC flag count tables\n",
    "        if set to 'native', merge all native timestep QAQC flag count tables\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # only run for a valid \"timestep\" input\n",
    "    if timestep not in (\"hourly\", \"native\"):\n",
    "        print(\"invalid timestep: \", timestep)\n",
    "        return None\n",
    "\n",
    "    station_list = pd.read_csv(stations_csv_path)\n",
    "    network_list = station_list[\"network\"].unique()\n",
    "\n",
    "    for network in network_list:\n",
    "        network_sum_flag_counts(network, timestep)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b4f2e",
   "metadata": {},
   "source": [
    "## Step 1: Generate flag sum tables for ever network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47140f",
   "metadata": {},
   "source": [
    "First, loop through every network, combining each of their station flag count tables into one table. The result is one flag count table at each timestep - native and hourly - for every network.\n",
    "\n",
    "This will take around 1 hour to run for both timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_station_tables('native')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_station_tables(\"hourly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb0332",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25dfb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \"CDEC\"\n",
    "station = \"CDEC_PVP\"\n",
    "timestep = 'native'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018fd430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending summed counts dataframe for CDEC to: s3://wecc-historical-wx/4_merge_wx/per_network_flag_counts/CDEC_flag_counts_native_timestep.csv\n"
     ]
    }
   ],
   "source": [
    "result = network_sum_flag_counts(network, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f051c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_meanings = pd.read_csv('era_qaqc_flag_meanings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f16ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the flag values in the count tables were saved as strings, so we need to convert to string to merge\n",
    "\n",
    "flag_meanings['Flag_value'] = flag_meanings['Flag_value'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82083e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflag_meanings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/pandas/core/frame.py:10083\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10064\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10065\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10066\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10079\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10080\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10081\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10084\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10092\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10093\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10097\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/pandas/core/reshape/merge.py:111\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 111\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/pandas/core/reshape/merge.py:688\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;66;03m# stacklevel chosen to be correct when this is reached via pd.merge\u001b[39;00m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;66;03m# (and not DataFrame.join)\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    685\u001b[0m         msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[1;32m    686\u001b[0m     )\n\u001b[0;32m--> 688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_left_right_on\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m cross_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1437\u001b[0m, in \u001b[0;36m_MergeOperation._validate_left_right_on\u001b[0;34m(self, left_on, right_on)\u001b[0m\n\u001b[1;32m   1435\u001b[0m common_cols \u001b[38;5;241m=\u001b[39m left_cols\u001b[38;5;241m.\u001b[39mintersection(right_cols)\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(common_cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo common columns to perform merge on. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1439\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge options: left_on=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft_on\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_on=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright_on\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_index=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1443\u001b[0m     )\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m left_cols\u001b[38;5;241m.\u001b[39mjoin(common_cols, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mis_unique\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_cols\u001b[38;5;241m.\u001b[39mjoin(common_cols, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mis_unique\n\u001b[1;32m   1447\u001b[0m ):\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData columns not unique: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(common_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMergeError\u001b[0m: No common columns to perform merge on. Merge options: left_on=None, right_on=None, left_index=False, right_index=False"
     ]
    }
   ],
   "source": [
    "test = result.merge(flag_meanings,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5cd3d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eraqc_flag_values</th>\n",
       "      <th>accum_pr</th>\n",
       "      <th>elevation</th>\n",
       "      <th>pr</th>\n",
       "      <th>hurs</th>\n",
       "      <th>ps_altimeter</th>\n",
       "      <th>ps_derived</th>\n",
       "      <th>ps</th>\n",
       "      <th>rsds</th>\n",
       "      <th>sfcWind_dir</th>\n",
       "      <th>sfcWind</th>\n",
       "      <th>tas</th>\n",
       "      <th>tdps_derived</th>\n",
       "      <th>Flag_value</th>\n",
       "      <th>QAQC_function</th>\n",
       "      <th>Flag_meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>525855.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>qaqc_deaccumulate_precip</td>\n",
       "      <td>Original precipitation data; deaccumulation pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eraqc_flag_values  accum_pr  elevation   pr  hurs  ps_altimeter  ps_derived  \\\n",
       "0                35  525855.0          0  0.0   0.0           0.0         0.0   \n",
       "\n",
       "    ps  rsds  sfcWind_dir  sfcWind  tas  tdps_derived Flag_value  \\\n",
       "0  0.0   0.0          0.0      0.0  0.0           0.0         35   \n",
       "\n",
       "              QAQC_function                                       Flag_meaning  \n",
       "0  qaqc_deaccumulate_precip  Original precipitation data; deaccumulation pr...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a8aa6",
   "metadata": {},
   "source": [
    "## Step 2: Generate total flag sum table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ad66c",
   "metadata": {},
   "source": [
    "Now combine all the network flag count tables generated in step 1 into one final flag count table. First at the hourly timestep, and then at the native timestep.\n",
    "\n",
    "Step 1 must be complete before moving on to this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_flag_counts('hourly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa00dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_flag_counts(\"native\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
