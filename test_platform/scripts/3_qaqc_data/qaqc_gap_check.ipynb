{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# necessary for using for loop ending at last day of month\n",
    "import calendar\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "# New logger function\n",
    "from log_config import logger\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = (\n",
    "    \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    ")\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\"\n",
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)\n",
    "def open_log_file_merge(file):\n",
    "    global log_file\n",
    "    log_file = file\n",
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "# -----------------------------------------------------------------------------\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "# -----------------------------------------------------------------------------\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data\n",
    "# -----------------------------------------------------------------------------\n",
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def printf(*args, verbose=True, log_file=None, **kwargs):\n",
    "    import datetime\n",
    "\n",
    "    tLog = lambda: datetime.datetime.utcnow().strftime(\"%m-%d-%Y %H:%M:%S\") + \" : \\t\"\n",
    "    args = [str(a) for a in args]\n",
    "\n",
    "    if verbose:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs) or print(\n",
    "                \" \".join([tLog(), *args]), file=log_file, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs)\n",
    "    else:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), file=log_file, **kwargs)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era_qc existing variables:\n",
      "[]\n",
      "nans created for ps_eraqc\n",
      "nans created for tas_eraqc\n",
      "nans created for tdps_eraqc\n",
      "nans created for pr_eraqc\n",
      "nans created for sfcWind_eraqc\n",
      "nans created for sfcWind_dir_eraqc\n",
      "nans created for elevation_eraqc\n",
      "nans created for ps_altimeter_eraqc\n",
      "nans created for psl_eraqc\n",
      "9 created era_qc variables\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3(\"ASOSAWOS\", \"ASOSAWOS_72494523293\", temp_dir)\n",
    "# [\"ASOSAWOS_74948400395\", \"ASOSAWOS_74509023244\", \"ASOSAWOS_72494523293\"]\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pr  pr_eraqc\n",
      "time                      \n",
      "2016-01-01   0.0       0.0\n",
      "2016-01-02   0.0       0.0\n",
      "2016-01-03   0.0       0.0\n",
      "2016-01-04   0.3       0.0\n",
      "2016-01-05  28.4       0.0\n",
      "...          ...       ...\n",
      "2016-12-27   0.0       0.0\n",
      "2016-12-28   0.0       0.0\n",
      "2016-12-29   0.0       0.0\n",
      "2016-12-30   0.0       0.0\n",
      "2016-12-31   0.0       0.0\n",
      "\n",
      "[366 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## filter by year and precipitation variable \n",
    "\n",
    "year = 2016\n",
    "var = 'pr'\n",
    "new_df = df.loc[df[\"year\"] == 2016] \n",
    "\n",
    "## sum daily precipitation values\n",
    "\n",
    "new_df = new_df[['time',var,var+'_eraqc']]\n",
    "df_daily_sum = new_df.resample(\"1D\", on=\"time\").sum()\n",
    "#.reset_index(), the time information gets lost in resampling when this is used \n",
    "print(df_daily_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pr  pr_eraqc\n",
      "time                      \n",
      "2016-01-01   0.0       0.0\n",
      "2016-01-02   0.0       0.0\n",
      "2016-01-03   0.0       0.0\n",
      "2016-01-04   0.3       0.0\n",
      "2016-01-05  28.4       0.0\n"
     ]
    }
   ],
   "source": [
    "## get current month observations\n",
    "\n",
    "month = 1\n",
    "# monthly_df = df_daily_sum.loc[df_daily_sum['time'].dt.month == month]\n",
    "monthly_df = df_daily_sum.loc[df_daily_sum.index.month == month]\n",
    "print(monthly_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pr  pr_eraqc\n",
      "time                      \n",
      "2016-01-18  62.7       0.0\n"
     ]
    }
   ],
   "source": [
    "## get the current day observation\n",
    "\n",
    "# current_obs = monthly_df.loc[monthly_df['time'].dt.day == 18]\n",
    "current_obs = monthly_df.loc[monthly_df.index.day == 18]\n",
    "print(current_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2016-01-18    True\n",
      "Freq: D, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "## substract every other day observatino from the current observation, then get the max difference\n",
    "\n",
    "diff_max = monthly_df['pr'].apply(lambda row: current_obs['pr'] - row).max()#.reset_index()\n",
    "\n",
    "## check if the max difference is past threshold\n",
    "\n",
    "check = diff_max > 60\n",
    "\n",
    "## the result is a boolean with the associated timeIndex \n",
    "\n",
    "print(check) \n",
    "\n",
    "# series -> dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2016-01-18\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# convert boolean series into dataframe, which will be used to assign flags in the original df\n",
    "\n",
    "check_df = check.to_frame(name='values')\n",
    "check_df.reset_index(inplace=True)\n",
    "check_df.columns = ['time','values']\n",
    "\n",
    "flagged_days = check_df[check_df['values']]\n",
    "\n",
    "# a dataframe of \"flagged\" datetimes\n",
    "\n",
    "flagged_days = flagged_days['time']\n",
    "\n",
    "print(flagged_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2016-01-18 00:00:00')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turned into a list in case that's useful? did not work either\n",
    "list = flagged_days.tolist()\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the df for testing\n",
    "\n",
    "df_copy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now trying to add flags to \"pr_eraqc\". there should be a flag assigned to 2016-01-18\n",
    "\n",
    "df_copy.loc[\n",
    "        (\n",
    "            (   \n",
    "                df_copy[\"year\"].isin(flagged_days)\n",
    "                & df_copy[\"month\"].isin(flagged_days)\n",
    "                & df_copy[\"day\"].isin(flagged_days)\n",
    "            )\n",
    "        ),\n",
    "        var + \"_eraqc\",\n",
    "    ] = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the flag was added (should be 32)\n",
    "# I am seeing only nan, alas\n",
    "\n",
    "df_copy['pr_eraqc'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function (not updated, don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begins with         # precip focused check\n",
    "        # for var in pr_vars_to_check:\n",
    "        #     df = qaqc_frequent_precip(df, var)\n",
    "\n",
    "def gap_check(df, vars_to_check, year, threshold, var, plot=True, verbose=False, local=False):\n",
    "    \"\"\"\n",
    "    gap check\n",
    "        - compare all precipitation obs in a single month, all years\n",
    "        - sums observations to daily timestep, then checks each daily sum to every other sum in that month\n",
    "        - flags days on which the sum is 300m more than any other daily observation in that month\n",
    "    Goal: flags precipitation values that are at least 300 mm larger than all other precipitation totals \n",
    "          for a given station and calendar month.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "        df [pd.DataFrame]: QAQC dataframe to run through test\n",
    "        var [str]: variable name\n",
    "        moderate_thresh [int]: moderate precipitation total to check\n",
    "        day_thresh [int]: num. of min consecutive days to flag\n",
    "        verbose [boolean]: whether to provide output to local env\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        df [pd.DataFrame]: QAQC dataframe with flagged values (see below for flag meaning)\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    PRELIMINARY: Thresholds/decisions may change with refinement.\n",
    "\n",
    "    Flag Meaning\n",
    "    ------------\n",
    "        \n",
    "    \"\"\"\n",
    "    ### Filter df to precipitation variables and sum daily observations\n",
    "    # TODO: when is year specified in frequent gaps check?\n",
    "\n",
    "    new_df = df.copy\n",
    "\n",
    "    new_df = new_df.loc[new_df[\"year\"] == year] \n",
    "    new_df = new_df[['time', var,var+'_eraqc']]\n",
    "    df_daily_sum = df.resample(\"1D\", on=\"time\").sum().reset_index()\n",
    "\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        month_flags = []\n",
    "\n",
    "        # Select month data\n",
    "        monthly_df = df_daily_sum.loc[df_daily_sum.index.month == month]\n",
    "\n",
    "        # Now to iterate over each day in the current month\n",
    "        end_day = calendar.monthrange(year, month)[1]\n",
    "\n",
    "        for day in range(1,end_day):\n",
    "            #print('Compare each day sum to every other day sum in a given month.')\n",
    "\n",
    "            current_obs = monthly_df[var].loc[monthly_df.index.day == day]\n",
    "            #print(monthly_df)\n",
    "    \n",
    "            diff = monthly_df[var].apply(lambda row: current_obs - row).max()\n",
    "\n",
    "            flag = diff > threshold\n",
    "\n",
    "            month_flags.append(flag)\n",
    "\n",
    "        month_flags = pd.concat(month_flags, axis=0)\n",
    "\n",
    "        print(month_flags)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_975/2058803070.py:27: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_daily_sum = df.resample(\"1D\", on=\"time\").sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2016-01-01    False\n",
      "2016-01-02    False\n",
      "2016-01-03    False\n",
      "2016-01-04    False\n",
      "2016-01-05    False\n",
      "2016-01-06    False\n",
      "2016-01-07    False\n",
      "2016-01-08    False\n",
      "2016-01-09    False\n",
      "2016-01-10    False\n",
      "2016-01-11    False\n",
      "2016-01-12    False\n",
      "2016-01-13    False\n",
      "2016-01-14    False\n",
      "2016-01-15    False\n",
      "2016-01-16    False\n",
      "2016-01-17    False\n",
      "2016-01-18     True\n",
      "2016-01-19    False\n",
      "2016-01-20    False\n",
      "2016-01-21    False\n",
      "2016-01-22    False\n",
      "2016-01-23    False\n",
      "2016-01-24    False\n",
      "2016-01-25    False\n",
      "2016-01-26    False\n",
      "2016-01-27    False\n",
      "2016-01-28    False\n",
      "2016-01-29    False\n",
      "2016-01-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-02-01    False\n",
      "2016-02-02    False\n",
      "2016-02-03    False\n",
      "2016-02-04    False\n",
      "2016-02-05    False\n",
      "2016-02-06    False\n",
      "2016-02-07    False\n",
      "2016-02-08    False\n",
      "2016-02-09    False\n",
      "2016-02-10    False\n",
      "2016-02-11    False\n",
      "2016-02-12    False\n",
      "2016-02-13    False\n",
      "2016-02-14    False\n",
      "2016-02-15    False\n",
      "2016-02-16    False\n",
      "2016-02-17    False\n",
      "2016-02-18    False\n",
      "2016-02-19    False\n",
      "2016-02-20    False\n",
      "2016-02-21    False\n",
      "2016-02-22    False\n",
      "2016-02-23    False\n",
      "2016-02-24    False\n",
      "2016-02-25    False\n",
      "2016-02-26    False\n",
      "2016-02-27    False\n",
      "2016-02-28    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-03-01    False\n",
      "2016-03-02    False\n",
      "2016-03-03    False\n",
      "2016-03-04    False\n",
      "2016-03-05    False\n",
      "2016-03-06     True\n",
      "2016-03-07    False\n",
      "2016-03-08    False\n",
      "2016-03-09    False\n",
      "2016-03-10    False\n",
      "2016-03-11    False\n",
      "2016-03-12    False\n",
      "2016-03-13    False\n",
      "2016-03-14    False\n",
      "2016-03-15    False\n",
      "2016-03-16    False\n",
      "2016-03-17    False\n",
      "2016-03-18    False\n",
      "2016-03-19    False\n",
      "2016-03-20    False\n",
      "2016-03-21    False\n",
      "2016-03-22    False\n",
      "2016-03-23    False\n",
      "2016-03-24    False\n",
      "2016-03-25    False\n",
      "2016-03-26    False\n",
      "2016-03-27    False\n",
      "2016-03-28    False\n",
      "2016-03-29    False\n",
      "2016-03-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-04-01    False\n",
      "2016-04-02    False\n",
      "2016-04-03    False\n",
      "2016-04-04    False\n",
      "2016-04-05    False\n",
      "2016-04-06    False\n",
      "2016-04-07    False\n",
      "2016-04-08    False\n",
      "2016-04-09    False\n",
      "2016-04-10    False\n",
      "2016-04-11    False\n",
      "2016-04-12    False\n",
      "2016-04-13    False\n",
      "2016-04-14    False\n",
      "2016-04-15    False\n",
      "2016-04-16    False\n",
      "2016-04-17    False\n",
      "2016-04-18    False\n",
      "2016-04-19    False\n",
      "2016-04-20    False\n",
      "2016-04-21    False\n",
      "2016-04-22    False\n",
      "2016-04-23    False\n",
      "2016-04-24    False\n",
      "2016-04-25    False\n",
      "2016-04-26    False\n",
      "2016-04-27    False\n",
      "2016-04-28    False\n",
      "2016-04-29    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-05-01    False\n",
      "2016-05-02    False\n",
      "2016-05-03    False\n",
      "2016-05-04    False\n",
      "2016-05-05    False\n",
      "2016-05-06    False\n",
      "2016-05-07    False\n",
      "2016-05-08    False\n",
      "2016-05-09    False\n",
      "2016-05-10    False\n",
      "2016-05-11    False\n",
      "2016-05-12    False\n",
      "2016-05-13    False\n",
      "2016-05-14    False\n",
      "2016-05-15    False\n",
      "2016-05-16    False\n",
      "2016-05-17    False\n",
      "2016-05-18    False\n",
      "2016-05-19    False\n",
      "2016-05-20    False\n",
      "2016-05-21    False\n",
      "2016-05-22    False\n",
      "2016-05-23    False\n",
      "2016-05-24    False\n",
      "2016-05-25    False\n",
      "2016-05-26    False\n",
      "2016-05-27    False\n",
      "2016-05-28    False\n",
      "2016-05-29    False\n",
      "2016-05-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-06-01    False\n",
      "2016-06-02    False\n",
      "2016-06-03    False\n",
      "2016-06-04    False\n",
      "2016-06-05    False\n",
      "2016-06-06    False\n",
      "2016-06-07    False\n",
      "2016-06-08    False\n",
      "2016-06-09    False\n",
      "2016-06-10    False\n",
      "2016-06-11    False\n",
      "2016-06-12    False\n",
      "2016-06-13    False\n",
      "2016-06-14    False\n",
      "2016-06-15    False\n",
      "2016-06-16    False\n",
      "2016-06-17    False\n",
      "2016-06-18    False\n",
      "2016-06-19    False\n",
      "2016-06-20    False\n",
      "2016-06-21    False\n",
      "2016-06-22    False\n",
      "2016-06-23    False\n",
      "2016-06-24    False\n",
      "2016-06-25    False\n",
      "2016-06-26    False\n",
      "2016-06-27    False\n",
      "2016-06-28    False\n",
      "2016-06-29    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-07-01    False\n",
      "2016-07-02    False\n",
      "2016-07-03    False\n",
      "2016-07-04    False\n",
      "2016-07-05    False\n",
      "2016-07-06    False\n",
      "2016-07-07    False\n",
      "2016-07-08    False\n",
      "2016-07-09    False\n",
      "2016-07-10    False\n",
      "2016-07-11    False\n",
      "2016-07-12    False\n",
      "2016-07-13    False\n",
      "2016-07-14    False\n",
      "2016-07-15    False\n",
      "2016-07-16    False\n",
      "2016-07-17    False\n",
      "2016-07-18    False\n",
      "2016-07-19    False\n",
      "2016-07-20    False\n",
      "2016-07-21    False\n",
      "2016-07-22    False\n",
      "2016-07-23    False\n",
      "2016-07-24    False\n",
      "2016-07-25    False\n",
      "2016-07-26    False\n",
      "2016-07-27    False\n",
      "2016-07-28    False\n",
      "2016-07-29    False\n",
      "2016-07-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-08-01    False\n",
      "2016-08-02    False\n",
      "2016-08-03    False\n",
      "2016-08-04    False\n",
      "2016-08-05    False\n",
      "2016-08-06    False\n",
      "2016-08-07    False\n",
      "2016-08-08    False\n",
      "2016-08-09    False\n",
      "2016-08-10    False\n",
      "2016-08-11    False\n",
      "2016-08-12    False\n",
      "2016-08-13    False\n",
      "2016-08-14    False\n",
      "2016-08-15    False\n",
      "2016-08-16    False\n",
      "2016-08-17    False\n",
      "2016-08-18    False\n",
      "2016-08-19    False\n",
      "2016-08-20    False\n",
      "2016-08-21    False\n",
      "2016-08-22    False\n",
      "2016-08-23    False\n",
      "2016-08-24    False\n",
      "2016-08-25    False\n",
      "2016-08-26    False\n",
      "2016-08-27    False\n",
      "2016-08-28    False\n",
      "2016-08-29    False\n",
      "2016-08-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-09-01    False\n",
      "2016-09-02    False\n",
      "2016-09-03    False\n",
      "2016-09-04    False\n",
      "2016-09-05    False\n",
      "2016-09-06    False\n",
      "2016-09-07    False\n",
      "2016-09-08    False\n",
      "2016-09-09    False\n",
      "2016-09-10    False\n",
      "2016-09-11    False\n",
      "2016-09-12    False\n",
      "2016-09-13    False\n",
      "2016-09-14    False\n",
      "2016-09-15    False\n",
      "2016-09-16    False\n",
      "2016-09-17    False\n",
      "2016-09-18    False\n",
      "2016-09-19    False\n",
      "2016-09-20    False\n",
      "2016-09-21    False\n",
      "2016-09-22    False\n",
      "2016-09-23    False\n",
      "2016-09-24    False\n",
      "2016-09-25    False\n",
      "2016-09-26    False\n",
      "2016-09-27    False\n",
      "2016-09-28    False\n",
      "2016-09-29    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-10-01    False\n",
      "2016-10-02    False\n",
      "2016-10-03    False\n",
      "2016-10-04    False\n",
      "2016-10-05    False\n",
      "2016-10-06    False\n",
      "2016-10-07    False\n",
      "2016-10-08    False\n",
      "2016-10-09    False\n",
      "2016-10-10    False\n",
      "2016-10-11    False\n",
      "2016-10-12    False\n",
      "2016-10-13    False\n",
      "2016-10-14    False\n",
      "2016-10-15    False\n",
      "2016-10-16    False\n",
      "2016-10-17    False\n",
      "2016-10-18    False\n",
      "2016-10-19    False\n",
      "2016-10-20    False\n",
      "2016-10-21    False\n",
      "2016-10-22    False\n",
      "2016-10-23    False\n",
      "2016-10-24    False\n",
      "2016-10-25    False\n",
      "2016-10-26    False\n",
      "2016-10-27    False\n",
      "2016-10-28    False\n",
      "2016-10-29    False\n",
      "2016-10-30    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-11-01    False\n",
      "2016-11-02    False\n",
      "2016-11-03    False\n",
      "2016-11-04    False\n",
      "2016-11-05    False\n",
      "2016-11-06    False\n",
      "2016-11-07    False\n",
      "2016-11-08    False\n",
      "2016-11-09    False\n",
      "2016-11-10    False\n",
      "2016-11-11    False\n",
      "2016-11-12    False\n",
      "2016-11-13    False\n",
      "2016-11-14    False\n",
      "2016-11-15    False\n",
      "2016-11-16    False\n",
      "2016-11-17    False\n",
      "2016-11-18    False\n",
      "2016-11-19    False\n",
      "2016-11-20    False\n",
      "2016-11-21    False\n",
      "2016-11-22    False\n",
      "2016-11-23    False\n",
      "2016-11-24    False\n",
      "2016-11-25    False\n",
      "2016-11-26    False\n",
      "2016-11-27    False\n",
      "2016-11-28    False\n",
      "2016-11-29    False\n",
      "Freq: D, dtype: bool\n",
      "time\n",
      "2016-12-01    False\n",
      "2016-12-02    False\n",
      "2016-12-03    False\n",
      "2016-12-04    False\n",
      "2016-12-05    False\n",
      "2016-12-06    False\n",
      "2016-12-07    False\n",
      "2016-12-08    False\n",
      "2016-12-09    False\n",
      "2016-12-10    False\n",
      "2016-12-11    False\n",
      "2016-12-12    False\n",
      "2016-12-13    False\n",
      "2016-12-14    False\n",
      "2016-12-15    False\n",
      "2016-12-16    False\n",
      "2016-12-17    False\n",
      "2016-12-18    False\n",
      "2016-12-19    False\n",
      "2016-12-20    False\n",
      "2016-12-21    False\n",
      "2016-12-22    False\n",
      "2016-12-23    False\n",
      "2016-12-24    False\n",
      "2016-12-25    False\n",
      "2016-12-26    False\n",
      "2016-12-27    False\n",
      "2016-12-28    False\n",
      "2016-12-29    False\n",
      "2016-12-30    False\n",
      "Freq: D, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "pr_vars_to_run = [\n",
    "        \"pr_5min\",\n",
    "        \"pr_15min\",\n",
    "        \"pr_1h\",\n",
    "        \"pr_24h\",\n",
    "        \"pr_localmid\",\n",
    "    ]\n",
    "pr_vars_to_check = [\n",
    "        var for var in df.columns \n",
    "        if any(True for item in pr_vars_to_run if item in var)\n",
    "        and not any(True for item in pr_vars_to_run if item in var)\n",
    "    ]\n",
    "\n",
    "year = 2016 \n",
    "threshold = 60\n",
    "\n",
    "output = gap_check(df, pr_vars_to_run, year, threshold, 'pr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # Index to flag finds where df_month is out of the distribution\n",
    "                # index_to_flag = (df_month < low) | (df_month > high)\n",
    "\n",
    "                # # Since grouping, the index of df_month is years\n",
    "                # years_to_flag = df_month[index_to_flag].index\n",
    "\n",
    "                # flag all obs in that day\n",
    "                # bad = np.logical_and(df[\"month\"] == month, df[\"year\"].isin(years_to_flag))\n",
    "                # df.loc[bad, var + \"_eraqc\"] = 21  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
