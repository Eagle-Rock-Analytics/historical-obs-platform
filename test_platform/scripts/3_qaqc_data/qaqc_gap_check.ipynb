{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "import scipy.stats as stats\n",
    "\n",
    "import s3fs\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "# New logger function\n",
    "from log_config import logger\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "## Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "## Set relative paths to other folders and objects in repository.\n",
    "bucket_name = \"wecc-historical-wx\"\n",
    "wecc_terr = (\n",
    "    \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_land.shp\"\n",
    ")\n",
    "wecc_mar = \"s3://wecc-historical-wx/0_maps/WECC_Informational_MarineCoastal_Boundary_marine.shp\"\n",
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)\n",
    "def open_log_file_merge(file):\n",
    "    global log_file\n",
    "    log_file = file\n",
    "def read_nc_from_s3(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "# -----------------------------------------------------------------------------\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "# -----------------------------------------------------------------------------\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data\n",
    "# -----------------------------------------------------------------------------\n",
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "def printf(*args, verbose=True, log_file=None, **kwargs):\n",
    "    import datetime\n",
    "\n",
    "    tLog = lambda: datetime.datetime.utcnow().strftime(\"%m-%d-%Y %H:%M:%S\") + \" : \\t\"\n",
    "    args = [str(a) for a in args]\n",
    "\n",
    "    if verbose:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs) or print(\n",
    "                \" \".join([tLog(), *args]), file=log_file, **kwargs\n",
    "            )\n",
    "        else:\n",
    "            print(\" \".join([tLog(), *args]), **kwargs)\n",
    "    else:\n",
    "        if log_file is not None:\n",
    "            print(\" \".join([tLog(), *args]), file=log_file, **kwargs)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in single dc file from AWS\n",
    "ds = read_nc_from_s3(\"ASOSAWOS\", \"ASOSAWOS_72494523293\", temp_dir)\n",
    "# [\"ASOSAWOS_74948400395\", \"ASOSAWOS_74509023244\", \"ASOSAWOS_72494523293\"]\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df = qaqc_ds_to_df(ds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check differences between all values in a month\n",
    "\n",
    "<div style=\"width: 70%;\">\n",
    "  The next function checks for a given pandas series the difference\n",
    "between all rows and flags the ones that exceed a threshold ]\n",
    "    \n",
    " - (Note that special care needs to be taken with self-checking):\n",
    "   - `diff_matrix = np.abs(series.values[:, None] - series.values)` creates a matrix of difference of each row to every other row\n",
    "   - `exceeds_threshold = diff_matrix > threshold` converts the matrix to True/False if exceeds the threshold\n",
    "   - `np.fill_diagonal(exceeds_threshold, True)` fills the diagonal with True. This way, the self-comparison, which will be always False (since the difference is zero) becomes True and helps differentiathe real large differences than self-checking\n",
    " - At the end, returns a `pd.Series` with the same index as the input series, that way you can compare directly\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T02:37:13.458472Z",
     "iopub.status.busy": "2025-02-01T02:37:13.457786Z",
     "iopub.status.idle": "2025-02-01T02:37:13.464812Z",
     "shell.execute_reply": "2025-02-01T02:37:13.463911Z",
     "shell.execute_reply.started": "2025-02-01T02:37:13.458435Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_differences(series, threshold=200):\n",
    "\n",
    "    # Compute pairwise differences, between values in the pandas series and all other values\n",
    "    # for all values in the column\n",
    "    diff_matrix = np.diff(series.values[:, None] - series.values)\n",
    "    \n",
    "    # Check for values exceeding threshold\n",
    "    exceeds_threshold = diff_matrix > threshold \n",
    " \n",
    "    # Exclude self-comparison    \n",
    "    np.fill_diagonal(exceeds_threshold, True)    \n",
    "    \n",
    "    # Identify Rows with Any Exceeding Differences\n",
    "    rows_with_exceeding_diff = exceeds_threshold.all(axis=1)\n",
    "\n",
    "    # row_has_diffs_above_threshold = pd.Series(\n",
    "    return pd.Series(rows_with_exceeding_diff, name=\"exceeds_threshold\", index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest version of `qaqc_unusual_gaps_precip`\n",
    "\n",
    "<div style=\"width: 70%;\">\n",
    "  This way uses the `.gropuby` and `.transform` functions that is much cleaner and\n",
    "    better in performance that doing loops. \n",
    "\n",
    "  By doing `.groupby(\"month\").transform(custom_func)` it auto selects and group each month\n",
    "  and applys the `custom_func` to each group\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T02:37:14.046408Z",
     "iopub.status.busy": "2025-02-01T02:37:14.045703Z",
     "iopub.status.idle": "2025-02-01T02:37:14.056415Z",
     "shell.execute_reply": "2025-02-01T02:37:14.055325Z",
     "shell.execute_reply.started": "2025-02-01T02:37:14.046366Z"
    }
   },
   "outputs": [],
   "source": [
    "def qaqc_unusual_gaps_precip(df, var, threshold=200, verbose=False):\n",
    "    ### Filter df to precipitation variables and sum daily observations\n",
    "\n",
    "    logger.info(\"Running qaqc_unusual_gaps_precip on: {}\".format(var))\n",
    "    new_df = df.copy()\n",
    "    df_valid = grab_valid_obs(new_df, var)\n",
    "\n",
    "    # aggregate to daily, subset on time, var, and eraqc var\n",
    "    df_sub = df_valid[[\"time\", 'year','month', 'day', var, var+\"_eraqc\"]]\n",
    "    df_dy = df_sub.resample(\"1D\", on=\"time\").agg({var: \"sum\",var+\"_eraqc\": \"first\", \"year\": \"first\", \"month\": \"first\", \"day\": \"first\"})#.reset_index()\n",
    "\n",
    "    ### returns a flag column with True or False\n",
    "    ### variable output is True for flag values, False for non-flagged values\n",
    "    output = df_dy.groupby(\"month\")[var].transform(check_differences, threshold=threshold)\n",
    "    flagged = output[output]\n",
    "    flagged_str = flagged.map({True: 33, False: 1})\n",
    "\n",
    "    # backflag all observations in the input dataframe, on days flagged for exceeding the threshold\n",
    "    new_df[var+'_eraqc'] = new_df['time'].dt.date.map(flagged_str)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      time        ps     tas    tdps   pr  sfcWind  \\\n",
      "386613 2016-01-01 00:53:00  101880.0  282.55  275.95  0.0      3.6   \n",
      "386614 2016-01-01 01:53:00  101920.0  282.55  275.95  0.0      0.0   \n",
      "386615 2016-01-01 02:53:00  101950.0  280.95  275.35  0.0      0.0   \n",
      "\n",
      "        sfcWind_dir  elevation qaqc_process ps_qc  ...  ps_altimeter_eraqc  \\\n",
      "386613        280.0       15.0         V030     5  ...                 NaN   \n",
      "386614          NaN       15.0         V030     5  ...                 NaN   \n",
      "386615          NaN       15.0         V030     5  ...                 NaN   \n",
      "\n",
      "       psl_eraqc  anemometer_height_m thermometer_height_m  \\\n",
      "386613       NaN                10.06                  NaN   \n",
      "386614       NaN                10.06                  NaN   \n",
      "386615       NaN                10.06                  NaN   \n",
      "\n",
      "                     station hour day month  year        date  \n",
      "386613  ASOSAWOS_72494523293    0   1     1  2016  2016-01-01  \n",
      "386614  ASOSAWOS_72494523293    1   1     1  2016  2016-01-01  \n",
      "386615  ASOSAWOS_72494523293    2   1     1  2016  2016-01-01  \n",
      "\n",
      "[3 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "sub_df = df[df['year'].isin([2016,2017])]\n",
    "print(sub_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T02:37:15.373340Z",
     "iopub.status.busy": "2025-02-01T02:37:15.372640Z",
     "iopub.status.idle": "2025-02-01T02:37:15.494851Z",
     "shell.execute_reply": "2025-02-01T02:37:15.494515Z",
     "shell.execute_reply.started": "2025-02-01T02:37:15.373286Z"
    }
   },
   "outputs": [],
   "source": [
    "output = qaqc_unusual_gaps_precip(sub_df, 'pr', threshold=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399386    33\n",
      "399387    33\n",
      "399388    33\n",
      "399389    33\n",
      "399390    33\n",
      "399391    33\n",
      "399392    33\n",
      "399393    33\n",
      "399394    33\n",
      "399395    33\n",
      "399396    33\n",
      "399397    33\n",
      "399398    33\n",
      "399399    33\n",
      "399400    33\n",
      "399401    33\n",
      "399402    33\n",
      "399403    33\n",
      "399404    33\n",
      "399405    33\n",
      "399406    33\n",
      "399407    33\n",
      "399408    33\n",
      "399409    33\n",
      "399410    33\n",
      "399411    33\n",
      "399412    33\n",
      "399413    33\n",
      "399414    33\n",
      "399415    33\n",
      "399416    33\n",
      "399417    33\n",
      "399418    33\n",
      "399419    33\n",
      "Name: pr_eraqc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filtered_df = output[output['time'].dt.date == pd.to_datetime('2017-04-08').date()]\n",
    "print(filtered_df['pr_eraqc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 33.])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pr_eraqc'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now running qaqc_unusual_gaps() with new function included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from qaqc_plot import *\n",
    "    from qaqc_utils import *\n",
    "    from qaqc_wholestation import *\n",
    "    from qaqc_logic_checks import *\n",
    "    from qaqc_buoy_check import *\n",
    "    from qaqc_frequent import *\n",
    "    from qaqc_unusual_gaps import *\n",
    "    from qaqc_unusual_large_jumps import *\n",
    "    from qaqc_climatological_outlier import *\n",
    "    from qaqc_unusual_streaks import *\n",
    "except Exception as e:\n",
    "    print(\"Error importing qaqc script: {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n",
      "/home/machuca/code/historical-obs-platform/test_platform/scripts/3_qaqc_data/qaqc_utils.py:337: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, [var + \"_eraqc\"]] = 19  # see era_qaqc_flag_meanings.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final = qaqc_unusual_gaps(sub_df, iqr_thresh=5, plots=True, verbose=False, local=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
