{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ee035f",
   "metadata": {},
   "source": [
    "Motivation: individual gross outliers from general station distribution are a common error in obs data by random recording, reporting, formatting, or instrumentation errors\n",
    "\n",
    "Process:\n",
    "1. uses individual observation deviations derived from monthly mean climatology calculated for each hour of the day\n",
    "2. climatologies calculated using winsorised data to remove initial effect of outliers\n",
    "    - Winsorising: all values beyond threhsold value from mean are set to that threshold value\n",
    "    - 5 and 95% for hadisd\n",
    "    - number of data values in population remains the same, not trimmed\n",
    "3. raw unwinsorised observations are anomalised using these climatologies\n",
    "4. standardized by IQR for that month and hour\n",
    "    - IQR cannot be less than 1.5degC\n",
    "5. values are low-pass filtered to remove any climate change signal causing overzealous removal at ends of time series\n",
    "6. gaussian is fitted to the histogram of anomalies for each month\n",
    "7. threshold value, rounded outwards where crosses y=0.1 line\n",
    "8. distribution beyond threhsold value is scanned for gap, equal to bin width or more\n",
    "9. all values beyond gap are flagged\n",
    "10. obs that fall between critical threshold value and gap or critical threshold and end of distribution are tentatively flagged\n",
    "    - these may be later reinstated on comparison with good data from neighboring stations\n",
    "\n",
    "Notes:\n",
    "- when applied to SLP, frequently flags storm signals, which may be of high interest, so this test is not applied to pressure data\n",
    "- hadisd only applies to temp and dewpoint temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92465473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a5967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_mon_mean_hourly(df, var, month, hour):\n",
    "    '''Calculate the monthly mean climatology for each of the day'''\n",
    "    \n",
    "    df_m_h = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)]\n",
    "    clim_value = df_m_h[var].mean(numeric_only = True)\n",
    "    \n",
    "    # special handling if value is nan? \n",
    "    \n",
    "    return clim_value\n",
    "\n",
    "def iqr_range_monhour(df, var, month, hour):\n",
    "    '''Calculates the monthly interquartile range per hour'''\n",
    "    \n",
    "    q1 = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)].quantile(0.25, numeric_only=True)\n",
    "    q3 = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)].quantile(0.75, numeric_only=True)\n",
    "    \n",
    "    iqr_df = q3 - q1\n",
    "    iqr_df_val = iqr_df[var]\n",
    "    \n",
    "    # iqr cannot be less than 1.5Â°C in order to preserve low variance stations\n",
    "    if iqr_df_val < 1.5:\n",
    "        iqr_df_val = 1.5\n",
    "    else:\n",
    "        iqr_df_val = iqr_df_val\n",
    "            \n",
    "    return iqr_df_val\n",
    "\n",
    "\n",
    "def clim_standardized_anom(df, vars_to_anom):\n",
    "    '''\n",
    "    First anomalizes data by monthly climatology for each hour, then\n",
    "    standardizes by the monthly climatological anomaly IQR for each hour\n",
    "    '''\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        for m in range(1,13,1):\n",
    "            for h in range(0,24,1):\n",
    "                # each hour in each month\n",
    "                anom_value = clim_mon_mean_hourly(df, var, month=m, hour=h)\n",
    "                iqr_value = iqr_range_monhour(df, var, month=m, hour=h)\n",
    "                \n",
    "                # locate obs within specific month/hour\n",
    "                df_m_h = df.loc[(df.time.dt.month == m) & (df.time.dt.hour == h)]\n",
    "                \n",
    "                # calculate the monthly climatological anomaly by hour and standardize by iqr\n",
    "                df2.loc[(df.time.dt.month == m) & \n",
    "                        (df.time.dt.hour == h), \n",
    "                        var] = (df_m_h[var] - anom_value) / iqr_value\n",
    "                \n",
    "    return df2\n",
    "\n",
    "def winsorize_temps(df, vars_to_anom, winz_limits):\n",
    "    '''\n",
    "    Replaces potential spurious outliers by limiting the extreme values\n",
    "    using the winz_limits set (default is 5% and 95% percentiles)\n",
    "    '''\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        for m in range(1,13,1):\n",
    "            for h in range(0,24,1):\n",
    "                if h not in df.loc[df.time.dt.hour == h]:\n",
    "                    continue # some stations only report some hours\n",
    "                else:\n",
    "                    df_m_h = df.loc[(df.time.dt.month == m) & (df.time.dt.hour == h)]\n",
    "\n",
    "                    # winsorize only vars in vars_to_anom\n",
    "                    df_w = winsorize(df_m_h[var], limits=winz_limits, nan_policy='omit')\n",
    "\n",
    "                    df2.loc[(df.time.dt.month == m) & (df.time.dt.hour == h),\n",
    "                           var] = df_w\n",
    "                \n",
    "    return df2\n",
    "\n",
    "def median_yr_anom(df, var):\n",
    "    '''Get median anomaly per year'''\n",
    "    \n",
    "    monthly_anoms = []\n",
    "    \n",
    "    # identify years in data\n",
    "    years = df.time.dt.year.unique()\n",
    "    \n",
    "    for yr in years:\n",
    "        df_yr = df.loc[df.time.dt.year == yr]\n",
    "\n",
    "        ann_anom = df_yr[var].median()\n",
    "        monthly_anoms.append(ann_anom)\n",
    "        \n",
    "    return monthly_anoms\n",
    "\n",
    "def low_pass_filter_weights(median_anoms, month_low, month_high, filter_low, filter_high):\n",
    "    '''Calculates weights for low pass filter'''\n",
    "    \n",
    "    filter_wgts = [1, 2, 3, 2, 1]\n",
    "    \n",
    "    if np.sum(filter_wgts[filter_low:filter_high] * \n",
    "              np.ceil(median_anoms[month_low:month_high] - \n",
    "                      np.floor(median_anoms[month_low:month_high]))) == 0:\n",
    "        weight = 0\n",
    "    \n",
    "    else:\n",
    "        weight = (\n",
    "            np.sum(filter_wgts[filter_low:filter_high] * np.ceil(median_anoms[month_low:month_high])) / \n",
    "            np.sum(filter_wgts[filter_low:filter_high] * np.ceil(median_anoms[month_low:month_high] - \n",
    "                                                                 np.floor(median_anoms[month_low:month_high])))\n",
    "        )\n",
    "        \n",
    "    return weight\n",
    "\n",
    "def low_pass_filter(df, vars_to_anom):\n",
    "    '''\n",
    "    Low pass filtering on observations to remove any climate change signal \n",
    "    causing overzealous removal at ends of time series\n",
    "    '''\n",
    "    # identify years in data\n",
    "    years = df.time.dt.year.unique()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        \n",
    "        median_anoms = median_yr_anom(df, var)\n",
    "    \n",
    "        for yr in range(len(years)):\n",
    "            if yr == 0:\n",
    "                month_low, month_high = 0, 3\n",
    "                filter_low, filter_high = 2, 5\n",
    "                \n",
    "            elif yr == 1:\n",
    "                month_low, month_high = 0, 4\n",
    "                filter_low, filter_high = 1, 5\n",
    "                \n",
    "            elif yr == len(years)-2:\n",
    "                month_low, month_high = -4, -1\n",
    "                filter_low, filter_high = 0, 3\n",
    "\n",
    "            elif yr == len(years)-1:\n",
    "                month_low, month_high = -3, -1\n",
    "                filter_low, filter_high = 0, 2\n",
    "\n",
    "            else:\n",
    "                month_low, month_high = yr-2, yr+3\n",
    "                filter_low, filter_high = 0, 5\n",
    "                            \n",
    "            if np.sum(np.abs(median_anoms[month_low:month_high])) != 0:\n",
    "                weights = low_pass_filter_weights(median_anoms, month_low, month_high, filter_low, filter_high)\n",
    "                      \n",
    "            # want to return specific year of data at a specific variable, the variable minus weight value\n",
    "            df.loc[(df.time.dt.year == years[yr]), var] = df.loc[df.time.dt.year == years[yr]][var] - weights\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "## distribution gap plotting helpers\n",
    "def create_bins(data, bin_size=0.25):\n",
    "    '''Create bins from data covering entire data range'''\n",
    "\n",
    "    # set up bins\n",
    "    b_min = np.floor(np.nanmin(data))\n",
    "    b_max = np.ceil(np.nanmax(data))\n",
    "    bins = np.arange(b_min - bin_size, b_max + (3. * bin_size), bin_size)\n",
    "\n",
    "    return bins\n",
    "\n",
    "def pdf_bounds(df, mu, sigma, bins):\n",
    "    '''Calculate pdf distribution, return pdf and threshold bounds'''\n",
    "\n",
    "    y = stats.norm.pdf(bins, mu, sigma)\n",
    "    \n",
    "    # add vertical lines to indicate thresholds where pdf y=0.1\n",
    "    pdf_bounds = np.argwhere(y > 0.1)\n",
    "    \n",
    "    # find first index\n",
    "    left_bnd = round(bins[pdf_bounds[0][0] -1])\n",
    "    right_bnd = round(bins[pdf_bounds[-1][0] + 1])\n",
    "    thresholds = (left_bnd - 1, right_bnd + 1)\n",
    "    \n",
    "    return (y, left_bnd - 1, right_bnd + 1)\n",
    "\n",
    "def clim_outlier_plot(df, var, month, network):\n",
    "    '''\n",
    "    Produces a histogram of monthly standardized distribution\n",
    "    with PDF overlay and threshold lines where pdf falls below y=0.1.\n",
    "    Any bin that is outside of the threshold is visually flagged.\n",
    "    \n",
    "    Differs from dist_gap_part2_plot for the climatological outlier\n",
    "    as IQR standardization does not occur within plotting\n",
    "    '''\n",
    "    \n",
    "    # select month\n",
    "    df = df.loc[df.time.dt.month == month]\n",
    "    \n",
    "    # determine number of bins\n",
    "    bins = create_bins(df)\n",
    "    \n",
    "    # plot histogram\n",
    "    ax = plt.hist(df, bins=bins, log=False, density=True, alpha=0.3)\n",
    "    xmin, xmax = plt.xlim()\n",
    "    plt.ylim(ymin=0.1)\n",
    "    \n",
    "    # plot pdf\n",
    "    mu = np.nanmean(df)\n",
    "    sigma = np.nanmean(df)\n",
    "    y = stats.norm.pdf(bins, mu, sigma)\n",
    "    l = plt.plot(bins, y, 'k--', linewidth=1)\n",
    "    \n",
    "    # add vertical lines to indicate thresholds where pdf y=0.1\n",
    "    pdf_bounds = np.argwhere(y > 0.1)\n",
    "    \n",
    "    # find first index\n",
    "    left_bnd = round(bins[pdf_bounds[0][0] - 1])\n",
    "    right_bnd = round(bins[pdf_bounds[-1][0] + 1])\n",
    "    thresholds = (left_bnd - 1, right_bnd + 1)\n",
    "    \n",
    "    plt.axvline(thresholds[1], color='r') # right tail\n",
    "    plt.axvline(thresholds[0], color='r') # left tail\n",
    "    \n",
    "    # flag visually obs that are beyond threshold\n",
    "    for bar in ax[2].patches:\n",
    "        x = bar.get_x() + 0.5 * bar.get_width()\n",
    "        if x > thresholds[1]: # right tail\n",
    "            bar.set_color('r')\n",
    "        elif x < thresholds[0]: # left tail\n",
    "            bar.set_color('r')\n",
    "            \n",
    "    # title and useful annotations\n",
    "    plt.title('Climatological outlier check, {0}: {1}'.format(df['station'].unique()[0], var), fontsize=10);\n",
    "    plt.annotate('Month: {}'.format(month), xy=(0.025, 0.95), xycoords='axes fraction', fontsize=8);\n",
    "    plt.annotate('Mean: {}'.format(round(mu,3)), xy=(0.025, 0.9), xycoords='axes fraction', fontsize=8);\n",
    "    plt.annotate('Std.Dev: {}'.format(round(sigma,3)), xy=(0.025, 0.85), xycoords='axes fraction', fontsize=8);\n",
    "    plt.ylabel('Frequency (obs)')\n",
    "    \n",
    "    # save figure to AWS\n",
    "    bucket_name = 'wecc-historical-wx'\n",
    "    directory = '3_qaqc_wx'\n",
    "    img_data = BytesIO()\n",
    "    plt.savefig(img_data, format='png')\n",
    "    img_data.seek(0)\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    figname = 'qaqc_climatological_outlier_{0}_{1}_{2}'.format(df['station'].unique()[0], var, month)\n",
    "    bucket.put_object(Body=img_data, ContentType='image/png',\n",
    "                     Key='{0}/{1}/qaqc_figs/{2}.png'.format(\n",
    "                     directory, network, figname))\n",
    "    \n",
    "    # close figures to save memory\n",
    "    plt.close()\n",
    "\n",
    "def qaqc_climatological_outlier(df, winsorize=True, winz_limits=[0.05,0.05], plot=True, verbose=True):\n",
    "    '''\n",
    "    Flags individual gross outliers from climatological distribution.\n",
    "    Only applied to air temperature and dew point temperature\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        df [pd.DataFrame]: station dataset converted to dataframe through QAQC pipeline\n",
    "        plots [bool]: if True, produces plots of any flagged data and saved to AWS\n",
    "        winsorize [bool]: if True, raw observations are winsorized to remove spurious outliers first\n",
    "        winz_limits [list]: if winsorize is True, values represent the low and high percentiles to standardize to\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "        qaqc success:\n",
    "            df [pd.DataFrame]: QAQC dataframe with flagged values (see below for flag meaning)\n",
    "        qaqc failure:\n",
    "            None\n",
    "            \n",
    "    Flag meaning:\n",
    "    -------------\n",
    "        25,qaqc_climatological_outlier,Value flagged as a climatological outlier\n",
    "    '''\n",
    "    \n",
    "    #### ONLY IN NOTEBOOK DEVELOPMENT, REMOVED IN CODE FOR PIPELINE #####\n",
    "    df = df.reset_index()\n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month # sets month to new variable\n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year # sets year to new variable  \n",
    "    \n",
    "    vars_to_check = ['tas', 'tdps', 'tdps_derived']\n",
    "    vars_to_anom = [v for v in vars_to_check if v in df.columns]\n",
    "    \n",
    "    # whole station bypass check\n",
    "    df, pass_flag = qaqc_dist_whole_stn_bypass_check(df, vars_to_anom)\n",
    "    \n",
    "    if pass_flag == 'fail':\n",
    "        return df\n",
    "    else:\n",
    "        for var in vars_to_anom:      \n",
    "\n",
    "            # only work with non-flagged values\n",
    "            df_valid = df.loc[df[var+'_eraqc'].isnull()]\n",
    "\n",
    "            # winsorize data by percentiles\n",
    "            if winsorize == True:\n",
    "                df_std = winsorize_temps(df_valid, vars_to_anom, winz_limits)\n",
    "            else:\n",
    "                df_std = df_valid\n",
    "\n",
    "            # standardize data by monthly climatological anomalies by hour\n",
    "            df_std = clim_standardized_anom(df_std, vars_to_anom)\n",
    "\n",
    "            # apply low pass filter\n",
    "            df_std = low_pass_filter(df_std, vars_to_anom)\n",
    "\n",
    "            # gaussian is fitted to the histogram of anomalies for each month\n",
    "            # similar to distributional gap check\n",
    "            # FUTURE DEV (v2 of product):\n",
    "                # HadISD: obs that fall between critical threshold value and gap or critical threshold and end of distribution are tentatively flagged\n",
    "                # May be later reinstated on comparison with good data from neighboring stations\n",
    "\n",
    "            for month in range(1,13):\n",
    "\n",
    "                df_m = df_std.loc[df_valid.time.dt.month == month]\n",
    "\n",
    "                # determine number of bins\n",
    "                bins = create_bins(df_m[var])\n",
    "\n",
    "                # pdf\n",
    "                mu = np.nanmean(df_m[var])\n",
    "                sigma = np.nanstd(df_m[var])\n",
    "                y, left_bnd, right_bnd = pdf_bounds(df_m[var], mu, sigma, bins)\n",
    "\n",
    "                # identify gaps as below y=0.1 from histogram, not pdf\n",
    "                y_hist, bins = np.histogram(df_m[var], bins=bins, density=True)\n",
    "\n",
    "                # identify bin indices outside of thresholds and check if bin is above 0.1\n",
    "                bins_to_check = [i for i, n in enumerate(bins) if n <= left_bnd or n >= right_bnd][:-1] # remove last item due to # of bins exceeding hist by 1\n",
    "                if len(bins_to_check) != 0:\n",
    "                    for b in bins_to_check:\n",
    "                        if y_hist[b] > 0.1:\n",
    "                            print('Flagging outliers in {0}, month {1}'.format(var, month))\n",
    "                            # list of index of full df to flag, not standardized df\n",
    "                            idx_to_flag = [i for i in df_m.loc[(df_m[var] >= bins[b]) & (df_m[var] < bins[b+1])].index]\n",
    "                            for i in idx_to_flag:\n",
    "                                df.loc[df.index == i, var+'_eraqc'] = 25 # see era_qaqc_flag_meanings.csv \n",
    "\n",
    "        if plot == True:\n",
    "            for var in vars_to_anom:\n",
    "                if 25 in df[var+'_eraqc'].values: # only plot a figure if flag is present\n",
    "                    clim_outlier_plot(df, var, network=df['station'].unique()[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daeea895",
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the bad values in VCAPCD\n",
    "def qaqc_missing_vals(df, verbose=True):\n",
    "    '''\n",
    "    Checks data to be qaqc'ed for any errant missing values that made it through cleaning\n",
    "    Converts those missing values to NAs\n",
    "    Searches for missing values in 3_qaqc_data/missing_data_flags.csv\n",
    "    '''\n",
    "\n",
    "    missing_vals = pd.read_csv('missing_data_flags.csv')\n",
    "\n",
    "    all_vars = [col for col in df.columns if 'qc' not in col]\n",
    "    obs_vars = [var for var in all_vars if var not in ['lon','lat','time','elevation','station','anemometer_height_m','thermometer_height_m']]\n",
    "    \n",
    "    try:\n",
    "        for item in obs_vars:\n",
    "            # pull missing values which are appropriate for the range of real values for each variable \n",
    "            missing_codes = missing_vals.loc[missing_vals['variable'].str.contains(item) | missing_vals['variable'].str.contains('all')]\n",
    "\n",
    "            # values in column that == missing_flag values, replace with NAs\n",
    "            # note numerical vals converted to strings first to match missing_flag formatting\n",
    "            df[item] = np.where(df[item].astype(str).isin(missing_codes['missing_flag']), float('NaN'), df[item])\n",
    "\n",
    "            print('Updating missing values for: {}'.format(item))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def qaqc_world_record(df, verbose=True):\n",
    "    '''\n",
    "    Checks if temperature, dewpoint, windspeed, or sea level pressure are outside North American world records\n",
    "    If outside minimum or maximum records, flags values\n",
    "    '''\n",
    "    try:\n",
    "        # world records from HadISD protocol, cross-checked with WMO database\n",
    "        # https://wmo.asu.edu/content/world-meteorological-organization-global-weather-climate-extremes-archive\n",
    "        T_X = {\"North_America\":329.92} #K\n",
    "        T_N = {\"North_America\":210.15} #K\n",
    "        D_X = {\"North_America\":329.85} #K\n",
    "        D_N = {\"North_America\":173.15} #K\n",
    "        W_X = {\"North_America\":113.2} #m/s\n",
    "        W_N = {\"North_America\":0.} #m/s\n",
    "        S_X = {\"North_America\":108330} #Pa\n",
    "        S_N = {\"North_America\":87000} #Pa\n",
    "\n",
    "        maxes = {\"tas\": T_X, \"tdps\": D_X, \"tdps_derived\": D_X, \"sfcWind\": W_X, \"psl\": S_X}\n",
    "        mins = {\"tas\": T_N, \"tdps\": D_N, \"tdps_derived\": D_N, \"sfcWind\": W_N, \"psl\": S_N}\n",
    "\n",
    "        # variable names to check against world record limits\n",
    "        wr_vars = ['tas', 'tdps_derived', 'tdps', 'sfcWind', 'psl']\n",
    "\n",
    "        for var in wr_vars:\n",
    "            if var in list(df.columns):\n",
    "                isOffRecord = np.logical_or(df[var] < mins[var]['North_America'],\n",
    "                                            df[var] > maxes[var]['North_America'])\n",
    "                if isOffRecord.any():\n",
    "                    df.loc[isOffRecord, var + '_eraqc'] = 11\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(\"qaqc_world_record failed with Exception: {}\".format(e))\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def qaqc_dist_whole_stn_bypass_check(df, vars_to_check, min_num_months=5):\n",
    "    \"\"\"\n",
    "    Checks the number of valid observation months in order to proceed through monthly distribution checks. \n",
    "    Identifies whether a station record has too few months and produces a fail pass flag. \n",
    "    \"\"\"\n",
    "\n",
    "    # in order to grab the time information more easily -- would prefer not to do this\n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month # sets month to new variable\n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year # sets year to new variable\n",
    "             \n",
    "    # set up a \"pass_flag\" to determine if station proceeds through distribution function\n",
    "    pass_flag = 'pass'\n",
    "    \n",
    "    for var in vars_to_check:\n",
    "        for month in range(1,13):\n",
    "            # first check num of months in order to continue\n",
    "            month_to_check = df.loc[df['month'] == month]\n",
    "\n",
    "            # check for number of obs years\n",
    "            if (len(month_to_check.year.unique()) < 5):\n",
    "                df[var+'_eraqc'] = 18 # see era_qaqc_flag_meanings.csv\n",
    "                pass_flag = 'fail'\n",
    "\n",
    "    err_statement = 'Station has too short of an observation record to proceed through the monthly distribution qa/qc checks -- bypassing station'\n",
    "    \n",
    "    if pass_flag == 'fail':\n",
    "        print(err_statement)\n",
    "                \n",
    "    return (df, pass_flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e67740f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating missing values for: ps\n",
      "Updating missing values for: tas\n",
      "Updating missing values for: pr\n",
      "Updating missing values for: hurs\n",
      "Updating missing values for: rsds\n",
      "Updating missing values for: sfcWind\n",
      "Updating missing values for: sfcWind_dir\n",
      "Updating missing values for: pr_1h\n",
      "Updating missing values for: tdps_derived\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset('/Users/victoriaford/Desktop/eaglerock/Historical Data Platform/Train_Files/VCAPCD_ER.nc')\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "df = qaqc_missing_vals(df)\n",
    "df = qaqc_world_record(df)\n",
    "test_df = qaqc_climatological_outlier(df, winsorize=True, plot=True)\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fc457f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>time</th>\n",
       "      <th>ps</th>\n",
       "      <th>tas</th>\n",
       "      <th>pr</th>\n",
       "      <th>hurs</th>\n",
       "      <th>rsds</th>\n",
       "      <th>sfcWind</th>\n",
       "      <th>sfcWind_dir</th>\n",
       "      <th>ps_qc</th>\n",
       "      <th>...</th>\n",
       "      <th>pr_1h_qc</th>\n",
       "      <th>rsds_qc</th>\n",
       "      <th>tdps_derived</th>\n",
       "      <th>elevation</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>tas_eraqc</th>\n",
       "      <th>tdps_derived_eraqc</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [station, time, ps, tas, pr, hurs, rsds, sfcWind, sfcWind_dir, ps_qc, tas_qc, hurs_qc, sfcWind_qc, sfcWind_dir_qc, pr_1h, pr_1h_qc, rsds_qc, tdps_derived, elevation, lat, lon, tas_eraqc, tdps_derived_eraqc, month, year]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag = 25\n",
    "test_df.loc[(test_df['tdps_derived_eraqc'] == flag) | (test_df['tas_eraqc'] == flag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "144e541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103028\n",
      "tas 1 8669\n"
     ]
    }
   ],
   "source": [
    "var='tas'\n",
    "\n",
    "# df = df.reset_index()\n",
    "\n",
    "vars_to_anom = ['tas']\n",
    "df_valid = df.loc[df[var+'_eraqc'].isnull()]\n",
    "print(len(df_valid))\n",
    "\n",
    "# winsorize data by percentiles\n",
    "df_std = winsorize_temps(df_valid, vars_to_anom, winz_limits=[0.05, 0.05])\n",
    "\n",
    "# standardize data by monthly climatological anomalies by hour\n",
    "df_std = clim_standardized_anom(df_std, vars_to_anom)\n",
    "\n",
    "# apply low pass filter\n",
    "df_std = low_pass_filter(df_std, vars_to_anom)\n",
    "\n",
    "# gaussian is fitted to the histogram of anomalies for each month\n",
    "# similar to distributional gap check\n",
    "# FUTURE DEV (v2 of product):\n",
    "    # HadISD: obs that fall between critical threshold value and gap or critical threshold and end of distribution are tentatively flagged\n",
    "    # May be later reinstated on comparison with good data from neighboring stations\n",
    "\n",
    "for month in range(1,2):\n",
    "\n",
    "    df_m = df_std.loc[df_valid.time.dt.month == month]\n",
    "    print(var, month, len(df_m))\n",
    "    # determine number of bins\n",
    "    bins = create_bins(df_m[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2cc67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
