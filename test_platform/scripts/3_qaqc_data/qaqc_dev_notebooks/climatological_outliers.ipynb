{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ee035f",
   "metadata": {},
   "source": [
    "Motivation: individual gross outliers from general station distribution are a common error in obs data by random recording, reporting, formatting, or instrumentation errors\n",
    "\n",
    "Process:\n",
    "1. uses individual observation deviations derived from monthly mean climatology calculated for each hour of the day\n",
    "2. climatologies calculated using winsorised data to remove initial effect of outliers\n",
    "    - Winsorising: all values beyond threhsold value from mean are set to that threshold value\n",
    "    - 5 and 95% for hadisd\n",
    "    - number of data values in population remains the same, not trimmed\n",
    "3. raw unwinsorised observations are anomalised using these climatologies\n",
    "4. standardized by IQR for that month and hour\n",
    "    - IQR cannot be less than 1.5degC\n",
    "5. values are low-pass filtered to remove any climate change signal causing overzealous removal at ends of time series\n",
    "6. gaussian is fitted to the histogram of anomalies for each month\n",
    "7. threshold value, rounded outwards where crosses y=0.1 line\n",
    "8. distribution beyond threhsold value is scanned for gap, equal to bin width or more\n",
    "9. all values beyond gap are flagged\n",
    "10. obs that fall between critical threshold value and gap or critical threshold and end of distribution are tentatively flagged\n",
    "    - these may be later reinstated on comparison with good data from neighboring stations\n",
    "\n",
    "Notes:\n",
    "- when applied to SLP, frequently flags storm signals, which may be of high interest, so this test is not applied to pressure data\n",
    "- hadisd only applies to temp and dewpoint temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92465473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1e1ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoriaford/anaconda3/envs/histobs/lib/python3.11/site-packages/pandas/core/arrays/timedeltas.py:908: RuntimeWarning: invalid value encountered in cast\n",
      "  base = data.astype(np.int64)\n",
      "/Users/victoriaford/anaconda3/envs/histobs/lib/python3.11/site-packages/pandas/core/arrays/timedeltas.py:912: RuntimeWarning: invalid value encountered in cast\n",
      "  data = (base * m + (frac * m).astype(np.int64)).view(\"timedelta64[ns]\")\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset('/Users/victoriaford/Desktop/eaglerock/Historical Data Platform/Train_Files/ASOSAWOS_72051724165.nc')\n",
    "\n",
    "df = ds.to_dataframe()\n",
    "# df = df.reset_index()\n",
    "# df['month'] = pd.to_datetime(df['time']).dt.month # sets month to new variable\n",
    "# df['year'] = pd.to_datetime(df['time']).dt.year # sets year to new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48eba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_mon_mean_hourly(df, var, month, hour):\n",
    "    '''Calculate the monthly mean climatology for each of the day'''\n",
    "    \n",
    "    df_m_h = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)]\n",
    "    clim_value = df_m_h[var].mean(numeric_only = True)\n",
    "    \n",
    "    # special handling if value is nan? \n",
    "    \n",
    "    return clim_value\n",
    "\n",
    "def iqr_range_monhour(df, var, month, hour):\n",
    "    '''Calculates the monthly interquartile range per hour'''\n",
    "    \n",
    "    q1 = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)].quantile(0.25, numeric_only=True)\n",
    "    q3 = df.loc[(df.time.dt.month == month) & (df.time.dt.hour == hour)].quantile(0.75, numeric_only=True)\n",
    "    \n",
    "    iqr_df = q3 - q1\n",
    "    iqr_df_val = iqr_df[var]\n",
    "    \n",
    "    # iqr cannot be less than 1.5Â°C in order to preserve low variance stations\n",
    "    if iqr_df_val < 1.5:\n",
    "        iqr_df_val = 1.5\n",
    "    else:\n",
    "        iqr_df_val = iqr_df_val\n",
    "            \n",
    "    return iqr_df_val\n",
    "\n",
    "\n",
    "def clim_standardized_anom(df, vars_to_anom):\n",
    "    '''\n",
    "    First anomalizes data by monthly climatology for each hour, then\n",
    "    standardizes by the monthly climatological anomaly IQR for each hour\n",
    "    '''\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        for m in range(1,13,1):\n",
    "            for h in range(0,24,1):\n",
    "                # each hour in each month\n",
    "                anom_value = clim_mon_mean_hourly(df, var, month=m, hour=h)\n",
    "                iqr_value = iqr_range_monhour(df, var, month=m, hour=h)\n",
    "                \n",
    "                # locate obs within specific month/hour\n",
    "                df_m_h = df.loc[(df.time.dt.month == m) & (df.time.dt.hour == h)]\n",
    "                \n",
    "                # calculate the monthly climatological anomaly by hour and standardize by iqr\n",
    "                df2.loc[(df.time.dt.month == m) & \n",
    "                        (df.time.dt.hour == h), \n",
    "                        var] = (df_m_h[var] - anom_value) / iqr_value\n",
    "                \n",
    "    return df2\n",
    "\n",
    "def winsorize_temps(df, vars_to_anom, winz_limits):\n",
    "    '''\n",
    "    Replaces potential spurious outliers by limiting the extreme values\n",
    "    using the winz_limits set (default is 5% and 95% percentiles)\n",
    "    '''\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        for m in range(1,13,1):\n",
    "            for h in range(0,24,1):\n",
    "                \n",
    "                df_m_h = df.loc[(df.time.dt.month == m) & (df.time.dt.hour == h)]\n",
    "                \n",
    "                # winsorize only vars in vars_to_anom\n",
    "                df_w = winsorize(df_m_h[var], limits=winz_limits, nan_policy='omit')\n",
    "                \n",
    "                df2.loc[(df.time.dt.month == m) & (df.time.dt.hour == h),\n",
    "                       var] = df_w\n",
    "                \n",
    "    return df2\n",
    "\n",
    "# get average anomaly per year\n",
    "def median_yr_anom(df, var):\n",
    "    '''Get median anomaly per year'''\n",
    "    \n",
    "    monthly_anoms = []\n",
    "    \n",
    "    # identify years in data\n",
    "    years = df.time.dt.year.unique()\n",
    "    \n",
    "    for yr in years:\n",
    "        df_yr = df.loc[df.time.dt.year == yr]\n",
    "\n",
    "        ann_anom = df_yr[var].median()\n",
    "        monthly_anoms.append(ann_anom)\n",
    "        \n",
    "    return monthly_anoms\n",
    "\n",
    "def low_pass_filter_weights(median_anoms, month_low, month_high, filter_low, filter_high):\n",
    "    '''Calculates weights for low pass filter'''\n",
    "    \n",
    "    filter_wgts = [1, 2, 3, 2, 1]\n",
    "    \n",
    "    if np.sum(filter_wgts[filter_low:filter_high] * \n",
    "              np.ceil(median_anoms[month_low:month_high] - \n",
    "                      np.floor(median_anoms[month_low:month_high]))) == 0:\n",
    "        weight = 0\n",
    "    \n",
    "    else:\n",
    "        weight = (\n",
    "            np.sum(filter_wgts[filter_low:filter_high] * np.ceil(median_anoms[month_low:month_high])) / \n",
    "            np.sum(filter_wgts[filter_low:filter_high] * np.ceil(median_anoms[month_low:month_high] - \n",
    "                                                                 np.floor(median_anoms[month_low:month_high])))\n",
    "        )\n",
    "        \n",
    "    return weight\n",
    "\n",
    "def low_pass_filter(df, vars_to_anom):\n",
    "    '''\n",
    "    Low pass filtering on observations to remove any climate change signal \n",
    "    causing overzealous removal at ends of time series\n",
    "    '''\n",
    "    # identify years in data\n",
    "    years = df.time.dt.year.unique()\n",
    "    \n",
    "    for var in vars_to_anom:\n",
    "        \n",
    "        median_anoms = median_yr_anom(df, var)\n",
    "    \n",
    "        for yr in range(len(years)):\n",
    "            if yr == 0:\n",
    "                month_low, month_high = 0, 3\n",
    "                filter_low, filter_high = 2, 5\n",
    "                \n",
    "            elif yr == 1:\n",
    "                month_low, month_high = 0, 4\n",
    "                filter_low, filter_high = 1, 5\n",
    "                \n",
    "            elif yr == len(years)-2:\n",
    "                month_low, month_high = -4, -1\n",
    "                filter_low, filter_high = 0, 3\n",
    "\n",
    "            elif yr == len(years)-1:\n",
    "                month_low, month_high = -3, -1\n",
    "                filter_low, filter_high = 0, 2\n",
    "\n",
    "            else:\n",
    "                month_low, month_high = yr-2, yr+3\n",
    "                filter_low, filter_high = 0, 5\n",
    "                            \n",
    "            if np.sum(np.abs(median_anoms[month_low:month_high])) != 0:\n",
    "                weights = low_pass_filter_weights(median_anoms, month_low, month_high, filter_low, filter_high)\n",
    "                      \n",
    "            # want to return specific year of data at a specific variable, the variable minus weight value\n",
    "            df.loc[(df.time.dt.year == years[yr]), var] = df.loc[df.time.dt.year == years[yr]][var] - weights\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "## distribution gap plotting helpers\n",
    "def create_bins(data, bin_size=0.25):\n",
    "    '''Create bins from data covering entire data range'''\n",
    "\n",
    "    # set up bins\n",
    "    b_min = np.floor(np.nanmin(data))\n",
    "    b_max = np.ceil(np.nanmax(data))\n",
    "    bins = np.arange(b_min - bin_size, b_max + (3. * bin_size), bin_size)\n",
    "\n",
    "    return bins\n",
    "\n",
    "def pdf_bounds(df, mu, sigma, bins):\n",
    "    '''Calculate pdf distribution, return pdf and threshold bounds'''\n",
    "\n",
    "    y = stats.norm.pdf(bins, mu, sigma)\n",
    "    \n",
    "    # add vertical lines to indicate thresholds where pdf y=0.1\n",
    "    pdf_bounds = np.argwhere(y > 0.1)\n",
    "\n",
    "    # find first index\n",
    "    left_bnd = round(bins[pdf_bounds[0][0] -1])\n",
    "    right_bnd = round(bins[pdf_bounds[-1][0] + 1])\n",
    "    thresholds = (left_bnd - 1, right_bnd + 1)\n",
    "    \n",
    "    return (y, left_bnd - 1, right_bnd + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97a5967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_climatological_outlier(df, winsorize=True, winz_limits=[0.05,0.05], plot=True, verbose=True):\n",
    "    '''\n",
    "    Flags individual gross outliers from climatological distribution.\n",
    "    Only applied to air temperature and dew point temperature\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "        df [pd.DataFrame]: station dataset converted to dataframe through QAQC pipeline\n",
    "        plots [bool]: if True, produces plots of any flagged data and saved to AWS\n",
    "        winsorize [bool]: if True, raw observations are winsorized to remove spurious outliers first\n",
    "        winz_limits [list]: if winsorize is True, values represent the low and high percentiles to standardize to\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "        qaqc success:\n",
    "            df [pd.DataFrame]: QAQC dataframe with flagged values (see below for flag meaning)\n",
    "        qaqc failure:\n",
    "            None\n",
    "            \n",
    "    Flag meaning:\n",
    "    -------------\n",
    "        25,qaqc_climatological_outlier,Value flagged as a climatological outlier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #### ONLY IN NOTEBOOK DEVELOPMENT, REMOVED IN CODE FOR PIPELINE #####\n",
    "    df = df.reset_index()\n",
    "    df['month'] = pd.to_datetime(df['time']).dt.month # sets month to new variable\n",
    "    df['year'] = pd.to_datetime(df['time']).dt.year # sets year to new variable\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    \n",
    "    vars_to_check = ['tas', 'tdps', 'tdps_derived']\n",
    "    vars_to_anom = [v for v in vars_to_check if v in df.columns]\n",
    "    \n",
    "    # TO DO: filter to only use non-flagged data\n",
    "\n",
    "    # winsorize data by percentiles\n",
    "    if winsorize == True:\n",
    "        df = winsorize_temps(df, vars_to_anom, winz_limits)\n",
    "    else:\n",
    "        df = df\n",
    "        \n",
    "    # standardize data by monthly climatological anomalies by hour\n",
    "    df = clim_standardized_anom(df, vars_to_anom)\n",
    "\n",
    "    # apply low pass filter\n",
    "    df = low_pass_filter(df, vars_to_anom)\n",
    "        \n",
    "    # gaussian is fitted to the histogram of anomalies for each month\n",
    "        # threshold value, rounded outwards where crosses y=0.1 line\n",
    "        # distribution beyond threhsold value is scanned for gap, equal to bin width or more\n",
    "        # all values beyond gap are flagged\n",
    "            # HadISD: obs that fall between critical threshold value and gap or \n",
    "            # critical threshold and end of distribution are tentatively flagged\n",
    "            # these may be later reinstated on comparison with good data from neighboring stations\n",
    "            # into v2 of data product\n",
    "            \n",
    "    for var in vars_to_anom:\n",
    "        for month in range(1,13):\n",
    "            print(var, month)\n",
    "            \n",
    "            df = df.loc[df.time.dt.month == month]\n",
    "            \n",
    "            # determine number of bins\n",
    "            bins = create_bins(df[var])\n",
    "\n",
    "            # pdf\n",
    "            mu = np.nanmean(df[var])\n",
    "            sigma = np.nanstd(df[var])\n",
    "\n",
    "            y, left_bnd, right_bnd = pdf_bounds(df[var], mu, sigma, bins)\n",
    "            \n",
    "            # df index of where each obs falls in which hist bin\n",
    "            d = np.digitize(df[var], bins, right=True)\n",
    "            d = d-1 # start index at 0\n",
    "\n",
    "            # identify gaps as below y=0.1 from histogram, not pdf\n",
    "            y_hist, bins = np.histogram(df[var], bins=bins, density=True)\n",
    "            print(len(bins))\n",
    "            # bins are flagged for values beyond left_bnd, right_bnd\n",
    "            bins_beyond_left_bnd = np.argwhere(bins <= left_bnd)\n",
    "            if len(bins_beyond_left_bnd) != 0:\n",
    "                for data in bins_beyond_left_bnd:\n",
    "                    if y_hist[data] > 0.1: # bins with data > 0.1 beyond left_bnd\n",
    "                        # identify values to flag\n",
    "                        print(data)\n",
    "                        idx_to_flag = np.argwhere(d == data)\n",
    "                        for idx in idx_to_flag:\n",
    "                            df.iloc[idx, var+'_eraqc'] = 25 # see era_qaqc_flag_meanings.csv\n",
    "\n",
    "            bins_beyond_right_bnd = np.argwhere(bins >= right_bnd)\n",
    "            print(bins_beyond_right_bnd)\n",
    "            if len(bins_beyond_right_bnd) != 0:\n",
    "                for data in bins_beyond_right_bnd:\n",
    "                    if y_hist[data] > 0.1: # bins with data > 0.1 beyond right_bnd\n",
    "                        # identify values to flag\n",
    "                        print(data)\n",
    "                        idx_to_flag = np.argwhere(d == data)\n",
    "                        for idx in idx_to_flag:\n",
    "                            df.iloc[idx, var+'_eraqc'] = 25 # see era_qaqc_flag_meanings.csv\n",
    "                \n",
    "    if plot == True:\n",
    "        for var in vars_to_anom:\n",
    "            if 25 in df2[var+'_eraqc'].values: # only plot a figure if flag is present\n",
    "                clim_outlier_plot(df2, var, network=df['station'].unique())\n",
    "                \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e67740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tas 1\n",
      "20\n",
      "[[17]\n",
      " [18]\n",
      " [19]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 19 is out of bounds for axis 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mqaqc_climatological_outlier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_df\n",
      "Cell \u001b[0;32mIn[69], line 96\u001b[0m, in \u001b[0;36mqaqc_climatological_outlier\u001b[0;34m(df, winsorize, winz_limits, plot, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins_beyond_right_bnd) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m bins_beyond_right_bnd:\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my_hist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.1\u001b[39m: \u001b[38;5;66;03m# bins with data > 0.1 beyond right_bnd\u001b[39;00m\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;66;03m# identify values to flag\u001b[39;00m\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m     99\u001b[0m             idx_to_flag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(d \u001b[38;5;241m==\u001b[39m data)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 19 is out of bounds for axis 0 with size 19"
     ]
    }
   ],
   "source": [
    "test_df = qaqc_climatological_outlier(df, plot=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73ef9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df['month'] = pd.to_datetime(df['time']).dt.month # sets month to new variable\n",
    "df['year'] = pd.to_datetime(df['time']).dt.year # sets year to new variable\n",
    "\n",
    "vars_to_check = ['tas', 'tdps', 'tdps_derived']\n",
    "vars_to_anom = [v for v in vars_to_check if v in df.columns]\n",
    "\n",
    "# TO DO: filter to only use non-flagged data\n",
    "\n",
    "# winsorize data by percentiles\n",
    "df = winsorize_temps(df, vars_to_anom, winz_limits=[0.05, 0.05])\n",
    "\n",
    "# standardize data by monthly climatological anomalies by hour\n",
    "df = clim_standardized_anom(df, vars_to_anom)\n",
    "\n",
    "# apply low pass filter\n",
    "df = low_pass_filter(df, vars_to_anom)\n",
    "\n",
    "\n",
    "month=1\n",
    "var='tas'\n",
    "\n",
    "dfm = df.loc[df.time.dt.month==month]\n",
    "# determine number of bins\n",
    "bins = create_bins(dfm[var])\n",
    "\n",
    "# pdf\n",
    "mu = np.nanmean(dfm[var])\n",
    "sigma = np.nanstd(dfm[var])\n",
    "\n",
    "y, left_bnd, right_bnd = pdf_bounds(dfm[var], mu, sigma, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59a7563f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_bnd, right_bnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6124c84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , 0.        , 0.01553716,\n",
       "        0.09485843, 0.34099969, 0.4644792 , 0.58427885, 0.73556169,\n",
       "        0.73474394, 0.73924154, 0.24205254, 0.03884289, 0.00940407,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " array([-3.25, -3.  , -2.75, -2.5 , -2.25, -2.  , -1.75, -1.5 , -1.25,\n",
       "        -1.  , -0.75, -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ,\n",
       "         1.25,  1.5 ]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hist, bins = np.histogram(dfm[var], bins=bins, density=True)\n",
    "y_hist, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e2aa6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12, 12, ...,  9,  8,  8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.digitize(dfm[var], bins)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "769a7efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(d), np.max(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bins))\n",
    "# bins are flagged for values beyond left_bnd, right_bnd\n",
    "bins_beyond_left_bnd = np.argwhere(bins <= left_bnd)\n",
    "if len(bins_beyond_left_bnd) != 0:\n",
    "    for data in bins_beyond_left_bnd:\n",
    "        if y_hist[data] > 0.1: # bins with data > 0.1 beyond left_bnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d842a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029f413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b32aa25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9003   NaN\n",
      "Name: tas, dtype: float64\n",
      "9101   NaN\n",
      "Name: tas, dtype: float64\n",
      "25330   NaN\n",
      "Name: tas, dtype: float64\n",
      "25376   NaN\n",
      "Name: tas, dtype: float64\n",
      "25424   NaN\n",
      "Name: tas, dtype: float64\n",
      "25425   NaN\n",
      "Name: tas, dtype: float64\n",
      "25449   NaN\n",
      "Name: tas, dtype: float64\n",
      "25682   NaN\n",
      "Name: tas, dtype: float64\n",
      "25683   NaN\n",
      "Name: tas, dtype: float64\n",
      "25684   NaN\n",
      "Name: tas, dtype: float64\n",
      "25685   NaN\n",
      "Name: tas, dtype: float64\n",
      "25686   NaN\n",
      "Name: tas, dtype: float64\n",
      "25687   NaN\n",
      "Name: tas, dtype: float64\n",
      "25688   NaN\n",
      "Name: tas, dtype: float64\n",
      "25689   NaN\n",
      "Name: tas, dtype: float64\n",
      "25690   NaN\n",
      "Name: tas, dtype: float64\n",
      "25691   NaN\n",
      "Name: tas, dtype: float64\n",
      "25692   NaN\n",
      "Name: tas, dtype: float64\n",
      "25693   NaN\n",
      "Name: tas, dtype: float64\n",
      "25694   NaN\n",
      "Name: tas, dtype: float64\n",
      "25695   NaN\n",
      "Name: tas, dtype: float64\n",
      "25696   NaN\n",
      "Name: tas, dtype: float64\n",
      "25697   NaN\n",
      "Name: tas, dtype: float64\n",
      "25698   NaN\n",
      "Name: tas, dtype: float64\n",
      "25699   NaN\n",
      "Name: tas, dtype: float64\n",
      "25700   NaN\n",
      "Name: tas, dtype: float64\n",
      "25701   NaN\n",
      "Name: tas, dtype: float64\n",
      "25702   NaN\n",
      "Name: tas, dtype: float64\n",
      "25703   NaN\n",
      "Name: tas, dtype: float64\n",
      "25704   NaN\n",
      "Name: tas, dtype: float64\n",
      "25705   NaN\n",
      "Name: tas, dtype: float64\n",
      "25706   NaN\n",
      "Name: tas, dtype: float64\n",
      "25707   NaN\n",
      "Name: tas, dtype: float64\n",
      "25708   NaN\n",
      "Name: tas, dtype: float64\n",
      "25709   NaN\n",
      "Name: tas, dtype: float64\n",
      "25710   NaN\n",
      "Name: tas, dtype: float64\n",
      "25711   NaN\n",
      "Name: tas, dtype: float64\n",
      "25712   NaN\n",
      "Name: tas, dtype: float64\n",
      "25713   NaN\n",
      "Name: tas, dtype: float64\n",
      "25714   NaN\n",
      "Name: tas, dtype: float64\n",
      "25715   NaN\n",
      "Name: tas, dtype: float64\n",
      "25716   NaN\n",
      "Name: tas, dtype: float64\n",
      "25717   NaN\n",
      "Name: tas, dtype: float64\n",
      "25718   NaN\n",
      "Name: tas, dtype: float64\n",
      "25719   NaN\n",
      "Name: tas, dtype: float64\n",
      "25720   NaN\n",
      "Name: tas, dtype: float64\n",
      "25721   NaN\n",
      "Name: tas, dtype: float64\n",
      "25722   NaN\n",
      "Name: tas, dtype: float64\n",
      "25723   NaN\n",
      "Name: tas, dtype: float64\n",
      "25724   NaN\n",
      "Name: tas, dtype: float64\n",
      "25725   NaN\n",
      "Name: tas, dtype: float64\n",
      "25726   NaN\n",
      "Name: tas, dtype: float64\n",
      "25727   NaN\n",
      "Name: tas, dtype: float64\n",
      "25728   NaN\n",
      "Name: tas, dtype: float64\n",
      "25729   NaN\n",
      "Name: tas, dtype: float64\n",
      "25730   NaN\n",
      "Name: tas, dtype: float64\n",
      "25731   NaN\n",
      "Name: tas, dtype: float64\n",
      "25732   NaN\n",
      "Name: tas, dtype: float64\n",
      "25733   NaN\n",
      "Name: tas, dtype: float64\n",
      "25734   NaN\n",
      "Name: tas, dtype: float64\n",
      "25735   NaN\n",
      "Name: tas, dtype: float64\n",
      "25736   NaN\n",
      "Name: tas, dtype: float64\n",
      "25737   NaN\n",
      "Name: tas, dtype: float64\n",
      "25738   NaN\n",
      "Name: tas, dtype: float64\n",
      "25739   NaN\n",
      "Name: tas, dtype: float64\n",
      "25740   NaN\n",
      "Name: tas, dtype: float64\n",
      "25741   NaN\n",
      "Name: tas, dtype: float64\n",
      "25742   NaN\n",
      "Name: tas, dtype: float64\n",
      "25781   NaN\n",
      "Name: tas, dtype: float64\n",
      "25854   NaN\n",
      "Name: tas, dtype: float64\n",
      "25927   NaN\n",
      "Name: tas, dtype: float64\n",
      "25973   NaN\n",
      "Name: tas, dtype: float64\n",
      "25974   NaN\n",
      "Name: tas, dtype: float64\n",
      "25975   NaN\n",
      "Name: tas, dtype: float64\n",
      "26098   NaN\n",
      "Name: tas, dtype: float64\n",
      "26150   NaN\n",
      "Name: tas, dtype: float64\n",
      "26280   NaN\n",
      "Name: tas, dtype: float64\n",
      "26396   NaN\n",
      "Name: tas, dtype: float64\n",
      "26606   NaN\n",
      "Name: tas, dtype: float64\n",
      "26679   NaN\n",
      "Name: tas, dtype: float64\n",
      "26802   NaN\n",
      "Name: tas, dtype: float64\n",
      "45589   NaN\n",
      "Name: tas, dtype: float64\n",
      "45605   NaN\n",
      "Name: tas, dtype: float64\n",
      "45611   NaN\n",
      "Name: tas, dtype: float64\n",
      "45612   NaN\n",
      "Name: tas, dtype: float64\n",
      "45613   NaN\n",
      "Name: tas, dtype: float64\n",
      "45620   NaN\n",
      "Name: tas, dtype: float64\n",
      "45624   NaN\n",
      "Name: tas, dtype: float64\n",
      "45625   NaN\n",
      "Name: tas, dtype: float64\n",
      "45629   NaN\n",
      "Name: tas, dtype: float64\n",
      "45640   NaN\n",
      "Name: tas, dtype: float64\n",
      "45706   NaN\n",
      "Name: tas, dtype: float64\n",
      "45776   NaN\n",
      "Name: tas, dtype: float64\n",
      "45827   NaN\n",
      "Name: tas, dtype: float64\n",
      "45841   NaN\n",
      "Name: tas, dtype: float64\n",
      "57883   NaN\n",
      "Name: tas, dtype: float64\n",
      "57888   NaN\n",
      "Name: tas, dtype: float64\n",
      "57898   NaN\n",
      "Name: tas, dtype: float64\n",
      "57922   NaN\n",
      "Name: tas, dtype: float64\n",
      "57972   NaN\n",
      "Name: tas, dtype: float64\n",
      "58058   NaN\n",
      "Name: tas, dtype: float64\n",
      "58115   NaN\n",
      "Name: tas, dtype: float64\n",
      "58147   NaN\n",
      "Name: tas, dtype: float64\n",
      "58190   NaN\n",
      "Name: tas, dtype: float64\n",
      "58207   NaN\n",
      "Name: tas, dtype: float64\n",
      "58220   NaN\n",
      "Name: tas, dtype: float64\n",
      "58221   NaN\n",
      "Name: tas, dtype: float64\n",
      "58222   NaN\n",
      "Name: tas, dtype: float64\n",
      "58223   NaN\n",
      "Name: tas, dtype: float64\n",
      "58224   NaN\n",
      "Name: tas, dtype: float64\n",
      "58225   NaN\n",
      "Name: tas, dtype: float64\n",
      "58226   NaN\n",
      "Name: tas, dtype: float64\n",
      "58227   NaN\n",
      "Name: tas, dtype: float64\n",
      "58228   NaN\n",
      "Name: tas, dtype: float64\n",
      "58229   NaN\n",
      "Name: tas, dtype: float64\n",
      "58230   NaN\n",
      "Name: tas, dtype: float64\n",
      "58231   NaN\n",
      "Name: tas, dtype: float64\n",
      "58232   NaN\n",
      "Name: tas, dtype: float64\n",
      "58237   NaN\n",
      "Name: tas, dtype: float64\n",
      "58238   NaN\n",
      "Name: tas, dtype: float64\n",
      "58239   NaN\n",
      "Name: tas, dtype: float64\n",
      "58240   NaN\n",
      "Name: tas, dtype: float64\n",
      "58241   NaN\n",
      "Name: tas, dtype: float64\n",
      "58242   NaN\n",
      "Name: tas, dtype: float64\n",
      "58243   NaN\n",
      "Name: tas, dtype: float64\n",
      "58244   NaN\n",
      "Name: tas, dtype: float64\n",
      "58245   NaN\n",
      "Name: tas, dtype: float64\n",
      "58246   NaN\n",
      "Name: tas, dtype: float64\n",
      "58247   NaN\n",
      "Name: tas, dtype: float64\n",
      "58248   NaN\n",
      "Name: tas, dtype: float64\n",
      "58249   NaN\n",
      "Name: tas, dtype: float64\n",
      "58250   NaN\n",
      "Name: tas, dtype: float64\n",
      "58251   NaN\n",
      "Name: tas, dtype: float64\n",
      "58252   NaN\n",
      "Name: tas, dtype: float64\n",
      "58253   NaN\n",
      "Name: tas, dtype: float64\n",
      "58254   NaN\n",
      "Name: tas, dtype: float64\n",
      "58255   NaN\n",
      "Name: tas, dtype: float64\n",
      "58256   NaN\n",
      "Name: tas, dtype: float64\n",
      "58257   NaN\n",
      "Name: tas, dtype: float64\n",
      "58258   NaN\n",
      "Name: tas, dtype: float64\n",
      "58259   NaN\n",
      "Name: tas, dtype: float64\n",
      "58260   NaN\n",
      "Name: tas, dtype: float64\n",
      "58261   NaN\n",
      "Name: tas, dtype: float64\n",
      "58262   NaN\n",
      "Name: tas, dtype: float64\n",
      "58263   NaN\n",
      "Name: tas, dtype: float64\n",
      "58264   NaN\n",
      "Name: tas, dtype: float64\n",
      "58274   NaN\n",
      "Name: tas, dtype: float64\n",
      "58277   NaN\n",
      "Name: tas, dtype: float64\n",
      "58278   NaN\n",
      "Name: tas, dtype: float64\n",
      "58279   NaN\n",
      "Name: tas, dtype: float64\n",
      "58280   NaN\n",
      "Name: tas, dtype: float64\n",
      "58281   NaN\n",
      "Name: tas, dtype: float64\n",
      "58282   NaN\n",
      "Name: tas, dtype: float64\n",
      "58283   NaN\n",
      "Name: tas, dtype: float64\n",
      "58284   NaN\n",
      "Name: tas, dtype: float64\n",
      "58285   NaN\n",
      "Name: tas, dtype: float64\n",
      "58286   NaN\n",
      "Name: tas, dtype: float64\n",
      "58325   NaN\n",
      "Name: tas, dtype: float64\n",
      "58326   NaN\n",
      "Name: tas, dtype: float64\n",
      "58327   NaN\n",
      "Name: tas, dtype: float64\n",
      "58328   NaN\n",
      "Name: tas, dtype: float64\n",
      "58329   NaN\n",
      "Name: tas, dtype: float64\n",
      "58330   NaN\n",
      "Name: tas, dtype: float64\n",
      "58335   NaN\n",
      "Name: tas, dtype: float64\n",
      "58336   NaN\n",
      "Name: tas, dtype: float64\n",
      "58368   NaN\n",
      "Name: tas, dtype: float64\n",
      "58379   NaN\n",
      "Name: tas, dtype: float64\n",
      "58429   NaN\n",
      "Name: tas, dtype: float64\n",
      "58459   NaN\n",
      "Name: tas, dtype: float64\n",
      "94001   NaN\n",
      "Name: tas, dtype: float64\n",
      "94225   NaN\n",
      "Name: tas, dtype: float64\n",
      "94295   NaN\n",
      "Name: tas, dtype: float64\n",
      "94296   NaN\n",
      "Name: tas, dtype: float64\n",
      "94297   NaN\n",
      "Name: tas, dtype: float64\n",
      "94298   NaN\n",
      "Name: tas, dtype: float64\n",
      "94299   NaN\n",
      "Name: tas, dtype: float64\n",
      "94300   NaN\n",
      "Name: tas, dtype: float64\n",
      "94301   NaN\n",
      "Name: tas, dtype: float64\n",
      "94302   NaN\n",
      "Name: tas, dtype: float64\n",
      "94303   NaN\n",
      "Name: tas, dtype: float64\n",
      "94304   NaN\n",
      "Name: tas, dtype: float64\n",
      "94305   NaN\n",
      "Name: tas, dtype: float64\n",
      "94306   NaN\n",
      "Name: tas, dtype: float64\n",
      "94307   NaN\n",
      "Name: tas, dtype: float64\n",
      "94308   NaN\n",
      "Name: tas, dtype: float64\n",
      "94309   NaN\n",
      "Name: tas, dtype: float64\n",
      "94310   NaN\n",
      "Name: tas, dtype: float64\n",
      "94311   NaN\n",
      "Name: tas, dtype: float64\n",
      "94312   NaN\n",
      "Name: tas, dtype: float64\n",
      "94313   NaN\n",
      "Name: tas, dtype: float64\n",
      "94314   NaN\n",
      "Name: tas, dtype: float64\n",
      "94315   NaN\n",
      "Name: tas, dtype: float64\n",
      "94316   NaN\n",
      "Name: tas, dtype: float64\n",
      "94317   NaN\n",
      "Name: tas, dtype: float64\n",
      "94318   NaN\n",
      "Name: tas, dtype: float64\n",
      "94319   NaN\n",
      "Name: tas, dtype: float64\n",
      "94320   NaN\n",
      "Name: tas, dtype: float64\n",
      "94321   NaN\n",
      "Name: tas, dtype: float64\n",
      "94322   NaN\n",
      "Name: tas, dtype: float64\n",
      "94323   NaN\n",
      "Name: tas, dtype: float64\n",
      "94324   NaN\n",
      "Name: tas, dtype: float64\n",
      "94325   NaN\n",
      "Name: tas, dtype: float64\n",
      "94326   NaN\n",
      "Name: tas, dtype: float64\n",
      "94327   NaN\n",
      "Name: tas, dtype: float64\n",
      "94328   NaN\n",
      "Name: tas, dtype: float64\n",
      "94329   NaN\n",
      "Name: tas, dtype: float64\n",
      "94330   NaN\n",
      "Name: tas, dtype: float64\n",
      "94331   NaN\n",
      "Name: tas, dtype: float64\n",
      "94332   NaN\n",
      "Name: tas, dtype: float64\n",
      "94333   NaN\n",
      "Name: tas, dtype: float64\n",
      "94334   NaN\n",
      "Name: tas, dtype: float64\n",
      "94335   NaN\n",
      "Name: tas, dtype: float64\n",
      "94336   NaN\n",
      "Name: tas, dtype: float64\n",
      "94371   NaN\n",
      "Name: tas, dtype: float64\n",
      "94444   NaN\n",
      "Name: tas, dtype: float64\n",
      "94482   NaN\n",
      "Name: tas, dtype: float64\n",
      "118589   NaN\n",
      "Name: tas, dtype: float64\n",
      "118662   NaN\n",
      "Name: tas, dtype: float64\n",
      "118879   NaN\n",
      "Name: tas, dtype: float64\n",
      "118952   NaN\n",
      "Name: tas, dtype: float64\n",
      "119097   NaN\n",
      "Name: tas, dtype: float64\n",
      "119170   NaN\n",
      "Name: tas, dtype: float64\n",
      "119243   NaN\n",
      "Name: tas, dtype: float64\n",
      "119316   NaN\n",
      "Name: tas, dtype: float64\n",
      "119389   NaN\n",
      "Name: tas, dtype: float64\n",
      "119462   NaN\n",
      "Name: tas, dtype: float64\n",
      "119535   NaN\n",
      "Name: tas, dtype: float64\n",
      "119608   NaN\n",
      "Name: tas, dtype: float64\n",
      "119749   NaN\n",
      "Name: tas, dtype: float64\n",
      "120038   NaN\n",
      "Name: tas, dtype: float64\n",
      "120111   NaN\n",
      "Name: tas, dtype: float64\n",
      "120184   NaN\n",
      "Name: tas, dtype: float64\n",
      "120257   NaN\n",
      "Name: tas, dtype: float64\n",
      "120330   NaN\n",
      "Name: tas, dtype: float64\n",
      "120403   NaN\n",
      "Name: tas, dtype: float64\n",
      "120475   NaN\n",
      "Name: tas, dtype: float64\n",
      "120548   NaN\n",
      "Name: tas, dtype: float64\n",
      "120621   NaN\n",
      "Name: tas, dtype: float64\n",
      "120694   NaN\n",
      "Name: tas, dtype: float64\n",
      "142136   NaN\n",
      "Name: tas, dtype: float64\n",
      "142209   NaN\n",
      "Name: tas, dtype: float64\n",
      "142282   NaN\n",
      "Name: tas, dtype: float64\n",
      "142355   NaN\n",
      "Name: tas, dtype: float64\n",
      "142500   NaN\n",
      "Name: tas, dtype: float64\n",
      "142645   NaN\n",
      "Name: tas, dtype: float64\n",
      "142718   NaN\n",
      "Name: tas, dtype: float64\n",
      "142791   NaN\n",
      "Name: tas, dtype: float64\n",
      "142936   NaN\n",
      "Name: tas, dtype: float64\n",
      "143294   NaN\n",
      "Name: tas, dtype: float64\n",
      "143367   NaN\n",
      "Name: tas, dtype: float64\n",
      "143512   NaN\n",
      "Name: tas, dtype: float64\n",
      "143585   NaN\n",
      "Name: tas, dtype: float64\n",
      "143658   NaN\n",
      "Name: tas, dtype: float64\n",
      "143731   NaN\n",
      "Name: tas, dtype: float64\n",
      "143799   NaN\n",
      "Name: tas, dtype: float64\n",
      "143872   NaN\n",
      "Name: tas, dtype: float64\n",
      "144017   NaN\n",
      "Name: tas, dtype: float64\n",
      "144090   NaN\n",
      "Name: tas, dtype: float64\n",
      "144163   NaN\n",
      "Name: tas, dtype: float64\n",
      "163796   NaN\n",
      "Name: tas, dtype: float64\n",
      "163869   NaN\n",
      "Name: tas, dtype: float64\n",
      "164014   NaN\n",
      "Name: tas, dtype: float64\n",
      "164087   NaN\n",
      "Name: tas, dtype: float64\n",
      "164160   NaN\n",
      "Name: tas, dtype: float64\n",
      "164233   NaN\n",
      "Name: tas, dtype: float64\n",
      "164306   NaN\n",
      "Name: tas, dtype: float64\n",
      "164809   NaN\n",
      "Name: tas, dtype: float64\n",
      "165099   NaN\n",
      "Name: tas, dtype: float64\n",
      "165172   NaN\n",
      "Name: tas, dtype: float64\n",
      "165245   NaN\n",
      "Name: tas, dtype: float64\n",
      "165274   NaN\n",
      "Name: tas, dtype: float64\n",
      "165275   NaN\n",
      "Name: tas, dtype: float64\n",
      "165276   NaN\n",
      "Name: tas, dtype: float64\n",
      "165277   NaN\n",
      "Name: tas, dtype: float64\n",
      "165278   NaN\n",
      "Name: tas, dtype: float64\n",
      "165279   NaN\n",
      "Name: tas, dtype: float64\n",
      "165318   NaN\n",
      "Name: tas, dtype: float64\n",
      "165463   NaN\n",
      "Name: tas, dtype: float64\n",
      "165536   NaN\n",
      "Name: tas, dtype: float64\n",
      "165609   NaN\n",
      "Name: tas, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "idx = np.argwhere(d == 20)\n",
    "\n",
    "for i in idx:\n",
    "    print(dfm.iloc[i]['tas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26709a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station            ASOSAWOS_72051724165\n",
       "time                2015-01-01 19:50:00\n",
       "ps                              77550.0\n",
       "tas                           -2.765625\n",
       "tdps                          -2.685268\n",
       "pr                                  NaN\n",
       "sfcWind                             0.0\n",
       "sfcWind_dir                         NaN\n",
       "elevation                        2220.0\n",
       "qaqc_process                       V020\n",
       "ps_qc                                 5\n",
       "ps_altimeter                   101590.0\n",
       "ps_altimeter_qc                       5\n",
       "psl_qc                                9\n",
       "tas_qc                                7\n",
       "tdps_qc                               7\n",
       "pr_qc                                  \n",
       "pr_duration                         NaT\n",
       "pr_depth_qc                         NaN\n",
       "sfcWind_qc                            5\n",
       "sfcWind_method                        C\n",
       "sfcWind_dir_qc                        9\n",
       "lat                              41.824\n",
       "lon                            -110.556\n",
       "month                                 1\n",
       "year                               2015\n",
       "Name: 25304, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm.iloc[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1dab870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.00245324, 0.00858632, 0.04252274, 0.11530205,\n",
       "        0.11775529, 0.15169171, 0.24900337, 0.30338342, 0.49432689,\n",
       "        0.53480527, 0.69344782, 0.67954615, 0.42113871, 0.14310539,\n",
       "        0.03598078, 0.00531534, 0.00163549, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]),\n",
       " array([-3.25, -3.  , -2.75, -2.5 , -2.25, -2.  , -1.75, -1.5 , -1.25,\n",
       "        -1.  , -0.75, -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ,\n",
       "         1.25,  1.5 ,  1.75,  2.  ,  2.25,  2.5 ]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(dfm['tas'], bins=bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "117bc8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25, -3.  , -2.75, -2.5 , -2.25, -2.  , -1.75, -1.5 , -1.25,\n",
       "       -1.  , -0.75, -0.5 , -0.25,  0.  ,  0.25,  0.5 ,  0.75,  1.  ,\n",
       "        1.25,  1.5 ,  1.75,  2.  ,  2.25,  2.5 ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f778d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins[23]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
