{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does the following\n",
    "\n",
    "1. identifies stations that need to be concatenated using station location\n",
    "\n",
    "2. concatenates these target stations, with special handing for the following cases\n",
    "\n",
    "    a. subsets containing more than two stations\n",
    "\n",
    "    b. stations with temporal overlap\n",
    "\n",
    "3. moves/renames (under discussion) datasets that were input to concatenation to a separate folder in AWS (as opposed to deleting them outright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:48:44.156571Z",
     "iopub.status.busy": "2025-05-01T20:48:44.155847Z",
     "iopub.status.idle": "2025-05-01T20:48:45.932428Z",
     "shell.execute_reply": "2025-05-01T20:48:45.931874Z",
     "shell.execute_reply.started": "2025-05-01T20:48:44.156530Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# Import qaqc stage calc functions\n",
    "from QAQC_pipeline import  qaqc_ds_to_df\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:48:45.933600Z",
     "iopub.status.busy": "2025-05-01T20:48:45.933289Z",
     "iopub.status.idle": "2025-05-01T20:48:45.988065Z",
     "shell.execute_reply": "2025-05-01T20:48:45.987674Z",
     "shell.execute_reply.started": "2025-05-01T20:48:45.933589Z"
    }
   },
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify stations in the target network MARITIME, and ASOSAWOS that have exactly matching latitudes and longitudes. We then assign a unique ID to each susbet of stations identified and construct a dataframe of ERA-IDs and subset IDs, then send this to AWS for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:48:46.490711Z",
     "iopub.status.busy": "2025-05-01T20:48:46.489901Z",
     "iopub.status.idle": "2025-05-01T20:48:46.495717Z",
     "shell.execute_reply": "2025-05-01T20:48:46.494773Z",
     "shell.execute_reply.started": "2025-05-01T20:48:46.490665Z"
    }
   },
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"ASOSAWOS\",\"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:48:46.772490Z",
     "iopub.status.busy": "2025-05-01T20:48:46.771678Z",
     "iopub.status.idle": "2025-05-01T20:48:46.783851Z",
     "shell.execute_reply": "2025-05-01T20:48:46.782905Z",
     "shell.execute_reply.started": "2025-05-01T20:48:46.772444Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenation_check(station_list: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "    station_list: pd.DataFrame\n",
    "        list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success: returns input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "    if failure: None\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:48:47.415050Z",
     "iopub.status.busy": "2025-05-01T20:48:47.414297Z",
     "iopub.status.idle": "2025-05-01T20:48:47.427261Z",
     "shell.execute_reply": "2025-05-01T20:48:47.425580Z",
     "shell.execute_reply.started": "2025-05-01T20:48:47.415009Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    station__names_list: pd.DataFrame\n",
    "        list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success: uploads list of stations to be concatenated to AWS\n",
    "    if failure: None\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        # the csv is stored in each station folder within 3_qaqc_wx\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"/concat_list_{}.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concatenate Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatenation process is now split into a series of modular functions.\n",
    "\n",
    "The function concatenate_stations() does the following:\n",
    "\n",
    "- Reads in the concatenation stations list for a given network and, for each subset of stations\n",
    "\n",
    "    - if there are two stations in the subset, give those stations to helper function _df_concat()\n",
    "    - if there are more than two station in the subset, give those stations to helper function _more_than_2()\n",
    "    send final concatenated dataframe to _concat_export_help() for export\n",
    "\n",
    "The helper function _df_concat() concatenates pairs of input dataframes.\n",
    "\n",
    "- If there is temporal overlap between the two stations, it gives them to helpder function _overlap_concat()\n",
    "- If not, it merges the two stations\n",
    "\n",
    "\n",
    "The helper function _more_than_2() iteratively concatenates pairs of stations within a subset of more than two stations. It take the two oldest stations, gives them to _df_concat(). Then is takes the output from this and sends it and the NEXT station to _df_concat(), and so forth, until all stations have been concatenated.\n",
    "\n",
    "The helper function _overlap_concat() keeps the newer station data in the time range in which the two input stations overlap.\n",
    "\n",
    "Finally, the helper function _concat_export_help() renames the input datasets in AWS, then formats the concatenated dataframe for export, and then exports it to AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:33:39.649327Z",
     "iopub.status.busy": "2025-05-01T21:33:39.648517Z",
     "iopub.status.idle": "2025-05-01T21:33:39.663961Z",
     "shell.execute_reply": "2025-05-01T21:33:39.662876Z",
     "shell.execute_reply.started": "2025-05-01T21:33:39.649276Z"
    }
   },
   "outputs": [],
   "source": [
    "def _df_concat(\n",
    "    df_1: pd.DataFrame, df_2: pd.DataFrame, attrs_1: dict, attrs_2: dict\n",
    ") -> tuple[pd.DataFrame, str, str, dict]:\n",
    "    \"\"\"\n",
    "    Performs concatenation of input datasets, handling two cases\n",
    "        1.) temporal overlap between the datasets\n",
    "        2.) no temporal overlap\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_1: pd.DataFrame\n",
    "        station data\n",
    "    df_2: pd.DataFrame\n",
    "        dtation data\n",
    "    attrs_1: list of str\n",
    "        attributes of df_1\n",
    "    attrs_2: list of str\n",
    "        attributes of df_2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success:\n",
    "        df_concat: pd.DataFrame\n",
    "        stn_n_to_keep: str\n",
    "        stn_n_to_drop: str\n",
    "        attrs_new: dict\n",
    "    if failure: None\n",
    "    \"\"\"\n",
    "\n",
    "    # determine which dataset is older\n",
    "    if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "        # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "        # we also grab the name of the newer station in this step, for use later\n",
    "        df_new = df_2\n",
    "        attrs_new = attrs_2\n",
    "        df_old = df_1\n",
    "\n",
    "    else:\n",
    "        df_new = df_1\n",
    "        attrs_new = attrs_1\n",
    "        df_old = df_2\n",
    "\n",
    "    stn_n_to_keep = df_new[\"station\"].unique()[0]\n",
    "    stn_n_to_drop = df_old[\"station\"].unique()[0]\n",
    "    print(f\"Station will be concatenated and saved as: {stn_n_to_keep}\")\n",
    "\n",
    "    # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # If there is no overlap between the two time series, just concatenate\n",
    "    if len(df_overlap) == 0:\n",
    "        print(\"No overlap!\")\n",
    "        df_concat = pd.merge(df_old, df_new, how=\"outer\")\n",
    "\n",
    "    # If overlap exists, split into subsets and concatenate\n",
    "    else:\n",
    "        print(\"There is overlap\")\n",
    "        df_concat = _overlap_concat(df_old, df_new)\n",
    "\n",
    "    # Reset station name to be the newer station\n",
    "    df_concat[\"station\"] = stn_n_to_keep\n",
    "\n",
    "    return df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:33:42.155428Z",
     "iopub.status.busy": "2025-05-01T21:33:42.154653Z",
     "iopub.status.idle": "2025-05-01T21:33:42.164094Z",
     "shell.execute_reply": "2025-05-01T21:33:42.162446Z",
     "shell.execute_reply.started": "2025-05-01T21:33:42.155385Z"
    }
   },
   "outputs": [],
   "source": [
    "def _overlap_concat(df_new: pd.DataFrame, df_old: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles the cases in which there is overlap between the two input stations\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_new: pd.DataFrame\n",
    "        weather station network\n",
    "    df_old: pd.DataFrame\n",
    "        weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success: returns pd.DataFrame\n",
    "    if failure: None\n",
    "    \"\"\"\n",
    "\n",
    "    # identify where there is overlap in timestamps, and keep from newer station data\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "    print(f'Length of overlapping period: {len(df_overlap)}')\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = pd.concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:24:30.521506Z",
     "iopub.status.busy": "2025-05-01T21:24:30.520755Z",
     "iopub.status.idle": "2025-05-01T21:24:30.537265Z",
     "shell.execute_reply": "2025-05-01T21:24:30.535957Z",
     "shell.execute_reply.started": "2025-05-01T21:24:30.521466Z"
    }
   },
   "outputs": [],
   "source": [
    "def _more_than_2(network_name: str, stns_to_pair: pd.DataFrame) -> tuple[pd.DataFrame, dict, dict]:\n",
    "    \"\"\"\n",
    "    Performs pairwise concatenation on subsets of more than two stations flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: string\n",
    "        weather station network\n",
    "    stns_to_pair: pd.DataFrame\n",
    "        dataframe of the input station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success:\n",
    "        df_concat: pd.DataFrame\n",
    "        station_names: dict\n",
    "        attrs_new: dict\n",
    "    if failure: None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Concatenating the following stations:\", stns_to_pair)\n",
    "\n",
    "    # Load datasets into a list\n",
    "    datasets = [\n",
    "        xr.open_zarr(\n",
    "            \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stn\n",
    "            ),\n",
    "            consolidated=True,\n",
    "        )\n",
    "        for stn in stns_to_pair['ERA-ID']\n",
    "    ]\n",
    "\n",
    "    # Sort datasets by their max 'time'\n",
    "    datasets_sorted = sorted(datasets, key=lambda ds: ds['time'].max())\n",
    "\n",
    "    # Store station names, in order from oldest to newest\n",
    "    names = [ds.coords[\"station\"].values[0] for ds in datasets_sorted]\n",
    "\n",
    "    print('Newest station:', names[-1])\n",
    "\n",
    "    # Setup for the while loop\n",
    "    ds_1 = datasets_sorted[0]\n",
    "    df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(ds_1)\n",
    "    i = 0\n",
    "    end = len(datasets_sorted) -1\n",
    "\n",
    "    while i < end:\n",
    "\n",
    "        print('iteration:', i)\n",
    "\n",
    "        ds_2 = datasets_sorted[i+1]\n",
    "        df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(ds_2)\n",
    "\n",
    "        # Send to helper function for concatenation\n",
    "        df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "            df_1, df_2, attrs_1, attrs_2\n",
    "        )\n",
    "\n",
    "        df_1 = df_concat\n",
    "        attrs_1 = attrs_new\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Construct station names list, for updating attributes\n",
    "    newest_station = names[-1] # Get last station name from station name list\n",
    "    older_stations = \", \".join(names[:-1]) # Create a string containing all older station names\n",
    "    station_names = {\"station_name_new\": newest_station, \"old_stations\": older_stations}\n",
    "\n",
    "    print('Progressive concatenation for 2+ stations is complete.')\n",
    "\n",
    "    new_column = [newest_station] * len(df_concat)\n",
    "\n",
    "    df_concat['station'] = new_column\n",
    "\n",
    "    return df_concat, station_names, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:24:35.268737Z",
     "iopub.status.busy": "2025-05-01T21:24:35.267884Z",
     "iopub.status.idle": "2025-05-01T21:24:35.282854Z",
     "shell.execute_reply": "2025-05-01T21:24:35.281637Z",
     "shell.execute_reply.started": "2025-05-01T21:24:35.268692Z"
    }
   },
   "outputs": [],
   "source": [
    "def _concat_export_help(\n",
    "    df_concat: pd.DataFrame,\n",
    "    final_concat_list: list[str],\n",
    "    network_name: str,\n",
    "    attrs_new: dict,\n",
    "    station_names: dict,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prepares the final concatenated dataset for export by\n",
    "    - updating the attributes and\n",
    "    - converting one of the mulit-index levels to the correct datatype\n",
    "    then exports the dataset to AWS\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) retains the name of the newest station\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_concat: pd.DataFrame\n",
    "        dataframe of concatenated dataframes\n",
    "    final_concat_list: list[str]\n",
    "        list of stations that have been concatenated\n",
    "    network_name: str\n",
    "        weather station network\n",
    "    attrs_new: dict\n",
    "        attributes of newer dataframe that was input to concatenation\n",
    "    station_names: dict\n",
    "        library of station names, including the single new station name and a string of all the older station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if successful, exports dataset of concatenated dataframes to AWS\n",
    "    if failure, returns None\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### Rename input files\n",
    "    for station_name in final_concat_list:\n",
    "        new_name = \"{}_c\".format(station_name) \n",
    "        _rename_file(network_name, station_name, new_name)\n",
    "\n",
    "    ##### Prepare concatenated dataset for export\n",
    "\n",
    "    # Delete unnecessary columns and set index\n",
    "    df_concat = df_concat.drop([\"hour\", \"day\", \"month\", \"year\", \"date\"], axis=1)\n",
    "    df_to_export = df_concat.set_index([\"station\", \"time\"])\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_to_export.to_xarray()\n",
    "\n",
    "    # Convert datatype of station coordinate\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "    # Include past attributes\n",
    "    for i in attrs_new:\n",
    "        ds_concat.attrs[i] = attrs_new[i]\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nstation_matching.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Intermediary data product. This data has been subjected to cleaning, QA/QC, but may not have been standardized.\"\n",
    "    )\n",
    "\n",
    "    # Extract old and new station names from name dictionary\n",
    "    station_name_new = station_names[\"station_name_new\"]\n",
    "    station_name_old = station_names[\"old_stations\"]\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "            station_name_old,\n",
    "            station_name_new  # extract old and new station names from name dictionary\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ##### Export\n",
    "    # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "    #     network_name, station_name_new\n",
    "    # )\n",
    "    # print(\"Exporting....\", export_url)\n",
    "    # ds_concat.to_zarr(export_url, mode=\"w\") ## WHEN READY TO EXPORT\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:24:36.616573Z",
     "iopub.status.busy": "2025-05-01T21:24:36.615791Z",
     "iopub.status.idle": "2025-05-01T21:24:36.623785Z",
     "shell.execute_reply": "2025-05-01T21:24:36.622457Z",
     "shell.execute_reply.started": "2025-05-01T21:24:36.616527Z"
    }
   },
   "outputs": [],
   "source": [
    "def _rename_file(network: str, old_name: str, new_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Renames a given file in AWS by copying it over into the new name and then deleting the old file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network: str\n",
    "        weather station network name\n",
    "    old_name: str\n",
    "        name of input dataset\n",
    "    new_name: str\n",
    "        new name for input dataset (ie. \"_c\" added to the name)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success:\n",
    "        None\n",
    "    if failure:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        old_url = f\"s3://wecc-historical-wx/3_qaqc_wx/{network}/{old_name}.zarr\"\n",
    "        print('Original file name:',old_url)\n",
    "\n",
    "        # Copy original file, and re-name using input\n",
    "        # s3.copy_object(\n",
    "        #     Bucket=\"wecc-historical-wx\",\n",
    "        #     CopySource=old_url,\n",
    "        #     Key=new_name,\n",
    "        # )\n",
    "\n",
    "        # Delete older version of the file \n",
    "        # s3.delete_object(Bucket=\"wecc-historical-wx\", Key=old_name)\n",
    "        print(f\"File {old_name} renamed to {new_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:28:45.225673Z",
     "iopub.status.busy": "2025-05-01T21:28:45.224864Z",
     "iopub.status.idle": "2025-05-01T21:28:45.244855Z",
     "shell.execute_reply": "2025-05-01T21:28:45.244012Z",
     "shell.execute_reply.started": "2025-05-01T21:28:45.225629Z"
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_stations(network_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Coordinates the concatenation of input datasets and exports the final concatenated dataset.\n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: string\n",
    "        weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    if success: return a list of strings\n",
    "    if failure: None\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Uses the following helper functions\n",
    "        _df_concat(): concatenates two dataframes\n",
    "        _overlap_concat(): used by _df_concat() to concatenate two stations with overlapping time ranges\n",
    "        _more_than_2(): handles subsets with more than two stations, passing pairs to _df_concat() iteratively\n",
    "        _concat_export_help(): formats and exports concatenated dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    # Initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    # Read in full concat station list\n",
    "    print(network_name)\n",
    "    concat_list = pd.read_csv(\n",
    "        f\"s3://wecc-historical-wx/3_qaqc_wx/{network_name}/concat_list_{network_name}.csv\"\n",
    "    )\n",
    "\n",
    "    # Identify stns within designated network\n",
    "    concat_by_network = concat_list.loc[\n",
    "        concat_list.concat_subset.str.contains(network_name)\n",
    "    ]\n",
    "\n",
    "    ######### ! for testing\n",
    "    concat_by_network = concat_list[concat_list[\"concat_subset\"] == \"ASOSAWOS_2\"]\n",
    "    # concat_by_network = concat_by_network.head(4)\n",
    "    ######### ! for testing\n",
    "\n",
    "    unique_pair_names = concat_by_network.concat_subset.unique()\n",
    "    # For MARITIME, remove these stations becuase they're actually separate stations\n",
    "    if network_name == 'MARITIME':\n",
    "        unique_pair_names = unique_pair_names[1:]\n",
    "        unique_pair_name = unique_pair_name[~unique_pair_name[\"ERA-ID\"].isin['MARITIME_LJPC1','MARITIME_LJAC1']]\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    print(\n",
    "        f\"There are {len(concat_by_network)} stations to be concatenated into {len(unique_pair_names)} station pairs within {network_name}...\"\n",
    "    )\n",
    "    print(unique_pair_names)\n",
    "\n",
    "    # Set up pairs\n",
    "    for pair in unique_pair_names:\n",
    "        print(pair)\n",
    "        # Pull out stations corresponding to pair name\n",
    "        stns_to_pair = concat_by_network.loc[concat_by_network.concat_subset == pair]\n",
    "\n",
    "        if len(stns_to_pair) == 2:  # 2 stations to concat together\n",
    "            print(\"\\n\", stns_to_pair)\n",
    "\n",
    "            # Import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[0][\"ERA-ID\"]\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[1][\"ERA-ID\"]\n",
    "            )\n",
    "\n",
    "            print(\"Retrieving....\", url_1)\n",
    "            print(\"Retrieving....\", url_2)\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            # Convert to dataframes with corresponding information\n",
    "            df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(ds_1)\n",
    "            df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(ds_2)\n",
    "\n",
    "            # Send to helper function for concatenation\n",
    "            df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "                df_1, df_2, attrs_1, attrs_2\n",
    "            )\n",
    "\n",
    "            # Construct dictionary of old and new station names\n",
    "            station_names ={\"station_name_new\":stn_n_to_keep, \"old_stations\":stn_n_to_drop}\n",
    "\n",
    "        else:\n",
    "            # If there are more than 2 stations in the given subset, pass to _more_than_2()\n",
    "            print(\"More than 2 stations within a subset.\")\n",
    "            df_concat, station_names, attrs_new = _more_than_2(\n",
    "                network_name,\n",
    "                stns_to_pair\n",
    "            )\n",
    "        \n",
    "        print(f\"Length of new dataframe: {len(df_concat)}\")\n",
    "\n",
    "        # # Add concatenated station names to station name list\n",
    "        # final_concat_list.extend(stns_to_pair[\"ERA-ID\"].tolist())\n",
    "\n",
    "        # # Send concatenated dataframe to helper function for export\n",
    "        # _concat_export_help(  \n",
    "        #     df_concat,\n",
    "        #     final_concat_list,\n",
    "        #     network_name,\n",
    "        #     attrs_new,\n",
    "        #     station_names,\n",
    "        # )\n",
    "\n",
    "    # print(\"Concatenated stations: \", final_concat_list)\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Network Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T20:49:12.524841Z",
     "iopub.status.busy": "2025-05-01T20:49:12.523724Z",
     "iopub.status.idle": "2025-05-01T20:49:12.529340Z",
     "shell.execute_reply": "2025-05-01T20:49:12.528515Z",
     "shell.execute_reply.started": "2025-05-01T20:49:12.524791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target network for concatenation: options are \"ASOSAWOS\", \"MARITIME\"\n",
    "network_name = \"ASOSAWOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T21:33:44.759558Z",
     "iopub.status.busy": "2025-05-01T21:33:44.758861Z",
     "iopub.status.idle": "2025-05-01T21:34:02.046030Z",
     "shell.execute_reply": "2025-05-01T21:34:02.045639Z",
     "shell.execute_reply.started": "2025-05-01T21:33:44.759520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOSAWOS\n",
      "There are 2 stations to be concatenated into 1 station pairs within ASOSAWOS...\n",
      "['ASOSAWOS_2']\n",
      "ASOSAWOS_2\n",
      "\n",
      "                  ERA-ID concat_subset\n",
      "4  ASOSAWOS_72272093063    ASOSAWOS_2\n",
      "5  ASOSAWOS_72272193063    ASOSAWOS_2\n",
      "Retrieving.... s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/ASOSAWOS_72272093063.zarr\n",
      "Retrieving.... s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/ASOSAWOS_72272193063.zarr\n",
      "Station will be concatenated and saved as: ASOSAWOS_72272193063\n",
      "There is overlap\n",
      "Length of overlapping period: 89474\n",
      "Length of new dataframe: 547348\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>elevation</th>\n",
       "      <th>elevation_eraqc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pr</th>\n",
       "      <th>pr_depth_qc</th>\n",
       "      <th>pr_duration</th>\n",
       "      <th>pr_eraqc</th>\n",
       "      <th>...</th>\n",
       "      <th>tdps_qc</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "      <th>station</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>psl</th>\n",
       "      <th>psl_eraqc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01 00:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.633</td>\n",
       "      <td>-108.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-01 00:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.633</td>\n",
       "      <td>-108.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-01 00:50:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.633</td>\n",
       "      <td>-108.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-01 01:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.633</td>\n",
       "      <td>-108.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-01 01:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.633</td>\n",
       "      <td>-108.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185499</th>\n",
       "      <td>2010-07-31 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.467</td>\n",
       "      <td>-109.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>101210.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185503</th>\n",
       "      <td>2010-07-31 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.467</td>\n",
       "      <td>-109.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>101200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185506</th>\n",
       "      <td>2010-07-31 21:26:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.467</td>\n",
       "      <td>-109.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185508</th>\n",
       "      <td>2010-07-31 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.467</td>\n",
       "      <td>-109.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>101230.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185512</th>\n",
       "      <td>2010-07-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.467</td>\n",
       "      <td>-109.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0 days 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASOSAWOS_72272193063</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>101110.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547348 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time  anemometer_height_m  elevation  elevation_eraqc  \\\n",
       "0      2005-01-01 00:10:00                  NaN     1659.0              NaN   \n",
       "1      2005-01-01 00:30:00                  NaN     1659.0              NaN   \n",
       "2      2005-01-01 00:50:00                  NaN     1659.0              NaN   \n",
       "3      2005-01-01 01:10:00                  NaN     1659.0              NaN   \n",
       "4      2005-01-01 01:30:00                  NaN     1659.0              NaN   \n",
       "...                    ...                  ...        ...              ...   \n",
       "185499 2010-07-31 20:00:00                  NaN     1266.0              NaN   \n",
       "185503 2010-07-31 21:00:00                  NaN     1266.0              NaN   \n",
       "185506 2010-07-31 21:26:00                  NaN     1266.0              NaN   \n",
       "185508 2010-07-31 22:00:00                  NaN     1266.0              NaN   \n",
       "185512 2010-07-31 23:00:00                  NaN     1266.0              NaN   \n",
       "\n",
       "           lat     lon   pr  pr_depth_qc     pr_duration  pr_eraqc  ...  \\\n",
       "0       32.633 -108.15  NaN          NaN             NaT       NaN  ...   \n",
       "1       32.633 -108.15  NaN          NaN             NaT       NaN  ...   \n",
       "2       32.633 -108.15  NaN          NaN             NaT       NaN  ...   \n",
       "3       32.633 -108.15  NaN          NaN             NaT       NaN  ...   \n",
       "4       32.633 -108.15  NaN          NaN             NaT       NaN  ...   \n",
       "...        ...     ...  ...          ...             ...       ...  ...   \n",
       "185499  31.467 -109.60  NaN          NaN             NaT       NaN  ...   \n",
       "185503  31.467 -109.60  NaN          NaN             NaT       NaN  ...   \n",
       "185506  31.467 -109.60  NaN          NaN             NaT       NaN  ...   \n",
       "185508  31.467 -109.60  0.0          2.0 0 days 01:00:00       NaN  ...   \n",
       "185512  31.467 -109.60  0.0          2.0 0 days 01:00:00       NaN  ...   \n",
       "\n",
       "       tdps_qc  thermometer_height_m               station  hour day  month  \\\n",
       "0            1                   NaN  ASOSAWOS_72272193063     0   1      1   \n",
       "1            1                   NaN  ASOSAWOS_72272193063     0   1      1   \n",
       "2            1                   NaN  ASOSAWOS_72272193063     0   1      1   \n",
       "3            1                   NaN  ASOSAWOS_72272193063     1   1      1   \n",
       "4            1                   NaN  ASOSAWOS_72272193063     1   1      1   \n",
       "...        ...                   ...                   ...   ...  ..    ...   \n",
       "185499       1                   NaN  ASOSAWOS_72272193063    20  31      7   \n",
       "185503       1                   NaN  ASOSAWOS_72272193063    21  31      7   \n",
       "185506       1                   NaN  ASOSAWOS_72272193063    21  31      7   \n",
       "185508       1                   NaN  ASOSAWOS_72272193063    22  31      7   \n",
       "185512       1                   NaN  ASOSAWOS_72272193063    23  31      7   \n",
       "\n",
       "        year        date       psl  psl_eraqc  \n",
       "0       2005  2005-01-01       NaN        NaN  \n",
       "1       2005  2005-01-01       NaN        NaN  \n",
       "2       2005  2005-01-01       NaN        NaN  \n",
       "3       2005  2005-01-01       NaN        NaN  \n",
       "4       2005  2005-01-01       NaN        NaN  \n",
       "...      ...         ...       ...        ...  \n",
       "185499  2010  2010-07-31  101210.0        NaN  \n",
       "185503  2010  2010-07-31  101200.0        NaN  \n",
       "185506  2010  2010-07-31       NaN        NaN  \n",
       "185508  2010  2010-07-31  101230.0        NaN  \n",
       "185512  2010  2010-07-31  101110.0        NaN  \n",
       "\n",
       "[547348 rows x 41 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_concat_list = concatenate_stations(network_name)\n",
    "final_concat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
