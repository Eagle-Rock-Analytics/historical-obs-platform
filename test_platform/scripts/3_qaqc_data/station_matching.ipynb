{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "## New logger function\n",
    "from log_config import logger\n",
    "\n",
    "# Import qaqc stage calc functions\n",
    "try:\n",
    "    from QAQC_pipeline import *\n",
    "except:\n",
    "    print(\"Error importing QAQC_pipeline.py\")\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\n",
    "    \"VALLEYWATER\"\n",
    "]  # [\"ASOSAWOS\",\"VALLEYWATER\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        # the csv is stored in each station folder within 3_qaqc_wx\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"/concat_list_{}.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapply_concat_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_networks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mapply_concat_check\u001b[0;34m(station_names_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_concat_check\u001b[39m(station_names_list):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function applies the conatenation check to a list of target stations. \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    It then upload a csv containing the ERA IDs and concatenation subset ID for \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     final_list \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame([])\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m station_names_list:\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m##### Import station list of target station\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2_clean_wx/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/stationlist_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(station,station)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concatenate Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multiindex_concat_nooverlap(m_old, m_new, name):\n",
    "    \"\"\"\n",
    "    Formats MultiIndex, ensuring that there are no duplicate times in the time index.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Drop duplicate times\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        m_old: xr.Dataset\n",
    "            older weather station dataset\n",
    "        m_new: xr.Dataset   \n",
    "            newer weather station dataset\n",
    "        name: str\n",
    "            newer station name\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            return a dataframe with a re-formatted MultiIndex\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # combine time indices of two multiindexes\n",
    "    tidx = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.Series(m_old.get_level_values(\"time\").values),\n",
    "                pd.Series(m_new.get_level_values(\"time\").values),\n",
    "            ]\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # idenitify if there are duplicate times\n",
    "    tidx = tidx.rename(columns={0: \"time\"})\n",
    "    tidx = tidx.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "    # PULL the station name from m_new and set to the same length\n",
    "    stnidx = (\n",
    "        pd.Series(name, index=np.arange(len(tidx)), name=\"station\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # combine into new df\n",
    "    df_new = pd.concat([stnidx, tidx], axis=1)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_export_help(\n",
    "    df_concat, network_name, attrs_new, station_names\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the final concatenated dataset for export by \n",
    "    - updating the attributes and \n",
    "    - converting one of the mulit-index levels to the correct datatype\n",
    "    then exports the dataset to AWS\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) retains the name of the newest station\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_concat: pd.DataFrame\n",
    "            dataframe of concatenated dataframes\n",
    "        network_name: str\n",
    "            weather station network\n",
    "        attrs_new: list of str\n",
    "            attributes of newer dataframe that was input to concatenation\n",
    "        station_name_new: str\n",
    "            name of newer station\n",
    "        station_name_old: str\n",
    "            name of older station\n",
    "        station_names: list of str\n",
    "            library of station names, included the single new station name and a string of all the older station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            None\n",
    "            exports dataset of concatenated dataframes to AWS\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Delete unnecessary columns and set index\n",
    "    df_concat = df_concat.drop([\"hour\", \"day\", \"month\", \"year\", \"date\"], axis=1)\n",
    "    df_to_export = df_concat.set_index([\"station\", \"time\"])\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_to_export.to_xarray()\n",
    "\n",
    "    # Convert datatype of station coordinate\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "    # Include past attributes\n",
    "    for i in attrs_new:\n",
    "        ds_concat.attrs[i] = attrs_new[i]\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nstation_matching.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Intermediary data product. This data has been subjected to cleaning, QA/QC, but may not have been standardized.\"\n",
    "    )\n",
    "\n",
    "    # Extract old and new station names from name dictionary\n",
    "    station_name_new = station_names[\"station_name_new\"]\n",
    "    station_name_old = station_names[\"old_stations\"]\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "            station_name_old,\n",
    "            station_name_new  # extract old and new station names from name dictionary\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## Export\n",
    "    # ! a test name is used below\n",
    "    # ! the final name will be that of the newer dataframe\n",
    "    export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "        network_name, \"TEST_concat\", station_name_new\n",
    "    )\n",
    "    print(\"Exporting....\", export_url)\n",
    "    # ds_concat.to_zarr(export_url, mode=\"w\") ## WHEN READY TO EXPORT\n",
    "\n",
    "    # ! output final concatenated dataset for testing\n",
    "    return ds_concat \n",
    "\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _overlap_concat(df_new,df_old):\n",
    "    \"\"\"\n",
    "    Handles the cases in which there is overlap between the two input stations\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_new: pd.DataFrame\n",
    "            weather station network\n",
    "        df_old: pd.DataFrame\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            return final concatenated dataset\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = pd.concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_concat(df_1, df_2, attrs_1, attrs_2):\n",
    "    \"\"\"\n",
    "    Performs concatenation of input datasets, handling two cases\n",
    "        1.) temporal overlap between the datasets\n",
    "        2.) no temporal overlap\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_1: pd.DataFrame\n",
    "            station data\n",
    "        df_2: pd.DataFrame\n",
    "            dtation data\n",
    "        attrs_1: list of str\n",
    "            attributes of df_1\n",
    "        attrs_2:\n",
    "            attributes of df_2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "        returns\n",
    "            df_concat: concatenated dataframe\n",
    "            stn_n_to_keep: name of newer station\n",
    "            stn_n_to_drop: name of older station\n",
    "            attrs_new: attributes for newer station\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # determine which dataset is older\n",
    "    if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "        # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "        # we also grab the name of the newer station in this step, for use later\n",
    "        df_new = df_2\n",
    "        attrs_new = attrs_2\n",
    "        df_old = df_1\n",
    "\n",
    "    else:\n",
    "        df_new = df_1\n",
    "        attrs_new = attrs_1\n",
    "        df_old = df_2\n",
    "\n",
    "    stn_n_to_keep = df_new[\"station\"].unique()[0]\n",
    "    stn_n_to_drop = df_old[\"station\"].unique()[0]\n",
    "    print(f\"Station will be concatenated and saved as: {stn_n_to_keep}\")\n",
    "\n",
    "    # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # If there is no overlap between the two time series, just concatenate\n",
    "    if len(df_overlap) == 0:\n",
    "        print(\"No overlap!\")\n",
    "        df_concat = pd.merge(df_old, df_new, how=\"outer\")\n",
    "        df_concat[\"station\"] = stn_n_to_keep\n",
    "\n",
    "    # If overlap exists, split into subsets and concatenate\n",
    "    else:\n",
    "        print(\"There is overlap\")\n",
    "        df_concat = _overlap_concat(df_old, df_new)\n",
    "\n",
    "    return df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _more_than_2(network_name,stns_to_pair):\n",
    "    \"\"\"\n",
    "    Performs pairwise concatenation on subsets of more than two stations flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        stns_to_pair: pd.DataFrame\n",
    "            dataframe of the input station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            returns concatenated dataframe, dictionary of old and new station names, and attributes of newest station\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Concatenating the following stations:\", stns_to_pair)\n",
    "\n",
    "    # Load datasets into a list\n",
    "    datasets = [\n",
    "        xr.open_zarr(\n",
    "            \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stn\n",
    "            ),\n",
    "            consolidated=True,\n",
    "        )\n",
    "        for stn in stns_to_pair['ERA-ID']\n",
    "    ]\n",
    "\n",
    "    # Sort datasets by their max 'time'\n",
    "    datasets_sorted = sorted(datasets, key=lambda ds: ds['time'].max())\n",
    "\n",
    "    # Store station names, in order from oldest to newest\n",
    "    names = [ds.coords[\"station\"].values[0] for ds in datasets_sorted]\n",
    "\n",
    "    print('newest station:', names[-1])\n",
    "\n",
    "    # Setup for the while loop\n",
    "    ds_1 = datasets_sorted[0]\n",
    "    df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(\n",
    "        ds_1, verbose=False\n",
    "    )\n",
    "    i = 0\n",
    "    end = len(datasets_sorted) -1\n",
    "\n",
    "    while i < end:\n",
    "\n",
    "        print('iteration:', i)\n",
    "\n",
    "        ds_2 = datasets_sorted[i+1]\n",
    "        df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(\n",
    "            ds_2, verbose=False\n",
    "        )\n",
    "\n",
    "        # Send to helper function for concatenation\n",
    "        df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "            df_1, df_2, attrs_1, attrs_2\n",
    "        )\n",
    "\n",
    "        df_1 = df_concat\n",
    "        attrs_1 = attrs_new\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Construct station names list, for updating attributes\n",
    "    newest_station = names[-1] # Get last station name from station name list\n",
    "    older_stations = \", \".join(names[:-1]) # Create a string containing all older station names\n",
    "    station_names = {\"station_name_new\": newest_station, \"old_stations\": older_stations}\n",
    "\n",
    "    print('Progression concatenation for 2+ stations is complete.')\n",
    "\n",
    "    return df_concat, station_names, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs2(network_name):\n",
    "    \"\"\"\n",
    "    Coordinates the concatenation of input datasets and exports the final concatenated dataset.\n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "    Uses the following helper functions\n",
    "        _df_concat(): concatenates two dataframes\n",
    "        _overlap_concat(): used by _df_concat() to concatenate two stations with overlapping time ranges\n",
    "        _more_than_2(): handles subsets with more than two stations, passing pairs to _df_concat() iteratively\n",
    "        _concat_export_help(): formats and exports concatenated dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    # Initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    # Read in full concat station list\n",
    "    print(network_name)\n",
    "    concat_list = pd.read_csv(\n",
    "        f\"s3://wecc-historical-wx/3_qaqc_wx/{network_name}/concat_list_{network_name}.csv\"\n",
    "    )\n",
    "\n",
    "    # Identify stns within designated network\n",
    "    concat_by_network = concat_list.loc[\n",
    "        concat_list.concat_subset.str.contains(network_name)\n",
    "    ]\n",
    "\n",
    "    # ! for testing\n",
    "    concat_by_network = concat_list[concat_list[\"concat_subset\"] == \"ASOSAWOS_3\"]\n",
    "    # ! for testing\n",
    "\n",
    "    # For MARITIME, remove these stations becuase they're actually separate stations\n",
    "    if network_name == 'MARITIME':\n",
    "        unique_pair_names = unique_pair_names[1:]\n",
    "        unique_pair_name = unique_pair_name[~unique_pair_name[\"ERA-ID\"].isin['MARITIME_LJPC1','MARITIME_LJAC1']]\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    unique_pair_names = concat_by_network.concat_subset.unique()\n",
    "    print(\n",
    "        f\"There are {len(concat_by_network)} stations to be concatenated into {len(unique_pair_names)} station pairs within {network_name}...\"\n",
    "    )\n",
    "\n",
    "    print(unique_pair_names)\n",
    "\n",
    "    # Set up pairs\n",
    "    for pair in unique_pair_names:\n",
    "        print(pair)\n",
    "        # pull out stations corresponding to pair name\n",
    "        stns_to_pair = concat_by_network.loc[concat_by_network.concat_subset == pair]\n",
    "\n",
    "        if len(stns_to_pair) == 2:  # 2 stations to concat together\n",
    "            print(\"\\n\", stns_to_pair)\n",
    "\n",
    "            # import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[0][\"ERA-ID\"]\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[1][\"ERA-ID\"]\n",
    "            )\n",
    "\n",
    "            print(\"Retrieving....\", url_1)\n",
    "            print(\"Retrieving....\", url_2)\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            # convert to dataframes with corresponding information\n",
    "            df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(\n",
    "                ds_1, verbose=False\n",
    "            )\n",
    "            df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(\n",
    "                ds_2, verbose=False\n",
    "            )\n",
    "\n",
    "            ##### Send to helper function for concatenation\n",
    "            df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "                df_1, df_2, attrs_1, attrs_2\n",
    "            )\n",
    "\n",
    "            station_names ={\"station_name_new\":stn_n_to_keep, \"old_stations\":stn_n_to_drop}\n",
    "\n",
    "            ds_final = _concat_export_help(\n",
    "                df_concat,\n",
    "                network_name,\n",
    "                attrs_new,\n",
    "                station_names  # stn_n_to_keep, stn_n_to_drop\n",
    "            )\n",
    "\n",
    "            final_concat_list.extend(stns_to_pair[\"ERA-ID\"].tolist())\n",
    "\n",
    "            # return ds_final, final_concat_list\n",
    "\n",
    "        else:\n",
    "            # If there are more than 2 stations in the given subset, pass to _more_than_2()\n",
    "            print(\"More than 2 stations within a subset\")\n",
    "            df_concat, station_names, attrs_new = _more_than_2(\n",
    "                network_name,\n",
    "                stns_to_pair\n",
    "            )\n",
    "\n",
    "            if df_concat is None: # If the concentation failed\n",
    "                print('Concatenation of >2 stations was unsuccessful')\n",
    "            else: # If it was successful, move on to the next steps\n",
    "                # add station names to station name list\n",
    "                final_concat_list.extend(stns_to_pair[\"ERA-ID\"].tolist())\n",
    "\n",
    "                ds_final = _concat_export_help(\n",
    "                    df_concat,\n",
    "                    network_name,\n",
    "                    attrs_new,\n",
    "                    station_names  # stn_n_to_keep, stn_n_to_drop\n",
    "                )\n",
    "    print(\"Concatenated stations: \", final_concat_list)\n",
    "    return ds_final, final_concat_list\n",
    "    # return final_concat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"ASOSAWOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOSAWOS\n",
      "There are 3 stations to be concatenated into 1 station pairs within ASOSAWOS...\n",
      "['ASOSAWOS_3']\n",
      "ASOSAWOS_3\n",
      "More than 2 stations within a subset\n",
      "Concatenating the following stations:                  ERA-ID concat_subset\n",
      "6  ASOSAWOS_74003503145    ASOSAWOS_3\n",
      "7  ASOSAWOS_72280503145    ASOSAWOS_3\n",
      "8  ASOSAWOS_69960403145    ASOSAWOS_3\n",
      "newest station: ASOSAWOS_74003503145\n",
      "iteration: 0\n",
      "Station will be concatenated and saved as: ASOSAWOS_69960403145\n",
      "No overlap!\n",
      "iteration: 1\n",
      "Station will be concatenated and saved as: ASOSAWOS_74003503145\n",
      "There is overlap\n",
      "Progression concatenation for 2+ stations is complete.\n",
      "Exporting.... s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/TEST_concat_ASOSAWOS_74003503145.zarr\n",
      "Concatenated stations:  ['ASOSAWOS_74003503145', 'ASOSAWOS_72280503145', 'ASOSAWOS_69960403145']\n"
     ]
    }
   ],
   "source": [
    "ds_to_export, final_concat_list = concatenate_station_pairs2(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list = pd.read_csv(\n",
    "    f\"s3://wecc-historical-wx/3_qaqc_wx/ASOSAWOS/concat_list_ASOSAWOS.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stns_to_pair = concat_list[concat_list['concat_subset']=='ASOSAWOS_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERA-ID</th>\n",
       "      <th>concat_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASOSAWOS_74003503145</td>\n",
       "      <td>ASOSAWOS_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASOSAWOS_72280503145</td>\n",
       "      <td>ASOSAWOS_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASOSAWOS_69960403145</td>\n",
       "      <td>ASOSAWOS_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ERA-ID concat_subset\n",
       "6  ASOSAWOS_74003503145    ASOSAWOS_3\n",
       "7  ASOSAWOS_72280503145    ASOSAWOS_3\n",
       "8  ASOSAWOS_69960403145    ASOSAWOS_3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stns_to_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \n",
    "    xr.open_zarr(\n",
    "        \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "            network_name, stn\n",
    "        ),\n",
    "        consolidated=True,\n",
    "    )\n",
    "    for stn in stns_to_pair['ERA-ID']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sorted = sorted(datasets, key=lambda ds: ds['time'].max()) # oldest is first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' ()>\n",
      "array('1987-12-31T23:00:00.000000000', dtype='datetime64[ns]')\n",
      "<xarray.DataArray 'time' ()>\n",
      "array('2009-12-31T23:51:00.000000000', dtype='datetime64[ns]')\n",
      "<xarray.DataArray 'time' ()>\n",
      "array('2022-08-31T23:57:00.000000000', dtype='datetime64[ns]')\n"
     ]
    }
   ],
   "source": [
    "print(datasets_sorted[0].time.max())\n",
    "print(datasets_sorted[1].time.max())\n",
    "print(datasets_sorted[2].time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =[ ds.coords[\"station\"].values[0] for ds in datasets_sorted]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mark stations that have been concatenated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
