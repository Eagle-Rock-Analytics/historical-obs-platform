{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n",
    "\n",
    "The goal of this notebook is to identify stations that changed IDs. This has been known to occur for Maritime and ASOSOAWOS stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3_clean(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr_from_s3(station_id, temp_dir):\n",
    "    \"\"\"Read zarr file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".zarr\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_{}.zarr\".format(\n",
    "        station_id\n",
    "    )\n",
    "    print(s3_url)\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"zarr\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for the pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        input data from the clean step\n",
    "    verbose : bool, optional\n",
    "        if True, provides runtime output to the terminal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        converted xr.Dataset into dataframe\n",
    "    MultiIndex : pd.Index\n",
    "        multi-index of station and time\n",
    "    attrs : list of str\n",
    "        attributes from xr.Dataset\n",
    "    var_attrs : list of str\n",
    "        variable attributes from xr.Dataset\n",
    "    era_qc_vars : list of str\n",
    "        QAQC variables\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is the notebook friendly version (no logger statements).\n",
    "    \"\"\"\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    # var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"ASOSAWOS\"]  # , \"VALLEYWATER\", \"ASOSAWOS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        # the csv is stored in each station folder within 3_qaqc_wx\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"/concat_list_{}.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concatenate Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of variables to be assigned\n",
    "\n",
    "float64_variables = [\n",
    "    \"anemometer_height_m\",\n",
    "    \"elevation\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"pr_15min\",\n",
    "    \"thermometer_height_m\",\n",
    "    \"ps\",\n",
    "    \"tas\",\n",
    "    \"tdps\",\n",
    "    \"pr\",\n",
    "    \"sfcWind\",\n",
    "    \"sfcWind_dir\",\n",
    "    \"ps_altimeter\",\n",
    "    \"pr_duration\",\n",
    "    \"ps_eraqc\",\n",
    "    \"tas_eraqc\",\n",
    "    \"tdps_eraqc\",\n",
    "    \"pr_eraqc\",\n",
    "    \"sfcWind_eraqc\",\n",
    "    \"sfcWind_dir_eraqc\",\n",
    "    \"elevation_eraqc\",\n",
    "    \"ps_altimeter_eraqc\",\n",
    "    \"pr_15min_eraqc\",\n",
    "]\n",
    "U16_variables = [\n",
    "    #\"raw_qc\",\n",
    "    \"qaqc_process\",\n",
    "    \"ps_qc\",\n",
    "    \"ps_altimeter_qc\",\n",
    "    \"psl_qc\",\n",
    "    \"tas_qc\",\n",
    "    \"tdps_qc\",\n",
    "    \"pr_qc\",\n",
    "    \"pr_depth_qc\",\n",
    "    \"sfcWind_qc\",\n",
    "    \"sfcWind_method\",\n",
    "    \"sfcWind_dir_qc\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datatypes(ds):\n",
    "    \"\"\"\n",
    "    Converts the datatypes of variables in a dataset based on external libraries. \n",
    "    Used in the station concatenation function.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        ds: xr.Dataset\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output dataset with coverted datatypes\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "    Uses the following externally defined dictionaries to assign datatypes to variables:\n",
    "    float32_variables: List\n",
    "            list of variables that will be converted to datatpe \"float64\"\n",
    "    U16_variables: List\n",
    "            list of variables that will be converted to datatpe \"<U16\"\n",
    "    \"\"\"\n",
    "    # Generate lists of variables from the external dicionaries that are actually present in the input dataset\n",
    "    existing_float64 = [\n",
    "        key for key in float64_variables if key in list(ds.keys())\n",
    "    ]\n",
    "    existing_U16 = [key for key in U16_variables if key in list(ds.keys())]\n",
    "\n",
    "    # Convert the datatypes of those variables, but only if those variables exist\n",
    "    if len(existing_float64) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        ds[existing_float64] = ds[existing_float64].astype(\"float64\")\n",
    "    \n",
    "    if len(existing_U16) == 0:\n",
    "        pass\n",
    "    else: \n",
    "        ds[existing_U16] = ds[existing_U16].astype(\"<U16\")\n",
    "\n",
    "    # And of the coordinates as well\n",
    "    ds.coords[\"station\"] = ds.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            # retrieve ERA IDs in this subset of stations\n",
    "            station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "            station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "            final_concat_list.append(station_1)\n",
    "            final_concat_list.append(station_2)\n",
    "\n",
    "            # import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_1\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_2\n",
    "            )\n",
    "\n",
    "            # TODO: open_zarr will be used for QAQC'd datasets\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            df_1 = ds_1.to_dataframe()\n",
    "            df_2 = ds_2.to_dataframe()\n",
    "\n",
    "            # apply reset index only to 'time', as we will need that for concatenation\n",
    "            df_1 = df_1.reset_index(level=\"time\")\n",
    "            df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "            # determine which dataset is older\n",
    "            if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "                # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                # we also grab the name of the newer station in this step, for use later\n",
    "                df_new = df_2\n",
    "                ds_new = ds_2\n",
    "\n",
    "                df_old = df_1\n",
    "                ds_old = ds_1\n",
    "            else:\n",
    "                df_new = df_1\n",
    "                ds_new = ds_1\n",
    "\n",
    "                df_old = df_2\n",
    "                ds_old = ds_2\n",
    "\n",
    "            # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "            df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "            # if there is no overlap between the two time series, just concatenate\n",
    "            if len(df_overlap) == 0:\n",
    "                df_concat = concat([df_old, df_new])\n",
    "\n",
    "            # if not, split into subsets and concatenate\n",
    "            else: \n",
    "                ##### Split datframes into subsets #####\n",
    "\n",
    "                # Remove data in time overlap between old and new\n",
    "                df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                ##### Concatenate subsets #####\n",
    "                df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "            ##### Now prepare the final concatenated dataframe for export\n",
    "            station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "            final_station_name = \"{}\".format(station_name_new)\n",
    "            new_index = [final_station_name] * len(df_concat)\n",
    "            df_concat.index = new_index\n",
    "            df_concat.index.name = \"station\"\n",
    "\n",
    "            # drop duplicate rows that were potentially generated in the concatenation process\n",
    "            df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "            # Add 'time' back into multi index\n",
    "            df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "            # Convert concatenated dataframe to dataset\n",
    "            ds_concat = df_concat.to_xarray()\n",
    "\n",
    "            #### Update attributes and datatypes #####\n",
    "\n",
    "            # Include past attributes\n",
    "            ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "            # Update 'history' attribute\n",
    "            timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "            ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "                \"history\"\n",
    "            ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "            # Update 'comment' attribute\n",
    "            ds_concat.attrs[\"comment\"] = (\n",
    "                \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "            )\n",
    "\n",
    "            # Add new qaqc_files_merged attribute\n",
    "            station_name_old = ds_old.coords[\"station\"].values[0]\n",
    "            ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "                    network_name, station_name_old, network_name, station_name_new\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert all datatypes, to enable export\n",
    "            ds_concat = convert_datatypes(ds_concat)\n",
    "\n",
    "            # ## Export ###\n",
    "            # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "            #     network_name, \"test_concat\", network_name # station_name_new\n",
    "            # )\n",
    "            # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return final_concat_list  \n",
    "    # return df_concat, df_new, df_old, df_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check option 1\n",
    "\n",
    "run concatenate_station_pairs() with the export part commented out and the second return statement uncommented, so the function does not export and instead returns df_concat, df_new, df_old, and df_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_name = \"VALLEYWATER\"\n",
    "# df_concat, df_new, df_old, df_overlap = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check option 2\n",
    "\n",
    "Run concatenate_station_pairs() as is, so it does export and returns the list of stations that were concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"VALLEYWATER\"\n",
    "output = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_1)\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_2)\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1 = ds_1.to_dataframe()\n",
    "df_2 = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1.reset_index(level=\"time\")\n",
    "df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "\n",
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_2_overlap = df_2[df_2[\"time\"].isin(df_concat[\"time\"])]\n",
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_1[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_1[\"time\"], df_1[\"pr_15min\"])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_2[\"time\"], df_2[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mark stations that have been concatenated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
