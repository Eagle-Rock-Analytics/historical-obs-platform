{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "## New logger function\n",
    "from log_config import logger\n",
    "\n",
    "# Import qaqc stage calc functions\n",
    "try:\n",
    "    from QAQC_pipeline import *\n",
    "except:\n",
    "    print(\"Error importing QAQC_pipeline.py\")\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"ASOSAWOS\",\"VALLEYWATER\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        # the csv is stored in each station folder within 3_qaqc_wx\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"/concat_list_{}.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Concatenate Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}/concat_list_{}.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    # ! you can truncate the concat list here, for testing\n",
    "    concat_list = concat_list.head(4)\n",
    "    # ! end\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            try: \n",
    "                # retrieve ERA IDs in this subset of stations\n",
    "                station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "                station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "                final_concat_list.append(station_1)\n",
    "                final_concat_list.append(station_2)\n",
    "\n",
    "                # import this subset of datasets and convert to dataframe\n",
    "                url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_1\n",
    "                )\n",
    "                url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_2\n",
    "                )\n",
    "\n",
    "                ds_1 = xr.open_zarr(url_1)\n",
    "                ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "                df_1,MultiIndex_1,attrs_1,var_attrs_1,era_qc_vars_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "                df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = (qaqc_ds_to_df(ds_2, verbose=False))\n",
    "\n",
    "                # determine which dataset is older\n",
    "                if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "                    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                    # we also grab the name of the newer station in this step, for use later\n",
    "                    df_new = df_2\n",
    "                    ds_new = ds_2\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_1\n",
    "                    ds_old = ds_1\n",
    "                    MultiIndex_old = MultiIndex_1\n",
    "\n",
    "                else:\n",
    "                    df_new = df_1\n",
    "                    ds_new = df_1\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_2\n",
    "                    ds_old = ds_2\n",
    "                    MultiIndex_old = MultiIndex_2\n",
    "\n",
    "                # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "                df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "                # if there is no overlap between the two time series, just concatenate\n",
    "                if len(df_overlap) == 0:\n",
    "                    df_concat = concat([df_old, df_new])\n",
    "\n",
    "                # if not, split into subsets and concatenate\n",
    "                else:\n",
    "                    ##### Split datframes into subsets #####\n",
    "\n",
    "                    # Remove data in time overlap between old and new\n",
    "                    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                    ##### Concatenate subsets #####\n",
    "                    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "                # ##### Now prepare the final concatenated dataframe for export\n",
    "                station_name_new = MultiIndex_new.get_level_values(\"station\")[1]\n",
    "                MultiIndex_concat = MultiIndex_new.union(MultiIndex_old)\n",
    "                MultiIndex_concat = pd.MultiIndex.from_tuples(\n",
    "                    [(station_name_new, lvl1) for _, lvl1 in MultiIndex_concat],\n",
    "                    names=MultiIndex_concat.names,\n",
    "                )\n",
    "\n",
    "                # drop duplicate rows that were potentially generated in the concatenation process\n",
    "                df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "                # drop 'station' and 'time'columns\n",
    "                df_concat = df_concat.drop([\"station\", \"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"], axis=1)\n",
    "\n",
    "                df_concat.index = MultiIndex_concat \n",
    "\n",
    "                # Convert concatenated dataframe to dataset\n",
    "                ds_concat = df_concat.to_xarray()\n",
    "\n",
    "                # #### Prepare for export #####\n",
    "\n",
    "                # Convert datatype of station coordinate\n",
    "                ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "                # # Include past attributes\n",
    "                ds_concat.attrs.update(attrs_new)\n",
    "\n",
    "                # Update 'history' attribute\n",
    "                timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "                ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "                    \"history\"\n",
    "                ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "                # Update 'comment' attribute\n",
    "                ds_concat.attrs[\"comment\"] = (\n",
    "                    \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "                )\n",
    "\n",
    "                # Add new qaqc_files_merged attribute\n",
    "                station_name_old = MultiIndex_old.get_level_values(\"station\")[1]\n",
    "                ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                    \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "                        station_name_old, station_name_new\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # ## Export ### \n",
    "                # ! a test name is used below \n",
    "                # ! the final name will be that of the newer dataframe\n",
    "                # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                #     network_name, \"test_concat\", station_name_new\n",
    "                # )\n",
    "                # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"Error concatenation stations of subset {}: {}\".format(subset_i, e)\n",
    "                )\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # return final_concat_list # ! this will be the final return statement, below is inlcluded for testing\n",
    "    return (\n",
    "        df_new,\n",
    "        df_old,\n",
    "        df_concat,\n",
    "        ds_concat,\n",
    "        final_concat_list,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"MARITIME\" # \"VALLEYWATER\", \"MARITIME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 1\n",
    "\n",
    "Run concatenate_station_pairs() as is, so the function does not export and instead returns df_concat, df_new, df_old, and df_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error concatenation stations of subset                  ERA-ID concat_subset\n",
      "0  ASOSAWOS_99999903053    ASOSAWOS_0\n",
      "1  ASOSAWOS_A0001403053    ASOSAWOS_0: group not found at path ''\n",
      "Error concatenation stations of subset                  ERA-ID concat_subset\n",
      "2  ASOSAWOS_72269593041    ASOSAWOS_1\n",
      "3  ASOSAWOS_99999993041    ASOSAWOS_1: group not found at path ''\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df_new' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     df_new,\n\u001b[1;32m      3\u001b[0m     df_old,\n\u001b[1;32m      4\u001b[0m     df_concat,\n\u001b[1;32m      5\u001b[0m     ds_concat,\n\u001b[1;32m      6\u001b[0m     final_concat_list,\n\u001b[0;32m----> 7\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_station_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[120], line 178\u001b[0m, in \u001b[0;36mconcatenate_station_pairs\u001b[0;34m(network_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# return final_concat_list # ! this will be the final return statement, below is inlcluded for testing\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mdf_new\u001b[49m,\n\u001b[1;32m    179\u001b[0m     df_old,\n\u001b[1;32m    180\u001b[0m     df_concat,\n\u001b[1;32m    181\u001b[0m     ds_concat,\n\u001b[1;32m    182\u001b[0m     final_concat_list,\n\u001b[1;32m    183\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df_new' referenced before assignment"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_new,\n",
    "    df_old,\n",
    "    df_concat,\n",
    "    ds_concat,\n",
    "    final_concat_list,\n",
    ") = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 2: \n",
    "\n",
    "Run concatenate_station_pairs() with the first return statement uncommented and the second commented, and the export section uncommented. So that the function actually exports the concatenated datasets. I've generated all the concatention lists (for VALLEYWATER, MARITIME, and ASOSAWOS) needed to run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "# TODO: you'll need to change the url\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}/{}_concat_list_{}.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_1)\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_2)\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1 = ds_1.to_dataframe()\n",
    "df_2 = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1.reset_index(level=\"time\")\n",
    "df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "\n",
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    ds_new = ds_2\n",
    "\n",
    "    df_old = df_1\n",
    "    ds_old = ds_1\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = ds_1\n",
    "\n",
    "    df_old = df_2\n",
    "    ds_old = ds_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_new_overlap = df_new[df_new[\"time\"].isin(df_concat[\"time\"])]\n",
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_new[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_overlap.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_var = 'ps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_new[\"time\"], df_new[vis_var])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_old[\"time\"], df_old[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mark stations that have been concatenated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
