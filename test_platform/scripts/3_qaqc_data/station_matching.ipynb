{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n",
    "\n",
    "The goal of this notebook is to identify stations that changed IDs. This has been known to occur for Maritime and ASOSOAWOS stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = s3fs.S3FileSystem  # must be set to this to use such commands as ls\n",
    "# s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/VALLEYWATER/\"\n",
    "mergedir = \"4_merge_wx/VALLEYWATER/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3_clean(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr_from_s3(station_id, temp_dir):\n",
    "    \"\"\"Read zarr file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".zarr\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_{}.zarr\".format(\n",
    "        station_id\n",
    "    )\n",
    "    print(s3_url)\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"zarr\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for the pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        input data from the clean step\n",
    "    verbose : bool, optional\n",
    "        if True, provides runtime output to the terminal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        converted xr.Dataset into dataframe\n",
    "    MultiIndex : pd.Index\n",
    "        multi-index of station and time\n",
    "    attrs : list of str\n",
    "        attributes from xr.Dataset\n",
    "    var_attrs : list of str\n",
    "        variable attributes from xr.Dataset\n",
    "    era_qc_vars : list of str\n",
    "        QAQC variables\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is the notebook friendly version (no logger statements).\n",
    "    \"\"\"\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    # var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load station lists for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in ASOSAWOS stations\n",
    "\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "asosawos = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/ASOSAWOS/stationlist_ASOSAWOS_cleaned.csv\",\n",
    ")\n",
    "asosawos_list = pd.read_csv(BytesIO(asosawos[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valleywater = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/VALLEYWATER/stationlist_VALLEYWATER_cleaned.csv\",\n",
    ")\n",
    "valleywater_list = pd.read_csv(BytesIO(valleywater[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/MARITIME/stationlist_MARITIME_cleaned.csv\",\n",
    ")\n",
    "maritime_list = pd.read_csv(BytesIO(maritime[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify candidates for concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes.\n",
    "\n",
    "Some additional methods to use:\n",
    "1. matching IDs, for stations in which those exist (NOT currently used)\n",
    "2. stations within a certain distance of each other (I've investigated this some, but would take consideraly more time to fully develop and may not be necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stations = [\"ASOSAWOS\", \"VALLEYWATER\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged as needing to be concatenated if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    new_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    new_station_list = new_station_list[~new_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    new_station_list[\"concat_subset\"] = new_station_list[\"concat_subset\"].astype(\"int32\")\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    new_station_list = new_station_list[era_id_col+['concat_subset']]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        new_station_list.rename(columns={'era-id': 'ERA-ID'}, inplace=True)\n",
    "\n",
    "    return new_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) \n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to AWS\n",
    "        \n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'S3FileSystem' has no attribute 'Bucket'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mapply_concat_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_stations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 46\u001b[0m, in \u001b[0;36mapply_concat_check\u001b[0;34m(station_names_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m     final_list \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([final_list,concat_list])\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m##### Upload to AWS\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBucket\u001b[49m(bucket_name)\u001b[38;5;241m.\u001b[39mupload_file(\n\u001b[1;32m     47\u001b[0m         final_list,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3_qaqc_wx/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcatenation_station_list_TEST\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_list\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'S3FileSystem' has no attribute 'Bucket'"
     ]
    }
   ],
   "source": [
    "test = apply_concat_check(target_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stations already indentified for concatenation are flagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maritime station:\n",
    "\n",
    "- MTYC1 and MEYC1\n",
    "\n",
    "- SMOC1 and ICAC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERA-ID</th>\n",
       "      <th>STATION_ID</th>\n",
       "      <th>OWNER</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>in_terr_wecc</th>\n",
       "      <th>in_mar_wecc</th>\n",
       "      <th>NETWORK</th>\n",
       "      <th>...</th>\n",
       "      <th>hurs</th>\n",
       "      <th>hurs_nobs</th>\n",
       "      <th>sfcWind</th>\n",
       "      <th>sfcWind_nobs</th>\n",
       "      <th>sfcWind_dir</th>\n",
       "      <th>sfcWind_dir_nobs</th>\n",
       "      <th>rsds</th>\n",
       "      <th>rsds_nobs</th>\n",
       "      <th>total_nobs</th>\n",
       "      <th>concat_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MARITIME_LJAC1</td>\n",
       "      <td>ljac1</td>\n",
       "      <td>O</td>\n",
       "      <td>9410230 - La Jolla, CA</td>\n",
       "      <td>32.867 N 117.257 W (32&amp;#176;52'1\" N 117&amp;#176;1...</td>\n",
       "      <td>32.867</td>\n",
       "      <td>-117.257</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1098253</td>\n",
       "      <td>Y</td>\n",
       "      <td>1099588</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1409901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MARITIME_LJPC1</td>\n",
       "      <td>ljpc1</td>\n",
       "      <td>R</td>\n",
       "      <td>La Jolla, CA (073)</td>\n",
       "      <td>32.867 N 117.257 W (32&amp;#176;52'0\" N 117&amp;#176;1...</td>\n",
       "      <td>32.867</td>\n",
       "      <td>-117.257</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>105188</td>\n",
       "      <td>Y</td>\n",
       "      <td>104295</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>135209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MARITIME_ICAC1</td>\n",
       "      <td>icac1</td>\n",
       "      <td>O</td>\n",
       "      <td>9410840 - Santa Monica Pier</td>\n",
       "      <td>34.008 N 118.500 W (34&amp;#176;0'28\" N 118&amp;#176;2...</td>\n",
       "      <td>34.008</td>\n",
       "      <td>-118.500</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>997061</td>\n",
       "      <td>Y</td>\n",
       "      <td>996677</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1084742</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MARITIME_SMOC1</td>\n",
       "      <td>smoc1</td>\n",
       "      <td>O</td>\n",
       "      <td>9410840 - Santa Monica, CA</td>\n",
       "      <td>34.008 N 118.500 W (34&amp;#176;0'30\" N 118&amp;#176;3...</td>\n",
       "      <td>34.008</td>\n",
       "      <td>-118.500</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>52823</td>\n",
       "      <td>Y</td>\n",
       "      <td>53110</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>282368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MARITIME_MEYC1</td>\n",
       "      <td>meyc1</td>\n",
       "      <td>O</td>\n",
       "      <td>9413450 - Monterey, CA</td>\n",
       "      <td>36.605 N 121.889 W (36&amp;#176;36'18\" N 121&amp;#176;...</td>\n",
       "      <td>36.605</td>\n",
       "      <td>-121.889</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>415573</td>\n",
       "      <td>Y</td>\n",
       "      <td>415191</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>538763</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MARITIME_MTYC1</td>\n",
       "      <td>mtyc1</td>\n",
       "      <td>O</td>\n",
       "      <td>9413450 - Monterey, CA</td>\n",
       "      <td>36.605 N 121.889 W (36&amp;#176;36'18\" N 121&amp;#176;...</td>\n",
       "      <td>36.605</td>\n",
       "      <td>-121.889</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>237168</td>\n",
       "      <td>Y</td>\n",
       "      <td>236901</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>874261</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MARITIME_MYXC1</td>\n",
       "      <td>myxc1</td>\n",
       "      <td>CN</td>\n",
       "      <td>Monterrey, CA</td>\n",
       "      <td>36.605 N 121.889 W (36&amp;#176;36'18\" N 121&amp;#176;...</td>\n",
       "      <td>36.605</td>\n",
       "      <td>-121.889</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MARITIME</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ERA-ID STATION_ID OWNER                         NAME  \\\n",
       "18  MARITIME_LJAC1      ljac1     O       9410230 - La Jolla, CA   \n",
       "19  MARITIME_LJPC1      ljpc1     R           La Jolla, CA (073)   \n",
       "17  MARITIME_ICAC1      icac1     O  9410840 - Santa Monica Pier   \n",
       "42  MARITIME_SMOC1      smoc1     O   9410840 - Santa Monica, CA   \n",
       "23  MARITIME_MEYC1      meyc1     O       9413450 - Monterey, CA   \n",
       "24  MARITIME_MTYC1      mtyc1     O       9413450 - Monterey, CA   \n",
       "25  MARITIME_MYXC1      myxc1    CN                Monterrey, CA   \n",
       "\n",
       "                                             LOCATION  LATITUDE  LONGITUDE  \\\n",
       "18  32.867 N 117.257 W (32&#176;52'1\" N 117&#176;1...    32.867   -117.257   \n",
       "19  32.867 N 117.257 W (32&#176;52'0\" N 117&#176;1...    32.867   -117.257   \n",
       "17  34.008 N 118.500 W (34&#176;0'28\" N 118&#176;2...    34.008   -118.500   \n",
       "42  34.008 N 118.500 W (34&#176;0'30\" N 118&#176;3...    34.008   -118.500   \n",
       "23  36.605 N 121.889 W (36&#176;36'18\" N 121&#176;...    36.605   -121.889   \n",
       "24  36.605 N 121.889 W (36&#176;36'18\" N 121&#176;...    36.605   -121.889   \n",
       "25  36.605 N 121.889 W (36&#176;36'18\" N 121&#176;...    36.605   -121.889   \n",
       "\n",
       "   in_terr_wecc in_mar_wecc   NETWORK  ... hurs hurs_nobs sfcWind  \\\n",
       "18            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "19            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "17            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "42            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "23            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "24            Y         NaN  MARITIME  ...    N         0       Y   \n",
       "25            Y         NaN  MARITIME  ...    N         0       N   \n",
       "\n",
       "   sfcWind_nobs sfcWind_dir  sfcWind_dir_nobs rsds  rsds_nobs total_nobs  \\\n",
       "18      1098253           Y           1099588    N          0    1409901   \n",
       "19       105188           Y            104295    N          0     135209   \n",
       "17       997061           Y            996677    N          0    1084742   \n",
       "42        52823           Y             53110    N          0     282368   \n",
       "23       415573           Y            415191    N          0     538763   \n",
       "24       237168           Y            236901    N          0     874261   \n",
       "25            0           N                 0    N          0          0   \n",
       "\n",
       "    concat_flag  \n",
       "18          0.0  \n",
       "19          0.0  \n",
       "17          1.0  \n",
       "42          1.0  \n",
       "23          2.0  \n",
       "24          2.0  \n",
       "25          2.0  \n",
       "\n",
       "[7 rows x 50 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maritime_out\n",
    "\n",
    "# Flagged Stations:\n",
    "# MARITIME_LJAC1 <=> MARITIME_LJPC1\n",
    "# MARITIME_ICAC1 <=> MARITIME_SMOC1\n",
    "# MARITIME_MEYC1 <=> MARITIME_MTYC1 <=> MARITIME_MYXC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously identified stations are indeed flagged. Along with an additional pair: MARITIME_LJAC1 and MARITIME_LJPC1. And a third station included with MARITIME_MEYC1 abd MARITIME_MTYC1: MARITIME_MYXC1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using ICAO values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "ERA-ID              6\n",
      "USAF                6\n",
      "WBAN                6\n",
      "STATION NAME        6\n",
      "CTRY                6\n",
      "                   ..\n",
      "sfcWind_dir         6\n",
      "sfcWind_dir_nobs    6\n",
      "rsds                6\n",
      "rsds_nobs           6\n",
      "total_nobs          6\n",
      "Length: 67, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "repeat_list = asosawos_list[asosawos_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "\n",
    "# how many unique ICAO duplicates are there?\n",
    "print(len(repeat_list[\"ICAO\"].unique()))\n",
    "\n",
    "print(repeat_list.groupby(\"ICAO\").count().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(repeat_list[\"ICAO\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate problem station KMLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf = repeat_list[repeat_list[\"ICAO\"] == \"KMLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION NAME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>MILFORD MUNICIPAL AP</td>\n",
       "      <td>38.417</td>\n",
       "      <td>-113.017</td>\n",
       "      <td>1948-07-23</td>\n",
       "      <td>1996-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>MILFORD MUNICIPAL AP</td>\n",
       "      <td>38.417</td>\n",
       "      <td>-113.017</td>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>1983-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>MILFORD MUNICIPAL AP</td>\n",
       "      <td>38.417</td>\n",
       "      <td>-113.017</td>\n",
       "      <td>1983-05-01</td>\n",
       "      <td>1985-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>MILFORD MUNICIPAL AP</td>\n",
       "      <td>38.417</td>\n",
       "      <td>-113.017</td>\n",
       "      <td>1985-06-01</td>\n",
       "      <td>1989-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>MILFORD MUNI BRISCOE</td>\n",
       "      <td>38.417</td>\n",
       "      <td>-113.017</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>MILFORD MUNICIPAL AIRPORT</td>\n",
       "      <td>38.423</td>\n",
       "      <td>-113.011</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  STATION NAME     LAT      LON  start_time    end_time\n",
       "61        MILFORD MUNICIPAL AP  38.417 -113.017  1948-07-23  1996-12-31\n",
       "154       MILFORD MUNICIPAL AP  38.417 -113.017  1977-01-01  1983-04-30\n",
       "166       MILFORD MUNICIPAL AP  38.417 -113.017  1983-05-01  1985-05-31\n",
       "170       MILFORD MUNICIPAL AP  38.417 -113.017  1985-06-01  1989-05-01\n",
       "185       MILFORD MUNI BRISCOE  38.417 -113.017  1997-01-01  2022-12-31\n",
       "226  MILFORD MUNICIPAL AIRPORT  38.423 -113.011  2005-01-01  2022-12-31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmlf[[\"STATION NAME\", \"LAT\", \"LON\", \"start_time\", \"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using station locations (lat, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataframe\n",
    "\n",
    "test = asosawos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols = [col for col in test.columns if col in lat_lon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAT', 'LON']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = asosawos_list.duplicated(subset=lat_lon_cols, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = test[test[\"concat_flag\"] == True].groupby(lat_lon_cols).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var_list = [\"end_time\", \"end-date\"]\n",
    "end_time_col = [col for col in test.columns if col in time_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end_time']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(\"concat_flag\")\n",
    "test = (\n",
    "    test.groupby([\"concat_flag\"])\n",
    "    .apply(lambda x: x.sort_values(end_time_col))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing ICAO identification and lat lon identification for ASOSAWOS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Carry Out Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the order of operations:\n",
    "\n",
    "1. Read in target stations, for each concat_flag\n",
    "2. Check if there is overlap in time ranges\n",
    "    1. IF so:  \n",
    "\n",
    "        split overall time range\n",
    "\n",
    "        construct dataset by grabbing newest station for each time range subset\n",
    "\n",
    "    \n",
    "    2. ELSE:\n",
    "\n",
    "        concatenate, with NAs in the gap\n",
    "\n",
    "\n",
    "Another option: pairwise concetenation\n",
    "\n",
    "For each subset of matching stations, first concatenate the two newest stations. Then, the next oldest, etc.\n",
    "\n",
    "\n",
    "Issues to address:\n",
    "\n",
    "1. when the time range of one station completely includes that of another in a subset (this occures a few times with ASOSAWOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "asosawos_test = asosawos_out[asosawos_out[\"concat_flag\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>concat_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1977-01-27</td>\n",
       "      <td>1987-06-30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1987-07-01</td>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_time    end_time  concat_flag\n",
       "222  2005-01-01  2022-12-31            3\n",
       "155  1977-01-27  1987-06-30            3\n",
       "171  1987-07-01  1997-12-31            3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asosawos_test[[\"start_time\", \"end_time\", \"concat_flag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct df with start and end time subsets, and ERA-ID of newest station\n",
    "dr1 = pd.date_range(start=\"1977-01-27\", end=\"1987-06-30\", freq=\"H\")\n",
    "dr2 = pd.date_range(start=\"1987-07-01\", end=\"1997-12-31\", freq=\"H\")\n",
    "dr3 = pd.date_range(start=\"2005-01-01\", end=\"2022-12-31\", freq=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = asosawos_test[\"start_time\"]\n",
    "ends = asosawos_test[\"end_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range = list(\n",
    "    range(asosawos_out[\"concat_flag\"].min(), asosawos_out[\"concat_flag\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_i = asosawos_out.iloc[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9801/473907313.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ERA-ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hist-obs/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5903\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5905\u001b[0m         ):\n\u001b[1;32m   5906\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5907\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "network_name = row_i[\"ERA-ID\"].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = row_i[\"ERA-ID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379    ASOSAWOS_A0001403053\n",
       "Name: ERA-ID, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_test(concat_list):\n",
    "    \"\"\"\n",
    "    Performs concatenation for stations in list of stations flagged for concatenation.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Import target datasets and convert to dataframe\n",
    "    flag_range = list(\n",
    "        range(concat_list[\"concat_flag\"].min(), concat_list[\"concat_flag\"].max())\n",
    "    )\n",
    "\n",
    "    for i in flag_range:\n",
    "        subset_list = concat_list[concat_list[\"concat_flag\"] == i]\n",
    "        subset_range = list(range(0, len(subset_list)))\n",
    "\n",
    "        url = {}\n",
    "        ds = {}\n",
    "        df = {}\n",
    "\n",
    "        for i in subset_range:\n",
    "\n",
    "            # extract information needed for dataset import\n",
    "            row_i = subset_list.iloc[[i]]\n",
    "            network_name = row_i[\"ERA-ID\"].split(\"_\")[\n",
    "                0\n",
    "            ]  # TODO: this does not work, when it really should\n",
    "            station_name = row_i[\"ERA-ID\"]\n",
    "\n",
    "            url[i] = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                network_name, network_name, station_name\n",
    "            )\n",
    "\n",
    "            ds[i] = xr.open_zarr(url[i])\n",
    "\n",
    "            df[i] = ds[i].to_dataframe()\n",
    "\n",
    "            # Apply reset index only to 'time', as we will need that for concatenation\n",
    "            df[i] = df[i].reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE SCRAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = asosawos_list_concat.groupby([\"ICAO\"]).apply(\n",
    "    lambda x: x.sort_values([\"end_time\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by end_time or end-date, depending on the station TODO: this is not necessary\n",
    "# time_var_list = ['end_time','end-date']\n",
    "# end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "# new_station_list = new_station_list.groupby('concat_flag').apply(lambda x: x.sort_values(end_time_or_date)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(df):\n",
    "    \"\"\"\n",
    "    Concatenates station data that has been flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.dataframe\n",
    "            staton data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "    # if there is overlap, then create subsets\n",
    "\n",
    "    # if no overlap, just concatenate\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_station_list(station_list, concat_list, duplicate_list):\n",
    "    \"\"\"\n",
    "    Reorders the input station list, necessary for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.dataframe\n",
    "\n",
    "        concat_list: pd.dataframe\n",
    "\n",
    "        duplicate_list: pd.dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output station list with stations to be concatenated at top, followed by potential duplicates\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    ##### subsets of station list\n",
    "\n",
    "    # stations that will be concatenated\n",
    "    concat_stations = station_list[station_list[\"ICAO\"].isin(concat_list)]\n",
    "\n",
    "    # potential duplicate stations\n",
    "    duplicate_stations = station_list[station_list[\"ICAO\"].isin(duplicate_list)]\n",
    "\n",
    "    # all remaining stations\n",
    "    remaining_stations = station_list[\n",
    "        ~station_list[\"ICAO\"].isin(duplicate_list + concat_list)\n",
    "    ]\n",
    "\n",
    "    ##### sort concat list alphabetically, to ensure that stations with the same ICAO are grouped together\n",
    "    concat_stations = concat_stations.sort_values(\"ICAO\")\n",
    "    duplicate_stations = duplicate_stations.sort_values(\"ICAO\")\n",
    "\n",
    "    ##### now within each ICAO, order by end time\n",
    "    concat_stations = concat_stations.groupby([\"ICAO\"]).apply(\n",
    "        lambda x: x.sort_values([\"end_time\"])\n",
    "    )\n",
    "\n",
    "    ##### concatenate susbets and reset index\n",
    "    new_list = concat(\n",
    "        [concat_stations, duplicate_stations, remaining_stations]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of null start times:\n",
      "0\n",
      "number of null end times:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for presence of start and end times\n",
    "\n",
    "time_check = repeat_list_subset.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "print(\"number of null start times:\")\n",
    "print(time_check[\"start_time\"].sum())\n",
    "\n",
    "print(\"number of null end times:\")\n",
    "print(time_check[\"end_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the start and end times are identical\n",
    "\n",
    "start_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")\n",
    "end_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KBOK', 'KMLF']\n",
      "['K20V']\n"
     ]
    }
   ],
   "source": [
    "end_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\"ICAO\"].tolist()\n",
    "start_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "    \"ICAO\"\n",
    "].tolist()\n",
    "\n",
    "print(end_list)\n",
    "print(start_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is going on with the stations that have duplicate start and end times? are they true duplicates?\n",
    "\n",
    "repeat_list_subset[repeat_list_subset[\"ICAO\"].isin(start_list + end_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in single dc file from AWS\n",
    "ds_1 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_72026294076\", temp_dir)\n",
    "ds_2 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_A0000594076\", temp_dir)\n",
    "\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "df_2 = qaqc_ds_to_df(ds_2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = df_1.lon.mean()\n",
    "lat = df_1.lat.mean()\n",
    "# print(\"{}, {:.5f}, {:.5f}\".format(id, lon, lat))\n",
    "\n",
    "\n",
    "# Plot time series of the data\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "df_1.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "df_2.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "\n",
    "ax.set_title(\"{}  ({:.3f}, {:.3f})\".format(id, lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_check_old(station_list):\n",
    "    \"\"\"\n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            list\n",
    "                list of ICAO values of stations that need to be concatenated\n",
    "            list\n",
    "                list of ICAO values of potential duplicate stations\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Generate list of repeat ICAOs\n",
    "    repeat_list = station_list[station_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "    repeat_list = repeat_list[\n",
    "        [\"ICAO\", \"ERA-ID\", \"STATION NAME\", \"start_time\", \"end_time\"]\n",
    "    ]\n",
    "\n",
    "    concat_list = repeat_list[\"ICAO\"].unique().tolist()\n",
    "\n",
    "    # And empty list to add potential duplicates to\n",
    "    duplicate_list = []\n",
    "\n",
    "    ##### Generate boolean for whether or not there are null start and/or end times\n",
    "    # TODO: may not be necessary\n",
    "    time_check = repeat_list.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "    end_nan_list = time_check[time_check[\"end_time\"] == True][\"ICAO\"].tolist()\n",
    "    start_nan_list = time_check[time_check[\"start_time\"] == True][\"ICAO\"].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_nan_list + end_nan_list\n",
    "\n",
    "    duplicate_list = duplicate_list\n",
    "\n",
    "    ##### Identify ICAOs with duplicate start end times\n",
    "    start_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    end_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    end_dup_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "    start_dup_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_dup_list + end_dup_list\n",
    "\n",
    "    # Generate final list of ICAOs for stations to be concatenated\n",
    "    concat_list = [x for x in concat_list if x not in duplicate_list]\n",
    "\n",
    "    return concat_list, duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1101/3479120207.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  asosawos_list_concat['ICAO'] = pd.Categorical(asosawos_list_concat['ICAO'], categories=concat_list, ordered=True)\n"
     ]
    }
   ],
   "source": [
    "# order the subset with only stations to concatenate\n",
    "\n",
    "asosawos_list_concat[\"ICAO\"] = pd.Categorical(\n",
    "    asosawos_list_concat[\"ICAO\"], categories=concat_list, ordered=True\n",
    ")\n",
    "\n",
    "test_list = asosawos_list_concat.sort_values(\"ICAO\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations within a certain distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into GeoDataFrames\n",
    "# using EPSG 3310\n",
    "\n",
    "gdf_asosawos = gpd.GeoDataFrame(\n",
    "    asosawos_list,\n",
    "    geometry=[\n",
    "        Point(lon, lat) for lon, lat in zip(asosawos_list[\"LON\"], asosawos_list[\"LAT\"])\n",
    "    ],\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 3: find the nearest point in the geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert emtpy columns\n",
    "\n",
    "gdf_asosawos[\"nearest_station\"] = pd.Series(dtype=\"U16\")\n",
    "gdf_asosawos[\"distance\"] = pd.Series(dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_asosawos.iterrows():\n",
    "    # geometry of individual row \n",
    "    point = row.geometry\n",
    "    # returns a multipoint object with the geometries of every row in the gdf\n",
    "    multipoint = gdf_asosawos.drop(index, axis=0).geometry.unary_union\n",
    "    # \n",
    "    queried_geom, nearest_geom = nearest_points(point, multipoint)\n",
    "    dist_from_point = \n",
    "    gdf_asosawos.loc[index, 'nearest_geometry'] = nearest_geom\n",
    "    gdf_asosawos.loc[index, 'distance'] = nearest_geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 2: distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the distance between points\n",
    "\n",
    "\n",
    "def distance_sort_filter(row, df2, buffer=None, id=False):\n",
    "\n",
    "    dist = df2.geometry.distance(row).sort_values()\n",
    "\n",
    "    if buffer:\n",
    "        dist = dist[dist < buffer]\n",
    "\n",
    "    if id:\n",
    "        distances = {\n",
    "            df2.loc[idx][\"WBAN\"]: value for idx, value in zip(dist.index, dist.values)\n",
    "        }\n",
    "    else:\n",
    "        distances = {idx: value for idx, value in zip(dist.index, dist.values)}\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 1: using sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a buffer around points in gdf1 (e.g., 10 km buffer)\n",
    "gdf_asosawos[\"buffer\"] = gdf_asosawos.geometry.buffer(\n",
    "    0.1\n",
    ")  # Buffer in degrees, 0.1 degrees approx equals 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join using the buffer\n",
    "merged = gpd.sjoin(\n",
    "    gdf_asosawos, gdf_asosawos[[\"geometry\", \"buffer\"]], how=\"inner\", predicate=\"within\"\n",
    ")\n",
    "\n",
    "# The 'merged' GeoDataFrame contains points from gdf_isd that are within the buffer around points in gdf_asosawos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    merged\n",
    ")  # there are not ISD stations within 10km of an ASOSAWOS station missed by the exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Round asosawos down to 3 decimal points of accuracy\n",
    "# asosawos_round = asosawos_list.round({\"LAT\": 3, \"LON\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential ways to check that two stations are duplicates\n",
    "1. identical total_nobs\n",
    "2. identical ERA IDs\n",
    "3. identical end or start times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract flagged stations\n",
    "\n",
    "asosawos_dup = asosawos_out[~asosawos_out[\"concat_flag\"].isna()]\n",
    "valleywater_dup = valleywater_out[~valleywater_out[\"concat_flag\"].isna()]\n",
    "maritime_dup = maritime_out[~maritime_out[\"concat_flag\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that are potentially duplicates\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Within stations flagged for concatenation, stations are flagged as potential duplicates\n",
    "            if either their start or end times are identical\n",
    "            - TODO: brainstorm alternative approaches\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information that has passed through the concatenation check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    ##### flag stations with repeat end or start times\n",
    "\n",
    "    time_end_list = [\"end_time\", \"end-date\"]\n",
    "    time_start_list = [\"start_time\", \"start-date\"]\n",
    "\n",
    "    end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "\n",
    "    new_station_list = (\n",
    "        new_station_list.groupby(\"concat_flag\")\n",
    "        .apply(lambda x: x.sort_values(end_time_or_date))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return new_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(network_name, station_old, station_new):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Import target datasets and convert to dataframe\n",
    "    old_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "    new_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "\n",
    "    ds_old = xr.open_zarr(old_url)\n",
    "    ds_new = xr.open_zarr(new_url)\n",
    "\n",
    "    df_old = ds_old.to_dataframe()\n",
    "    df_new = ds_new.to_dataframe()\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
