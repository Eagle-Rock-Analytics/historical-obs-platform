{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n",
    "\n",
    "The goal of this notebook is to identify stations that changed IDs. This has been known to occur for Maritime and ASOSOAWOS stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "import datetime\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# Import qaqc stage calc functions\n",
    "from QAQC_pipeline import *\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load station lists for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in ASOSAWOS stations\n",
    "\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "asosawos = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/ASOSAWOS/stationlist_ASOSAWOS_cleaned.csv\",\n",
    ")\n",
    "asosawos_list = pd.read_csv(BytesIO(asosawos[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valleywater = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/VALLEYWATER/stationlist_VALLEYWATER_cleaned.csv\",\n",
    ")\n",
    "valleywater_list = pd.read_csv(BytesIO(valleywater[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/MARITIME/stationlist_MARITIME_cleaned.csv\",\n",
    ")\n",
    "maritime_list = pd.read_csv(BytesIO(maritime[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes.\n",
    "\n",
    "Some additional methods to use:\n",
    "1. matching IDs, for stations in which those exist (NOT currently used)\n",
    "2. stations within a certain distance of each other (I've investigated this some, but would take consideraly more time to fully develop and may not be necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"VALLEYWATER\"]  # , \"ASOSAWOS\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"_copy\" \"/\"+ station + \"/\" + station + \"_concat_list_TEST.csv\"\n",
    "            #Key = qaqcdir + station + \"/{}_concat_list_TEST.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stations already indentified for concatenation are flagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maritime station:\n",
    "\n",
    "- MTYC1 and MEYC1\n",
    "\n",
    "- SMOC1 and ICAC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime_out\n",
    "\n",
    "# Flagged Stations:\n",
    "# MARITIME_LJAC1 <=> MARITIME_LJPC1\n",
    "# MARITIME_ICAC1 <=> MARITIME_SMOC1\n",
    "# MARITIME_MEYC1 <=> MARITIME_MTYC1 <=> MARITIME_MYXC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously identified stations are indeed flagged. Along with an additional pair: MARITIME_LJAC1 and MARITIME_LJPC1. And a third station included with MARITIME_MEYC1 abd MARITIME_MTYC1: MARITIME_MYXC1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using ICAO values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_list = asosawos_list[asosawos_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "\n",
    "# how many unique ICAO duplicates are there?\n",
    "print(len(repeat_list[\"ICAO\"].unique()))\n",
    "\n",
    "print(repeat_list.groupby(\"ICAO\").count().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(repeat_list[\"ICAO\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate problem station KMLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf = repeat_list[repeat_list[\"ICAO\"] == \"KMLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf[[\"STATION NAME\", \"LAT\", \"LON\", \"start_time\", \"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using station locations (lat, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataframe\n",
    "\n",
    "test = asosawos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols = [col for col in test.columns if col in lat_lon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = asosawos_list.duplicated(subset=lat_lon_cols, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = test[test[\"concat_flag\"] == True].groupby(lat_lon_cols).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var_list = [\"end_time\", \"end-date\"]\n",
    "end_time_col = [col for col in test.columns if col in time_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(\"concat_flag\")\n",
    "test = (\n",
    "    test.groupby([\"concat_flag\"])\n",
    "    .apply(lambda x: x.sort_values(end_time_col))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing ICAO identification and lat lon identification for ASOSAWOS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Carry Out Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multiindex_concat_nooverlap(m_old: xr.Dataset, m_new: xr.Dataset, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Formats MultiIndex, ensuring that there are no duplicate times in the time index.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "    1.) Drop duplicate times\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "    m_old: xr.Dataset\n",
    "        older weather station dataset\n",
    "    m_new: xr.Dataset   \n",
    "        newer weather station dataset\n",
    "    name: str\n",
    "        newer station name\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    if success:\n",
    "        return a pd.DataFrame with a re-formatted MultiIndex\n",
    "    if failure:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # combine time indices of two multiindexes\n",
    "    tidx = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                pd.Series(m_old.get_level_values(\"time\").values),\n",
    "                pd.Series(m_new.get_level_values(\"time\").values),\n",
    "            ]\n",
    "        )\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # idenitify if there are duplicate times\n",
    "    tidx = tidx.rename(columns={0: \"time\"})\n",
    "    tidx = tidx.sort_values(\"time\").drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "    # PULL the station name from m_new and set to the same length\n",
    "    stnidx = (\n",
    "        pd.Series(name, index=np.arange(len(tidx)), name=\"station\")\n",
    "        .reset_index()\n",
    "        .drop(columns=\"index\")\n",
    "    )\n",
    "\n",
    "    # combine into new df\n",
    "    df_new = pd.concat([stnidx, tidx], axis=1)\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_file(network: str, old_name: str, new_name: str) -> None:\n",
    "    try:\n",
    "        s3.copy_object(\n",
    "            Bucket=bucket, CopySource=f\"s3://{bucket}/3_qaqc_wx/{old_name}\", Key=new_name\n",
    "        )\n",
    "        s3.delete_object(Bucket=bucket, Key=old_name)\n",
    "        print(f\"File {old_name} renamed to {new_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_export_help(\n",
    "    df_concat: pd.DataFrame,\n",
    "    final_concat_list: pd.DataFrame,\n",
    "    network_name: str,\n",
    "    attrs_new: dict,\n",
    "    station_names: dict,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prepares the final concatenated dataset for export by \n",
    "    - updating the attributes and \n",
    "    - converting one of the mulit-index levels to the correct datatype\n",
    "    then exports the dataset to AWS\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) retains the name of the newest station\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_concat: pd.DataFrame\n",
    "            dataframe of concatenated dataframes\n",
    "        final_concat_list: pd.DataFrame\n",
    "\n",
    "        network_name: str\n",
    "            weather station network\n",
    "        attrs_new: dict\n",
    "            attributes of newer dataframe that was input to concatenation\n",
    "        station_names: dict\n",
    "            library of station names, including the single new station name and a string of all the older station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            None\n",
    "            exports dataset of concatenated dataframes to AWS\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    ##### Rename\n",
    "    \n",
    "\n",
    "    ##### Prepare final concatenated dataset for export\n",
    "\n",
    "    # Delete unnecessary columns and set index\n",
    "    df_concat = df_concat.drop([\"hour\", \"day\", \"month\", \"year\", \"date\"], axis=1)\n",
    "    df_to_export = df_concat.set_index([\"station\", \"time\"])\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_to_export.to_xarray()\n",
    "\n",
    "    # Convert datatype of station coordinate\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "    # Include past attributes\n",
    "    for i in attrs_new:\n",
    "        ds_concat.attrs[i] = attrs_new[i]\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nstation_matching.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Intermediary data product. This data has been subjected to cleaning, QA/QC, but may not have been standardized.\"\n",
    "    )\n",
    "\n",
    "    # Extract old and new station names from name dictionary\n",
    "    station_name_new = station_names[\"station_name_new\"]\n",
    "    station_name_old = station_names[\"old_stations\"]\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "            station_name_old,\n",
    "            station_name_new  # extract old and new station names from name dictionary\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ##### Export\n",
    "    # ! a test name is used below\n",
    "    # ! the final name will be that of the newer dataframe\n",
    "    export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "        network_name, \"TEST_concat\", station_name_new\n",
    "    )\n",
    "    print(\"Exporting....\", export_url)\n",
    "    # ds_concat.to_zarr(export_url, mode=\"w\") ## WHEN READY TO EXPORT\n",
    "\n",
    "    # ! output final concatenated dataset for testing\n",
    "    return ds_concat \n",
    "\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _overlap_concat(df_new: pd.DataFrame, df_old: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles the cases in which there is overlap between the two input stations\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_new: pd.DataFrame\n",
    "            weather station network\n",
    "        df_old: pd.DataFrame\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            df_concat: pd.DataFrame\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = pd.concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_concat(\n",
    "    df_1: pd.DataFrame, df_2: pd.DataFrame, attrs_1: dict, attrs_2: dict\n",
    ") -> tuple[pd.DataFrame, str, str, dict]:\n",
    "    \"\"\"\n",
    "    Performs concatenation of input datasets, handling two cases\n",
    "        1.) temporal overlap between the datasets\n",
    "        2.) no temporal overlap\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df_1: pd.DataFrame\n",
    "            station data\n",
    "        df_2: pd.DataFrame\n",
    "            dtation data\n",
    "        attrs_1: list of str\n",
    "            attributes of df_1\n",
    "        attrs_2: list of str\n",
    "            attributes of df_2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            df_concat: pd.DataFrame\n",
    "            stn_n_to_keep: str\n",
    "            stn_n_to_drop: str\n",
    "            attrs_new: dict\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # determine which dataset is older\n",
    "    if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "        # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "        # we also grab the name of the newer station in this step, for use later\n",
    "        df_new = df_2\n",
    "        attrs_new = attrs_2\n",
    "        df_old = df_1\n",
    "\n",
    "    else:\n",
    "        df_new = df_1\n",
    "        attrs_new = attrs_1\n",
    "        df_old = df_2\n",
    "\n",
    "    stn_n_to_keep = df_new[\"station\"].unique()[0]\n",
    "    stn_n_to_drop = df_old[\"station\"].unique()[0]\n",
    "    print(f\"Station will be concatenated and saved as: {stn_n_to_keep}\")\n",
    "\n",
    "    # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # If there is no overlap between the two time series, just concatenate\n",
    "    if len(df_overlap) == 0:\n",
    "        print(\"No overlap!\")\n",
    "        df_concat = pd.merge(df_old, df_new, how=\"outer\")\n",
    "        df_concat[\"station\"] = stn_n_to_keep\n",
    "\n",
    "    # If overlap exists, split into subsets and concatenate\n",
    "    else:\n",
    "        print(\"There is overlap\")\n",
    "        df_concat = _overlap_concat(df_old, df_new)\n",
    "\n",
    "    return df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _more_than_2(network_name: str, stns_to_pair: pd.DataFrame) -> tuple[pd.DataFrame, dict, dict]:\n",
    "    \"\"\"\n",
    "    Performs pairwise concatenation on subsets of more than two stations flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        stns_to_pair: pd.DataFrame\n",
    "            dataframe of the input station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            df_concat: pd.DataFrame\n",
    "            station_names: dict\n",
    "            attrs_new: dict\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Concatenating the following stations:\", stns_to_pair)\n",
    "\n",
    "    # Load datasets into a list\n",
    "    datasets = [\n",
    "        xr.open_zarr(\n",
    "            \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stn\n",
    "            ),\n",
    "            consolidated=True,\n",
    "        )\n",
    "        for stn in stns_to_pair['ERA-ID']\n",
    "    ]\n",
    "\n",
    "    # Sort datasets by their max 'time'\n",
    "    datasets_sorted = sorted(datasets, key=lambda ds: ds['time'].max())\n",
    "\n",
    "    # Store station names, in order from oldest to newest\n",
    "    names = [ds.coords[\"station\"].values[0] for ds in datasets_sorted]\n",
    "\n",
    "    print('Newest station:', names[-1])\n",
    "\n",
    "    # Setup for the while loop\n",
    "    ds_1 = datasets_sorted[0]\n",
    "    df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(\n",
    "        ds_1, verbose=False\n",
    "    )\n",
    "    i = 0\n",
    "    end = len(datasets_sorted) -1\n",
    "\n",
    "    while i < end:\n",
    "\n",
    "        print('iteration:', i)\n",
    "\n",
    "        ds_2 = datasets_sorted[i+1]\n",
    "        df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(\n",
    "            ds_2, verbose=False\n",
    "        )\n",
    "\n",
    "        # Send to helper function for concatenation\n",
    "        df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "            df_1, df_2, attrs_1, attrs_2\n",
    "        )\n",
    "\n",
    "        df_1 = df_concat\n",
    "        attrs_1 = attrs_new\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Construct station names list, for updating attributes\n",
    "    newest_station = names[-1] # Get last station name from station name list\n",
    "    older_stations = \", \".join(names[:-1]) # Create a string containing all older station names\n",
    "    station_names = {\"station_name_new\": newest_station, \"old_stations\": older_stations}\n",
    "\n",
    "    print('Progressive concatenation for 2+ stations is complete.')\n",
    "\n",
    "    new_column = [newest_station] * len(df_concat)\n",
    "\n",
    "    df_concat['station'] = new_column\n",
    "\n",
    "    return df_concat, station_names, attrs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_stations(network_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Coordinates the concatenation of input datasets and exports the final concatenated dataset.\n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            final_concat_list: list[str]\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "    Uses the following helper functions\n",
    "        _df_concat(): concatenates two dataframes\n",
    "        _overlap_concat(): used by _df_concat() to concatenate two stations with overlapping time ranges\n",
    "        _more_than_2(): handles subsets with more than two stations, passing pairs to _df_concat() iteratively\n",
    "        _concat_export_help(): formats and exports concatenated dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    # Initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    # Read in full concat station list\n",
    "    print(network_name)\n",
    "    concat_list = pd.read_csv(\n",
    "        f\"s3://wecc-historical-wx/3_qaqc_wx/{network_name}/concat_list_{network_name}.csv\"\n",
    "    )\n",
    "\n",
    "    # Identify stns within designated network\n",
    "    concat_by_network = concat_list.loc[\n",
    "        concat_list.concat_subset.str.contains(network_name)\n",
    "    ]\n",
    "\n",
    "    ######### ! for testing\n",
    "    concat_by_network = concat_list[concat_list[\"concat_subset\"] == \"ASOSAWOS_3\"]\n",
    "    # concat_by_network = concat_by_network.head(4)\n",
    "    ######### ! for testing\n",
    "\n",
    "    # For MARITIME, remove these stations becuase they're actually separate stations\n",
    "    if network_name == 'MARITIME':\n",
    "        unique_pair_names = unique_pair_names[1:]\n",
    "        unique_pair_name = unique_pair_name[~unique_pair_name[\"ERA-ID\"].isin['MARITIME_LJPC1','MARITIME_LJAC1']]\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    unique_pair_names = concat_by_network.concat_subset.unique()\n",
    "    print(\n",
    "        f\"There are {len(concat_by_network)} stations to be concatenated into {len(unique_pair_names)} station pairs within {network_name}...\"\n",
    "    )\n",
    "\n",
    "    print(unique_pair_names)\n",
    "\n",
    "    # Set up pairs\n",
    "    for pair in unique_pair_names:\n",
    "        print(pair)\n",
    "        # Pull out stations corresponding to pair name\n",
    "        stns_to_pair = concat_by_network.loc[concat_by_network.concat_subset == pair]\n",
    "\n",
    "        if len(stns_to_pair) == 2:  # 2 stations to concat together\n",
    "            print(\"\\n\", stns_to_pair)\n",
    "\n",
    "            # Import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[0][\"ERA-ID\"]\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, stns_to_pair.iloc[1][\"ERA-ID\"]\n",
    "            )\n",
    "\n",
    "            print(\"Retrieving....\", url_1)\n",
    "            print(\"Retrieving....\", url_2)\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            # Convert to dataframes with corresponding information\n",
    "            df_1, MultiIndex_1, attrs_1, var_attrs_1, era_qc_vars_1 = qaqc_ds_to_df(\n",
    "                ds_1, verbose=False\n",
    "            )\n",
    "            df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(\n",
    "                ds_2, verbose=False\n",
    "            )\n",
    "\n",
    "            # Send to helper function for concatenation\n",
    "            df_concat, stn_n_to_keep, stn_n_to_drop, attrs_new = _df_concat(\n",
    "                df_1, df_2, attrs_1, attrs_2\n",
    "            )\n",
    "\n",
    "            # Construct dictionary of old and new station names\n",
    "            station_names ={\"station_name_new\":stn_n_to_keep, \"old_stations\":stn_n_to_drop}\n",
    "\n",
    "        else:\n",
    "            # If there are more than 2 stations in the given subset, pass to _more_than_2()\n",
    "            print(\"More than 2 stations within a subset.\")\n",
    "            df_concat, station_names, attrs_new = _more_than_2(\n",
    "                network_name,\n",
    "                stns_to_pair\n",
    "            )\n",
    "\n",
    "        # Add concatenated station names to station name list\n",
    "        final_concat_list.extend(stns_to_pair[\"ERA-ID\"].tolist())\n",
    "\n",
    "        # Send concatenated dataframe to helper function for export\n",
    "        ds_final = _concat_export_help(  #! returns ds_final for testing\n",
    "            df_concat,\n",
    "            final_concat_list,\n",
    "            network_name,\n",
    "            attrs_new,\n",
    "            station_names,\n",
    "        )\n",
    "\n",
    "    print(\"Concatenated stations: \", final_concat_list)\n",
    "\n",
    "    return ds_final, final_concat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"ASOSAWOS\"\n",
    "final_concat_list = concatenate_stations(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the order of operations:\n",
    "\n",
    "1. Read in target stations, for each concat_flag\n",
    "2. Check if there is overlap in time ranges\n",
    "    1. IF so:  \n",
    "\n",
    "        split overall time range\n",
    "\n",
    "        construct dataset by grabbing newest station for each time range subset\n",
    "\n",
    "    \n",
    "    2. ELSE:\n",
    "\n",
    "        concatenate, with NAs in the gap\n",
    "\n",
    "\n",
    "Another option: pairwise concetenation\n",
    "\n",
    "For each subset of matching stations, first concatenate the two newest stations. Then, the next oldest, etc.\n",
    "\n",
    "\n",
    "Issues to address:\n",
    "\n",
    "1. when the time range of one station completely includes that of another in a subset (this occures a few times with ASOSAWOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate pairs of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of variables to be assigned\n",
    "\n",
    "float64_variables = [\n",
    "    \"anemometer_height_m\",\n",
    "    \"elevation\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"pr_15min\",\n",
    "    \"thermometer_height_m\",\n",
    "    \"ps\",\n",
    "    \"tas\",\n",
    "    \"tdps\",\n",
    "    \"pr\",\n",
    "    \"sfcWind\",\n",
    "    \"sfcWind_dir\",\n",
    "    \"ps_altimeter\",\n",
    "    \"pr_duration\",\n",
    "    \"ps_eraqc\",\n",
    "    \"tas_eraqc\",\n",
    "    \"tdps_eraqc\",\n",
    "    \"pr_eraqc\",\n",
    "    \"sfcWind_eraqc\",\n",
    "    \"sfcWind_dir_eraqc\",\n",
    "    \"elevation_eraqc\",\n",
    "    \"ps_altimeter_eraqc\",\n",
    "    \"pr_15min_eraqc\",\n",
    "]\n",
    "U16_variables = [\n",
    "    #\"raw_qc\",\n",
    "    \"qaqc_process\",\n",
    "    \"ps_qc\",\n",
    "    \"ps_altimeter_qc\",\n",
    "    \"psl_qc\",\n",
    "    \"tas_qc\",\n",
    "    \"tdps_qc\",\n",
    "    \"pr_qc\",\n",
    "    \"pr_depth_qc\",\n",
    "    \"sfcWind_qc\",\n",
    "    \"sfcWind_method\",\n",
    "    \"sfcWind_dir_qc\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datatypes(ds):\n",
    "    \"\"\"\n",
    "    Converts the datatypes of variables in a dataset based on external libraries. \n",
    "    Used in the station concatenation function.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        ds: xr.Dataset\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output dataset with coverted datatypes\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "    Uses the following externally defined dictionaries to assign datatypes to variables:\n",
    "    float32_variables: List\n",
    "            list of variables that will be converted to datatpe \"float64\"\n",
    "    U16_variables: List\n",
    "            list of variables that will be converted to datatpe \"<U16\"\n",
    "    \"\"\"\n",
    "    # Generate lists of variables from the external dicionaries that are actually present in the input dataset\n",
    "    existing_float64 = [\n",
    "        key for key in float64_variables if key in list(ds.keys())\n",
    "    ]\n",
    "    existing_U16 = [key for key in U16_variables if key in list(ds.keys())]\n",
    "\n",
    "    # Convert the datatypes of those variables, but only if those variables exist\n",
    "    if len(existing_float64) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        ds[existing_float64] = ds[existing_float64].astype(\"float64\")\n",
    "    \n",
    "    if len(existing_U16) == 0:\n",
    "        pass\n",
    "    else: \n",
    "        ds[existing_U16] = ds[existing_U16].astype(\"<U16\")\n",
    "\n",
    "    # And of the coordinates as well\n",
    "    ds.coords[\"station\"] = ds.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            # retrieve ERA IDs in this subset of stations\n",
    "            station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "            station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "            final_concat_list.append(station_1)\n",
    "            final_concat_list.append(station_2)\n",
    "\n",
    "            # import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_1\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_2\n",
    "            )\n",
    "\n",
    "            # TODO: open_zarr will be used for QAQC'd datasets\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            df_1 = ds_1.to_dataframe()\n",
    "            df_2 = ds_2.to_dataframe()\n",
    "\n",
    "            # apply reset index only to 'time', as we will need that for concatenation\n",
    "            df_1 = df_1.reset_index(level=\"time\")\n",
    "            df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "            # determine which dataset is older\n",
    "            if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "                # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                # we also grab the name of the newer station in this step, for use later\n",
    "                df_new = df_2\n",
    "                ds_new = ds_2\n",
    "\n",
    "                df_old = df_1\n",
    "                ds_old = ds_1\n",
    "            else:\n",
    "                df_new = df_1\n",
    "                ds_new = ds_1\n",
    "\n",
    "                df_old = df_2\n",
    "                ds_old = ds_2\n",
    "\n",
    "            # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "            df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "            # if there is no overlap between the two time series, just concatenate\n",
    "            if len(df_overlap) == 0:\n",
    "                df_concat = concat([df_old, df_new])\n",
    "\n",
    "            # if not, split into subsets and concatenate\n",
    "            else: \n",
    "                ##### Split datframes into subsets #####\n",
    "\n",
    "                # Remove data in time overlap between old and new\n",
    "                df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                ##### Concatenate subsets #####\n",
    "                df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "            ##### Now prepare the final concatenated dataframe for export\n",
    "            station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "            final_station_name = \"{}\".format(station_name_new)\n",
    "            new_index = [final_station_name] * len(df_concat)\n",
    "            df_concat.index = new_index\n",
    "            df_concat.index.name = \"station\"\n",
    "\n",
    "            # drop duplicate rows that were potentially generated in the concatenation process\n",
    "            df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "            # Add 'time' back into multi index\n",
    "            df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "            # Convert concatenated dataframe to dataset\n",
    "            ds_concat = df_concat.to_xarray()\n",
    "\n",
    "            #### Update attributes and datatypes #####\n",
    "\n",
    "            # Include past attributes\n",
    "            ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "            # Update 'history' attribute\n",
    "            timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "            ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "                \"history\"\n",
    "            ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "            # Update 'comment' attribute\n",
    "            ds_concat.attrs[\"comment\"] = (\n",
    "                \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "            )\n",
    "\n",
    "            # Add new qaqc_files_merged attribute\n",
    "            station_name_old = ds_old.coords[\"station\"].values[0]\n",
    "            ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "                    network_name, station_name_old, network_name, station_name_new\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert all datatypes, to enable export\n",
    "            ds_concat = convert_datatypes(ds_concat)\n",
    "\n",
    "            # ## Export ###\n",
    "            # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "            #     network_name, \"test_concat\", network_name # station_name_new\n",
    "            # )\n",
    "            # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return final_concat_list  \n",
    "    # return df_concat, df_new, df_old, df_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"VALLEYWATER\"\n",
    "final_concat_list = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "    network_name, station_1\n",
    ")\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "    network_name, station_2\n",
    ")\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1_original = ds_1.to_dataframe()\n",
    "df_2_original = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat_original = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the concatenated dataframe contain the input dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input_df is a subset of concatenated_df\n",
    "is_subset = df_1_original.isin(df_concat_original).all().all()\n",
    "\n",
    "print(\"Input DataFrame is a subset of concatenated DataFrame:\", is_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1_original.reset_index(level=\"time\")\n",
    "df_2 = df_2_original.reset_index(level=\"time\")\n",
    "df_concat = df_concat_original.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if time range of new dataframe is in the concatenated dataframe\n",
    "is_subset = df_1['time'].isin(df_concat['time']).all().all()\n",
    "\n",
    "print(\"Input DataFrame is a subset of concatenated DataFrame:\", is_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_1_overlap = df_1[df_1[\"time\"].isin(df_concat[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_1[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_overlap[\"lat\"] = df_1_overlap[\"lat\"].round(3)\n",
    "df_1_overlap[\"lon\"] = df_1_overlap[\"lon\"].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap[\"lat\"] = df_concat_overlap[\"lat\"].round(3)\n",
    "df_concat_overlap[\"lon\"] = df_concat_overlap[\"lon\"].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_overlap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_1[\"time\"], df_1[\"pr_15min\"])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_2[\"time\"], df_2[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_test(concat_list):\n",
    "    \"\"\"\n",
    "    Performs concatenation for stations in list of stations flagged for concatenation.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Import target datasets and convert to dataframe\n",
    "    flag_range = list(\n",
    "        range(concat_list[\"concat_flag\"].min(), concat_list[\"concat_flag\"].max())\n",
    "    )\n",
    "\n",
    "    for i in flag_range:\n",
    "        subset_list = concat_list[concat_list[\"concat_flag\"] == i]\n",
    "        subset_range = list(range(0, len(subset_list)))\n",
    "\n",
    "        url = {}\n",
    "        ds = {}\n",
    "        df = {}\n",
    "\n",
    "        for i in subset_range:\n",
    "\n",
    "            # extract information needed for dataset import\n",
    "            row_i = subset_list.iloc[[i]]\n",
    "            network_name = row_i[\"ERA-ID\"].split(\"_\")[\n",
    "                0\n",
    "            ]  # TODO: this does not work, when it really should\n",
    "            station_name = row_i[\"ERA-ID\"]\n",
    "\n",
    "            url[i] = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                network_name, network_name, station_name\n",
    "            )\n",
    "\n",
    "            ds[i] = xr.open_zarr(url[i])\n",
    "\n",
    "            df[i] = ds[i].to_dataframe()\n",
    "\n",
    "            # Apply reset index only to 'time', as we will need that for concatenation\n",
    "            df[i] = df[i].reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE SCRAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = asosawos_list_concat.groupby([\"ICAO\"]).apply(\n",
    "    lambda x: x.sort_values([\"end_time\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by end_time or end-date, depending on the station TODO: this is not necessary\n",
    "# time_var_list = ['end_time','end-date']\n",
    "# end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "# new_station_list = new_station_list.groupby('concat_flag').apply(lambda x: x.sort_values(end_time_or_date)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(df):\n",
    "    \"\"\"\n",
    "    Concatenates station data that has been flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.dataframe\n",
    "            staton data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "    # if there is overlap, then create subsets\n",
    "\n",
    "    # if no overlap, just concatenate\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_station_list(station_list, concat_list, duplicate_list):\n",
    "    \"\"\"\n",
    "    Reorders the input station list, necessary for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.dataframe\n",
    "\n",
    "        concat_list: pd.dataframe\n",
    "\n",
    "        duplicate_list: pd.dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output station list with stations to be concatenated at top, followed by potential duplicates\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    ##### subsets of station list\n",
    "\n",
    "    # stations that will be concatenated\n",
    "    concat_stations = station_list[station_list[\"ICAO\"].isin(concat_list)]\n",
    "\n",
    "    # potential duplicate stations\n",
    "    duplicate_stations = station_list[station_list[\"ICAO\"].isin(duplicate_list)]\n",
    "\n",
    "    # all remaining stations\n",
    "    remaining_stations = station_list[\n",
    "        ~station_list[\"ICAO\"].isin(duplicate_list + concat_list)\n",
    "    ]\n",
    "\n",
    "    ##### sort concat list alphabetically, to ensure that stations with the same ICAO are grouped together\n",
    "    concat_stations = concat_stations.sort_values(\"ICAO\")\n",
    "    duplicate_stations = duplicate_stations.sort_values(\"ICAO\")\n",
    "\n",
    "    ##### now within each ICAO, order by end time\n",
    "    concat_stations = concat_stations.groupby([\"ICAO\"]).apply(\n",
    "        lambda x: x.sort_values([\"end_time\"])\n",
    "    )\n",
    "\n",
    "    ##### concatenate susbets and reset index\n",
    "    new_list = concat(\n",
    "        [concat_stations, duplicate_stations, remaining_stations]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for presence of start and end times\n",
    "\n",
    "time_check = repeat_list_subset.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "print(\"number of null start times:\")\n",
    "print(time_check[\"start_time\"].sum())\n",
    "\n",
    "print(\"number of null end times:\")\n",
    "print(time_check[\"end_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the start and end times are identical\n",
    "\n",
    "start_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")\n",
    "end_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\"ICAO\"].tolist()\n",
    "start_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "    \"ICAO\"\n",
    "].tolist()\n",
    "\n",
    "print(end_list)\n",
    "print(start_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is going on with the stations that have duplicate start and end times? are they true duplicates?\n",
    "\n",
    "repeat_list_subset[repeat_list_subset[\"ICAO\"].isin(start_list + end_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in single dc file from AWS\n",
    "ds_1 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_72026294076\", temp_dir)\n",
    "ds_2 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_A0000594076\", temp_dir)\n",
    "\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "df_2 = qaqc_ds_to_df(ds_2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = df_1.lon.mean()\n",
    "lat = df_1.lat.mean()\n",
    "# print(\"{}, {:.5f}, {:.5f}\".format(id, lon, lat))\n",
    "\n",
    "\n",
    "# Plot time series of the data\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "df_1.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "df_2.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "\n",
    "ax.set_title(\"{}  ({:.3f}, {:.3f})\".format(id, lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_check_old(station_list):\n",
    "    \"\"\"\n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            list\n",
    "                list of ICAO values of stations that need to be concatenated\n",
    "            list\n",
    "                list of ICAO values of potential duplicate stations\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Generate list of repeat ICAOs\n",
    "    repeat_list = station_list[station_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "    repeat_list = repeat_list[\n",
    "        [\"ICAO\", \"ERA-ID\", \"STATION NAME\", \"start_time\", \"end_time\"]\n",
    "    ]\n",
    "\n",
    "    concat_list = repeat_list[\"ICAO\"].unique().tolist()\n",
    "\n",
    "    # And empty list to add potential duplicates to\n",
    "    duplicate_list = []\n",
    "\n",
    "    ##### Generate boolean for whether or not there are null start and/or end times\n",
    "    # TODO: may not be necessary\n",
    "    time_check = repeat_list.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "    end_nan_list = time_check[time_check[\"end_time\"] == True][\"ICAO\"].tolist()\n",
    "    start_nan_list = time_check[time_check[\"start_time\"] == True][\"ICAO\"].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_nan_list + end_nan_list\n",
    "\n",
    "    duplicate_list = duplicate_list\n",
    "\n",
    "    ##### Identify ICAOs with duplicate start end times\n",
    "    start_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    end_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    end_dup_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "    start_dup_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_dup_list + end_dup_list\n",
    "\n",
    "    # Generate final list of ICAOs for stations to be concatenated\n",
    "    concat_list = [x for x in concat_list if x not in duplicate_list]\n",
    "\n",
    "    return concat_list, duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the subset with only stations to concatenate\n",
    "\n",
    "asosawos_list_concat[\"ICAO\"] = pd.Categorical(\n",
    "    asosawos_list_concat[\"ICAO\"], categories=concat_list, ordered=True\n",
    ")\n",
    "\n",
    "test_list = asosawos_list_concat.sort_values(\"ICAO\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations within a certain distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into GeoDataFrames\n",
    "# using EPSG 3310\n",
    "\n",
    "gdf_asosawos = gpd.GeoDataFrame(\n",
    "    asosawos_list,\n",
    "    geometry=[\n",
    "        Point(lon, lat) for lon, lat in zip(asosawos_list[\"LON\"], asosawos_list[\"LAT\"])\n",
    "    ],\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 3: find the nearest point in the geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert emtpy columns\n",
    "\n",
    "gdf_asosawos[\"nearest_station\"] = pd.Series(dtype=\"U16\")\n",
    "gdf_asosawos[\"distance\"] = pd.Series(dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_asosawos.iterrows():\n",
    "    # geometry of individual row \n",
    "    point = row.geometry\n",
    "    # returns a multipoint object with the geometries of every row in the gdf\n",
    "    multipoint = gdf_asosawos.drop(index, axis=0).geometry.unary_union\n",
    "    # \n",
    "    queried_geom, nearest_geom = nearest_points(point, multipoint)\n",
    "    dist_from_point = \n",
    "    gdf_asosawos.loc[index, 'nearest_geometry'] = nearest_geom\n",
    "    gdf_asosawos.loc[index, 'distance'] = nearest_geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 2: distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the distance between points\n",
    "\n",
    "\n",
    "def distance_sort_filter(row, df2, buffer=None, id=False):\n",
    "\n",
    "    dist = df2.geometry.distance(row).sort_values()\n",
    "\n",
    "    if buffer:\n",
    "        dist = dist[dist < buffer]\n",
    "\n",
    "    if id:\n",
    "        distances = {\n",
    "            df2.loc[idx][\"WBAN\"]: value for idx, value in zip(dist.index, dist.values)\n",
    "        }\n",
    "    else:\n",
    "        distances = {idx: value for idx, value in zip(dist.index, dist.values)}\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 1: using sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a buffer around points in gdf1 (e.g., 10 km buffer)\n",
    "gdf_asosawos[\"buffer\"] = gdf_asosawos.geometry.buffer(\n",
    "    0.1\n",
    ")  # Buffer in degrees, 0.1 degrees approx equals 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join using the buffer\n",
    "merged = gpd.sjoin(\n",
    "    gdf_asosawos, gdf_asosawos[[\"geometry\", \"buffer\"]], how=\"inner\", predicate=\"within\"\n",
    ")\n",
    "\n",
    "# The 'merged' GeoDataFrame contains points from gdf_isd that are within the buffer around points in gdf_asosawos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    merged\n",
    ")  # there are not ISD stations within 10km of an ASOSAWOS station missed by the exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Round asosawos down to 3 decimal points of accuracy\n",
    "# asosawos_round = asosawos_list.round({\"LAT\": 3, \"LON\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential ways to check that two stations are duplicates\n",
    "1. identical total_nobs\n",
    "2. identical ERA IDs\n",
    "3. identical end or start times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract flagged stations\n",
    "\n",
    "asosawos_dup = asosawos_out[~asosawos_out[\"concat_flag\"].isna()]\n",
    "valleywater_dup = valleywater_out[~valleywater_out[\"concat_flag\"].isna()]\n",
    "maritime_dup = maritime_out[~maritime_out[\"concat_flag\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that are potentially duplicates\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Within stations flagged for concatenation, stations are flagged as potential duplicates\n",
    "            if either their start or end times are identical\n",
    "            - TODO: brainstorm alternative approaches\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information that has passed through the concatenation check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    ##### flag stations with repeat end or start times\n",
    "\n",
    "    time_end_list = [\"end_time\", \"end-date\"]\n",
    "    time_start_list = [\"start_time\", \"start-date\"]\n",
    "\n",
    "    end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "\n",
    "    new_station_list = (\n",
    "        new_station_list.groupby(\"concat_flag\")\n",
    "        .apply(lambda x: x.sort_values(end_time_or_date))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return new_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(network_name, station_old, station_new):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Import target datasets and convert to dataframe\n",
    "    old_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "    new_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "\n",
    "    ds_old = xr.open_zarr(old_url)\n",
    "    ds_new = xr.open_zarr(new_url)\n",
    "\n",
    "    df_old = ds_old.to_dataframe()\n",
    "    df_new = ds_new.to_dataframe()\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}/concat_list_{}.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    # ! you can truncate the concat list here, for testing\n",
    "    concat_list = concat_list.head(2)\n",
    "    # ! end\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            try: \n",
    "                # retrieve ERA IDs in this subset of stations\n",
    "                station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "                station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "                # import this subset of datasets and convert to dataframe\n",
    "                url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_1\n",
    "                )\n",
    "                url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_2\n",
    "                )\n",
    "\n",
    "                ds_1 = xr.open_zarr(url_1)\n",
    "                ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "                df_1,MultiIndex_1,attrs_1,var_attrs_1,era_qc_vars_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "                df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(ds_2, verbose=False)\n",
    "\n",
    "                # determine which dataset is older\n",
    "                if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "                    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                    # we also grab the name of the newer station in this step, for use later\n",
    "                    df_new = df_2\n",
    "                    ds_new = ds_2\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_1\n",
    "                    ds_old = ds_1\n",
    "                    MultiIndex_old = MultiIndex_1\n",
    "\n",
    "                else:\n",
    "                    df_new = df_1\n",
    "                    ds_new = df_1\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_2\n",
    "                    ds_old = ds_2\n",
    "                    MultiIndex_old = MultiIndex_2\n",
    "\n",
    "                # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "                df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "                # if there is no overlap between the two time series, just concatenate\n",
    "                if len(df_overlap) == 0:\n",
    "                    df_concat = concat([df_old, df_new])\n",
    "\n",
    "                # if not, split into subsets and concatenate\n",
    "                else:\n",
    "                    ##### Split datframes into subsets #####\n",
    "\n",
    "                    # Remove data in time overlap between old and new\n",
    "                    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                    ##### Concatenate subsets #####\n",
    "                    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "                # ##### Now prepare the final concatenated dataframe for export\n",
    "                station_name_new = MultiIndex_new.get_level_values(\"station\")[1]\n",
    "                \n",
    "                # ! This is where Neil and I made the change to address the issues\n",
    "                # ! \n",
    "                MultiIndex_old = pd.MultiIndex.from_tuples(\n",
    "                    [(station_name_new, lvl1) for _, lvl1 in MultiIndex_old],\n",
    "                    names=MultiIndex_new.names,\n",
    "                )\n",
    "\n",
    "                MultiIndex_concat = MultiIndex_new.union(MultiIndex_old)\n",
    "\n",
    "                # drop duplicate rows that were potentially generated in the concatenation process\n",
    "                df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "                # drop 'station' and 'time'columns\n",
    "                df_concat = df_concat.drop([\"station\", \"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"], axis=1)\n",
    "\n",
    "                print('length of MultiIndex_new')\n",
    "                print(len(MultiIndex_new))\n",
    "                print(\"length of MultiIndex_old\")\n",
    "                print(len(MultiIndex_old))\n",
    "                print(\"length of MultiIndex_concat\")\n",
    "                print(len(MultiIndex_concat))\n",
    "\n",
    "                print(\"length of df_new\")\n",
    "                print(len(df_new))\n",
    "                print(\"length of df_old\")\n",
    "                print(len(df_old))\n",
    "                print(\"length of df_concat\")\n",
    "                print(len(df_concat))\n",
    "\n",
    "                # ! This is where the issue! MultiIndex_concat and df_concat have difference lengths\n",
    "                df_concat.index = MultiIndex_concat\n",
    "\n",
    "                # # Convert concatenated dataframe to dataset\n",
    "                # ds_concat = df_concat.to_xarray()\n",
    "\n",
    "                # # #### Prepare for export #####\n",
    "\n",
    "                # # Convert datatype of station coordinate\n",
    "                # ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "                # # # Include past attributes\n",
    "                # ds_concat.attrs.update(attrs_new)\n",
    "\n",
    "                # # Update 'history' attribute\n",
    "                # timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "                # ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "                #     \"history\"\n",
    "                # ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "                # # Update 'comment' attribute\n",
    "                # ds_concat.attrs[\"comment\"] = (\n",
    "                #     \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "                # )\n",
    "\n",
    "                # # Add new qaqc_files_merged attribute\n",
    "                # station_name_old = MultiIndex_old.get_level_values(\"station\")[1]\n",
    "                # ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                #     \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "                #         station_name_old, station_name_new\n",
    "                #     )\n",
    "                # )\n",
    "\n",
    "                # ! this is here the renaming will go\n",
    "\n",
    "                # !\n",
    "\n",
    "                # ## Export ###\n",
    "                # ! a test name is used below\n",
    "                # ! the final name will be that of the newer dataframe\n",
    "                # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                #     network_name, \"test_concat\", station_name_new\n",
    "                # )\n",
    "                # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "                # record that the stations were concatenated\n",
    "                final_concat_list.append(station_1)\n",
    "                final_concat_list.append(station_2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"Error concatenating subset {}: {}\".format(subset_i, e)\n",
    "                )\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # return final_concat_list # ! this will be the final return statement, below is inlcluded for testing\n",
    "    # return (\n",
    "    #     df_new,\n",
    "    #     df_old,\n",
    "    #     df_concat,\n",
    "    #     ds_concat,\n",
    "    #     final_concat_list,\n",
    "    # )\n",
    "\n",
    "    return df_1, df_2, MultiIndex_1, MultiIndex_2, df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"MARITIME\" # \"VALLEYWATER\", \"MARITIME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1, df_2, MultiIndex_1, MultiIndex_2 = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LJAC1 - this should be new\n",
    "print(df_1['time'].min())\n",
    "print(df_1[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LJPC1\n",
    "print(df_2[\"time\"].min())\n",
    "print(df_2[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which dataset is older\n",
    "if df_2[\"time\"].max() > df_1[\"time\"].max():\n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    MultiIndex_new = MultiIndex_2\n",
    "\n",
    "    df_old = df_1\n",
    "    MultiIndex_old = MultiIndex_1\n",
    "\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = df_1\n",
    "    MultiIndex_new = MultiIndex_1\n",
    "\n",
    "    df_old = df_2\n",
    "    MultiIndex_old = MultiIndex_2\n",
    "\n",
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no overlap between the two time series, just concatenate\n",
    "if len(df_overlap) == 0:\n",
    "    df_concat = concat([df_old, df_new])\n",
    "\n",
    "# if not, split into subsets and concatenate\n",
    "else:\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Now prepare the final concatenated dataframe for export\n",
    "station_name_new = MultiIndex_new.get_level_values(\"station\")[1]\n",
    "\n",
    "MultiIndex_old = pd.MultiIndex.from_tuples(\n",
    "    [(station_name_new, lvl1) for _, lvl1 in MultiIndex_old],\n",
    "    names=MultiIndex_new.names,\n",
    ")\n",
    "\n",
    "MultiIndex_concat = MultiIndex_new.union(MultiIndex_old)\n",
    "\n",
    "\n",
    "# MultiIndex_concat = pd.MultiIndex.from_tuples(\n",
    "#     [(station_name_new, lvl1) for _, lvl1 in MultiIndex_concat],\n",
    "#     names=MultiIndex_concat.names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows that were potentially generated in the concatenation process\n",
    "df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "# drop 'station' and 'time'columns\n",
    "df_concat = df_concat.drop([\"station\", \"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"], axis=1)\n",
    "\n",
    "df_concat.index = MultiIndex_concat\n",
    "\n",
    "# Convert concatenated dataframe to dataset\n",
    "ds_concat = df_concat.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union is the issue - mismatch in timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1['time'].min())\n",
    "print(df_1[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_2[\"time\"].min())\n",
    "print(df_2[\"time\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_concat should span 2005-01-01 01:30:00 - 2022-08-31 23:54:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MultiIndex_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows that were potentially generated in the concatenation process\n",
    "df_concat_drop_dups = df_concat.drop_duplicates(subset=[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_concat_drop_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiIndex_concat and df_concat_drop_dups['time']\n",
    "index_time = list(MultiIndex_1.get_level_values(\"time\"))\n",
    "#df_time = list(df_concat_drop_dups['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiIndex_1.get_level_values(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df_concat[df_concat['time'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'station' and 'time'columns\n",
    "df_concat = df_concat.drop(\n",
    "    [\"station\", \"time\", \"hour\", \"day\", \"month\", \"year\", \"date\"], axis=1\n",
    ")\n",
    "\n",
    "df_concat.index = MultiIndex_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 1\n",
    "\n",
    "Run concatenate_station_pairs() as is, so the function does not export and instead returns df_concat, df_new, df_old, and df_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_new,\n",
    "    df_old,\n",
    "    df_concat,\n",
    "    ds_concat,\n",
    "    final_concat_list,\n",
    ") = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 2: \n",
    "\n",
    "Run concatenate_station_pairs() with the first return statement uncommented and the second commented, and the export section uncommented. So that the function actually exports the concatenated datasets. I've generated all the concatention lists (for VALLEYWATER, MARITIME, and ASOSAWOS) needed to run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "# TODO: you'll need to change the url\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}/{}_concat_list_{}.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_1)\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_2)\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1 = ds_1.to_dataframe()\n",
    "df_2 = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1.reset_index(level=\"time\")\n",
    "df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "\n",
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    ds_new = ds_2\n",
    "\n",
    "    df_old = df_1\n",
    "    ds_old = ds_1\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = ds_1\n",
    "\n",
    "    df_old = df_2\n",
    "    ds_old = ds_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_new_overlap = df_new[df_new[\"time\"].isin(df_concat[\"time\"])]\n",
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_new[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_overlap.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_var = 'ps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_new[\"time\"], df_new[vis_var])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_old[\"time\"], df_old[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
