{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n",
    "\n",
    "The goal of this notebook is to identify stations that changed IDs. This has been known to occur for Maritime and ASOSOAWOS stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "#s3 = s3fs.S3FileSystem  # must be set to this to use such commands as ls\n",
    "# s3 = boto3.resource('s3')\n",
    "# s3_client = boto3.client(\"s3\")\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3_clean(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr_from_s3(station_id, temp_dir):\n",
    "    \"\"\"Read zarr file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".zarr\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_{}.zarr\".format(\n",
    "        station_id\n",
    "    )\n",
    "    print(s3_url)\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"zarr\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for the pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        input data from the clean step\n",
    "    verbose : bool, optional\n",
    "        if True, provides runtime output to the terminal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        converted xr.Dataset into dataframe\n",
    "    MultiIndex : pd.Index\n",
    "        multi-index of station and time\n",
    "    attrs : list of str\n",
    "        attributes from xr.Dataset\n",
    "    var_attrs : list of str\n",
    "        variable attributes from xr.Dataset\n",
    "    era_qc_vars : list of str\n",
    "        QAQC variables\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is the notebook friendly version (no logger statements).\n",
    "    \"\"\"\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    # var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load station lists for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in ASOSAWOS stations\n",
    "\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "asosawos = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/ASOSAWOS/stationlist_ASOSAWOS_cleaned.csv\",\n",
    ")\n",
    "asosawos_list = pd.read_csv(BytesIO(asosawos[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valleywater = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/VALLEYWATER/stationlist_VALLEYWATER_cleaned.csv\",\n",
    ")\n",
    "valleywater_list = pd.read_csv(BytesIO(valleywater[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/MARITIME/stationlist_MARITIME_cleaned.csv\",\n",
    ")\n",
    "maritime_list = pd.read_csv(BytesIO(maritime[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes.\n",
    "\n",
    "Some additional methods to use:\n",
    "1. matching IDs, for stations in which those exist (NOT currently used)\n",
    "2. stations within a certain distance of each other (I've investigated this some, but would take consideraly more time to fully develop and may not be necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"VALLEYWATER\"]  # , \"ASOSAWOS\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"_copy\" \"/\"+ station + \"/\" + station + \"_concat_list_TEST.csv\"\n",
    "            #Key = qaqcdir + station + \"/{}_concat_list_TEST.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stations already indentified for concatenation are flagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maritime station:\n",
    "\n",
    "- MTYC1 and MEYC1\n",
    "\n",
    "- SMOC1 and ICAC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime_out\n",
    "\n",
    "# Flagged Stations:\n",
    "# MARITIME_LJAC1 <=> MARITIME_LJPC1\n",
    "# MARITIME_ICAC1 <=> MARITIME_SMOC1\n",
    "# MARITIME_MEYC1 <=> MARITIME_MTYC1 <=> MARITIME_MYXC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously identified stations are indeed flagged. Along with an additional pair: MARITIME_LJAC1 and MARITIME_LJPC1. And a third station included with MARITIME_MEYC1 abd MARITIME_MTYC1: MARITIME_MYXC1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using ICAO values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_list = asosawos_list[asosawos_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "\n",
    "# how many unique ICAO duplicates are there?\n",
    "print(len(repeat_list[\"ICAO\"].unique()))\n",
    "\n",
    "print(repeat_list.groupby(\"ICAO\").count().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(repeat_list[\"ICAO\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate problem station KMLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf = repeat_list[repeat_list[\"ICAO\"] == \"KMLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf[[\"STATION NAME\", \"LAT\", \"LON\", \"start_time\", \"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using station locations (lat, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataframe\n",
    "\n",
    "test = asosawos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols = [col for col in test.columns if col in lat_lon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = asosawos_list.duplicated(subset=lat_lon_cols, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = test[test[\"concat_flag\"] == True].groupby(lat_lon_cols).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var_list = [\"end_time\", \"end-date\"]\n",
    "end_time_col = [col for col in test.columns if col in time_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(\"concat_flag\")\n",
    "test = (\n",
    "    test.groupby([\"concat_flag\"])\n",
    "    .apply(lambda x: x.sort_values(end_time_col))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing ICAO identification and lat lon identification for ASOSAWOS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Carry Out Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the order of operations:\n",
    "\n",
    "1. Read in target stations, for each concat_flag\n",
    "2. Check if there is overlap in time ranges\n",
    "    1. IF so:  \n",
    "\n",
    "        split overall time range\n",
    "\n",
    "        construct dataset by grabbing newest station for each time range subset\n",
    "\n",
    "    \n",
    "    2. ELSE:\n",
    "\n",
    "        concatenate, with NAs in the gap\n",
    "\n",
    "\n",
    "Another option: pairwise concetenation\n",
    "\n",
    "For each subset of matching stations, first concatenate the two newest stations. Then, the next oldest, etc.\n",
    "\n",
    "\n",
    "Issues to address:\n",
    "\n",
    "1. when the time range of one station completely includes that of another in a subset (this occures a few times with ASOSAWOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate pairs of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of variables to be assigned\n",
    "\n",
    "float64_variables = [\n",
    "    \"anemometer_height_m\",\n",
    "    \"elevation\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"pr_15min\",\n",
    "    \"thermometer_height_m\",\n",
    "    \"ps\",\n",
    "    \"tas\",\n",
    "    \"tdps\",\n",
    "    \"pr\",\n",
    "    \"sfcWind\",\n",
    "    \"sfcWind_dir\",\n",
    "    \"ps_altimeter\",\n",
    "    \"pr_duration\",\n",
    "    \"ps_eraqc\",\n",
    "    \"tas_eraqc\",\n",
    "    \"tdps_eraqc\",\n",
    "    \"pr_eraqc\",\n",
    "    \"sfcWind_eraqc\",\n",
    "    \"sfcWind_dir_eraqc\",\n",
    "    \"elevation_eraqc\",\n",
    "    \"ps_altimeter_eraqc\",\n",
    "    \"pr_15min_eraqc\",\n",
    "]\n",
    "U16_variables = [\n",
    "    #\"raw_qc\",\n",
    "    \"qaqc_process\",\n",
    "    \"ps_qc\",\n",
    "    \"ps_altimeter_qc\",\n",
    "    \"psl_qc\",\n",
    "    \"tas_qc\",\n",
    "    \"tdps_qc\",\n",
    "    \"pr_qc\",\n",
    "    \"pr_depth_qc\",\n",
    "    \"sfcWind_qc\",\n",
    "    \"sfcWind_method\",\n",
    "    \"sfcWind_dir_qc\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datatypes(ds):\n",
    "    \"\"\"\n",
    "    Converts the datatypes of variables in a dataset based on external libraries. \n",
    "    Used in the station concatenation function.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        ds: xr.Dataset\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output dataset with coverted datatypes\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "    Uses the following externally defined dictionaries to assign datatypes to variables:\n",
    "    float32_variables: List\n",
    "            list of variables that will be converted to datatpe \"float64\"\n",
    "    U16_variables: List\n",
    "            list of variables that will be converted to datatpe \"<U16\"\n",
    "    \"\"\"\n",
    "    # Generate lists of variables from the external dicionaries that are actually present in the input dataset\n",
    "    existing_float64 = [\n",
    "        key for key in float64_variables if key in list(ds.keys())\n",
    "    ]\n",
    "    existing_U16 = [key for key in U16_variables if key in list(ds.keys())]\n",
    "\n",
    "    # Convert the datatypes of those variables, but only if those variables exist\n",
    "    if len(existing_float64) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        ds[existing_float64] = ds[existing_float64].astype(\"float64\")\n",
    "    \n",
    "    if len(existing_U16) == 0:\n",
    "        pass\n",
    "    else: \n",
    "        ds[existing_U16] = ds[existing_U16].astype(\"<U16\")\n",
    "\n",
    "    # And of the coordinates as well\n",
    "    ds.coords[\"station\"] = ds.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            # retrieve ERA IDs in this subset of stations\n",
    "            station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "            station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "            final_concat_list.append(station_1)\n",
    "            final_concat_list.append(station_2)\n",
    "\n",
    "            # import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_1\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                network_name, station_2\n",
    "            )\n",
    "\n",
    "            # TODO: open_zarr will be used for QAQC'd datasets\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            df_1 = ds_1.to_dataframe()\n",
    "            df_2 = ds_2.to_dataframe()\n",
    "\n",
    "            # apply reset index only to 'time', as we will need that for concatenation\n",
    "            df_1 = df_1.reset_index(level=\"time\")\n",
    "            df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "            # determine which dataset is older\n",
    "            if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "                # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                # we also grab the name of the newer station in this step, for use later\n",
    "                df_new = df_2\n",
    "                ds_new = ds_2\n",
    "\n",
    "                df_old = df_1\n",
    "                ds_old = ds_1\n",
    "            else:\n",
    "                df_new = df_1\n",
    "                ds_new = ds_1\n",
    "\n",
    "                df_old = df_2\n",
    "                ds_old = ds_2\n",
    "\n",
    "            # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "            df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "            # if there is no overlap between the two time series, just concatenate\n",
    "            if len(df_overlap) == 0:\n",
    "                df_concat = concat([df_old, df_new])\n",
    "\n",
    "            # if not, split into subsets and concatenate\n",
    "            else: \n",
    "                ##### Split datframes into subsets #####\n",
    "\n",
    "                # Remove data in time overlap between old and new\n",
    "                df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                ##### Concatenate subsets #####\n",
    "                df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "            ##### Now prepare the final concatenated dataframe for export\n",
    "            station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "            final_station_name = \"{}\".format(station_name_new)\n",
    "            new_index = [final_station_name] * len(df_concat)\n",
    "            df_concat.index = new_index\n",
    "            df_concat.index.name = \"station\"\n",
    "\n",
    "            # drop duplicate rows that were potentially generated in the concatenation process\n",
    "            df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "            # Add 'time' back into multi index\n",
    "            df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "            # Convert concatenated dataframe to dataset\n",
    "            ds_concat = df_concat.to_xarray()\n",
    "\n",
    "            #### Update attributes and datatypes #####\n",
    "\n",
    "            # Include past attributes\n",
    "            ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "            # Update 'history' attribute\n",
    "            timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "            ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "                \"history\"\n",
    "            ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "            # Update 'comment' attribute\n",
    "            ds_concat.attrs[\"comment\"] = (\n",
    "                \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "            )\n",
    "\n",
    "            # Add new qaqc_files_merged attribute\n",
    "            station_name_old = ds_old.coords[\"station\"].values[0]\n",
    "            ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "                    network_name, station_name_old, network_name, station_name_new\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert all datatypes, to enable export\n",
    "            ds_concat = convert_datatypes(ds_concat)\n",
    "\n",
    "            # ## Export ###\n",
    "            # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "            #     network_name, \"test_concat\", network_name # station_name_new\n",
    "            # )\n",
    "            # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return final_concat_list  \n",
    "    # return df_concat, df_new, df_old, df_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"VALLEYWATER\"\n",
    "final_concat_list = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "    network_name, station_1\n",
    ")\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "    network_name, station_2\n",
    ")\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1_original = ds_1.to_dataframe()\n",
    "df_2_original = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat_original = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the concatenated dataframe contain the input dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input DataFrame is a subset of concatenated DataFrame: False\n"
     ]
    }
   ],
   "source": [
    "# Check if input_df is a subset of concatenated_df\n",
    "is_subset = df_1_original.isin(df_concat_original).all().all()\n",
    "\n",
    "print(\"Input DataFrame is a subset of concatenated DataFrame:\", is_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1_original.reset_index(level=\"time\")\n",
    "df_2 = df_2_original.reset_index(level=\"time\")\n",
    "df_concat = df_concat_original.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input DataFrame is a subset of concatenated DataFrame: True\n"
     ]
    }
   ],
   "source": [
    "# Check if time range of new dataframe is in the concatenated dataframe\n",
    "is_subset = df_1['time'].isin(df_concat['time']).all().all()\n",
    "\n",
    "print(\"Input DataFrame is a subset of concatenated DataFrame:\", is_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_1_overlap = df_1[df_1[\"time\"].isin(df_concat[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_1[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_overlap[\"lat\"] = df_1_overlap[\"lat\"].round(3)\n",
    "df_1_overlap[\"lon\"] = df_1_overlap[\"lon\"].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44984/2480729328.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_concat_overlap[\"lat\"] = df_concat_overlap[\"lat\"].round(3)\n",
      "/tmp/ipykernel_44984/2480729328.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_concat_overlap[\"lon\"] = df_concat_overlap[\"lon\"].round(3)\n"
     ]
    }
   ],
   "source": [
    "df_concat_overlap[\"lat\"] = df_concat_overlap[\"lat\"].round(3)\n",
    "df_concat_overlap[\"lon\"] = df_concat_overlap[\"lon\"].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>elevation</th>\n",
       "      <th>elevation_eraqc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pr_15min</th>\n",
       "      <th>pr_15min_eraqc</th>\n",
       "      <th>raw_qc</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6144</th>\n",
       "      <td>1974-06-21 08:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6144</th>\n",
       "      <td>1974-06-21 08:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6144</th>\n",
       "      <td>1974-06-21 08:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6144</th>\n",
       "      <td>1974-06-21 09:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6144</th>\n",
       "      <td>1974-06-21 09:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                time  anemometer_height_m  elevation  \\\n",
       "station                                                                \n",
       "VALLEYWATER_6144 1974-06-21 08:15:00                  NaN        0.0   \n",
       "VALLEYWATER_6144 1974-06-21 08:30:00                  NaN        0.0   \n",
       "VALLEYWATER_6144 1974-06-21 08:45:00                  NaN        0.0   \n",
       "VALLEYWATER_6144 1974-06-21 09:00:00                  NaN        0.0   \n",
       "VALLEYWATER_6144 1974-06-21 09:15:00                  NaN        0.0   \n",
       "\n",
       "                  elevation_eraqc     lat      lon  pr_15min  pr_15min_eraqc  \\\n",
       "station                                                                        \n",
       "VALLEYWATER_6144              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6144              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6144              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6144              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6144              3.0  37.332 -122.081       0.0             NaN   \n",
       "\n",
       "                    raw_qc  thermometer_height_m  \n",
       "station                                           \n",
       "VALLEYWATER_6144  Approved                   NaN  \n",
       "VALLEYWATER_6144  Approved                   NaN  \n",
       "VALLEYWATER_6144  Approved                   NaN  \n",
       "VALLEYWATER_6144  Approved                   NaN  \n",
       "VALLEYWATER_6144  Approved                   NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat_overlap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>elevation</th>\n",
       "      <th>elevation_eraqc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pr_15min</th>\n",
       "      <th>pr_15min_eraqc</th>\n",
       "      <th>raw_qc</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6053</th>\n",
       "      <td>1974-06-21 08:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6053</th>\n",
       "      <td>1974-06-21 08:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6053</th>\n",
       "      <td>1974-06-21 08:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6053</th>\n",
       "      <td>1974-06-21 09:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALLEYWATER_6053</th>\n",
       "      <td>1974-06-21 09:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.332</td>\n",
       "      <td>-122.081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Approved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                time  anemometer_height_m  elevation  \\\n",
       "station                                                                \n",
       "VALLEYWATER_6053 1974-06-21 08:15:00                  NaN        0.0   \n",
       "VALLEYWATER_6053 1974-06-21 08:30:00                  NaN        0.0   \n",
       "VALLEYWATER_6053 1974-06-21 08:45:00                  NaN        0.0   \n",
       "VALLEYWATER_6053 1974-06-21 09:00:00                  NaN        0.0   \n",
       "VALLEYWATER_6053 1974-06-21 09:15:00                  NaN        0.0   \n",
       "\n",
       "                  elevation_eraqc     lat      lon  pr_15min  pr_15min_eraqc  \\\n",
       "station                                                                        \n",
       "VALLEYWATER_6053              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6053              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6053              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6053              3.0  37.332 -122.081       0.0             NaN   \n",
       "VALLEYWATER_6053              3.0  37.332 -122.081       0.0             NaN   \n",
       "\n",
       "                    raw_qc  thermometer_height_m  \n",
       "station                                           \n",
       "VALLEYWATER_6053  Approved                   NaN  \n",
       "VALLEYWATER_6053  Approved                   NaN  \n",
       "VALLEYWATER_6053  Approved                   NaN  \n",
       "VALLEYWATER_6053  Approved                   NaN  \n",
       "VALLEYWATER_6053  Approved                   NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1_overlap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_1[\"time\"], df_1[\"pr_15min\"])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_2[\"time\"], df_2[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[\"pr_15min\"])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"pr_15min\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_test(concat_list):\n",
    "    \"\"\"\n",
    "    Performs concatenation for stations in list of stations flagged for concatenation.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Import target datasets and convert to dataframe\n",
    "    flag_range = list(\n",
    "        range(concat_list[\"concat_flag\"].min(), concat_list[\"concat_flag\"].max())\n",
    "    )\n",
    "\n",
    "    for i in flag_range:\n",
    "        subset_list = concat_list[concat_list[\"concat_flag\"] == i]\n",
    "        subset_range = list(range(0, len(subset_list)))\n",
    "\n",
    "        url = {}\n",
    "        ds = {}\n",
    "        df = {}\n",
    "\n",
    "        for i in subset_range:\n",
    "\n",
    "            # extract information needed for dataset import\n",
    "            row_i = subset_list.iloc[[i]]\n",
    "            network_name = row_i[\"ERA-ID\"].split(\"_\")[\n",
    "                0\n",
    "            ]  # TODO: this does not work, when it really should\n",
    "            station_name = row_i[\"ERA-ID\"]\n",
    "\n",
    "            url[i] = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                network_name, network_name, station_name\n",
    "            )\n",
    "\n",
    "            ds[i] = xr.open_zarr(url[i])\n",
    "\n",
    "            df[i] = ds[i].to_dataframe()\n",
    "\n",
    "            # Apply reset index only to 'time', as we will need that for concatenation\n",
    "            df[i] = df[i].reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE SCRAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = asosawos_list_concat.groupby([\"ICAO\"]).apply(\n",
    "    lambda x: x.sort_values([\"end_time\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by end_time or end-date, depending on the station TODO: this is not necessary\n",
    "# time_var_list = ['end_time','end-date']\n",
    "# end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "# new_station_list = new_station_list.groupby('concat_flag').apply(lambda x: x.sort_values(end_time_or_date)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(df):\n",
    "    \"\"\"\n",
    "    Concatenates station data that has been flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.dataframe\n",
    "            staton data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "    # if there is overlap, then create subsets\n",
    "\n",
    "    # if no overlap, just concatenate\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_station_list(station_list, concat_list, duplicate_list):\n",
    "    \"\"\"\n",
    "    Reorders the input station list, necessary for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.dataframe\n",
    "\n",
    "        concat_list: pd.dataframe\n",
    "\n",
    "        duplicate_list: pd.dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output station list with stations to be concatenated at top, followed by potential duplicates\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    ##### subsets of station list\n",
    "\n",
    "    # stations that will be concatenated\n",
    "    concat_stations = station_list[station_list[\"ICAO\"].isin(concat_list)]\n",
    "\n",
    "    # potential duplicate stations\n",
    "    duplicate_stations = station_list[station_list[\"ICAO\"].isin(duplicate_list)]\n",
    "\n",
    "    # all remaining stations\n",
    "    remaining_stations = station_list[\n",
    "        ~station_list[\"ICAO\"].isin(duplicate_list + concat_list)\n",
    "    ]\n",
    "\n",
    "    ##### sort concat list alphabetically, to ensure that stations with the same ICAO are grouped together\n",
    "    concat_stations = concat_stations.sort_values(\"ICAO\")\n",
    "    duplicate_stations = duplicate_stations.sort_values(\"ICAO\")\n",
    "\n",
    "    ##### now within each ICAO, order by end time\n",
    "    concat_stations = concat_stations.groupby([\"ICAO\"]).apply(\n",
    "        lambda x: x.sort_values([\"end_time\"])\n",
    "    )\n",
    "\n",
    "    ##### concatenate susbets and reset index\n",
    "    new_list = concat(\n",
    "        [concat_stations, duplicate_stations, remaining_stations]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for presence of start and end times\n",
    "\n",
    "time_check = repeat_list_subset.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "print(\"number of null start times:\")\n",
    "print(time_check[\"start_time\"].sum())\n",
    "\n",
    "print(\"number of null end times:\")\n",
    "print(time_check[\"end_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the start and end times are identical\n",
    "\n",
    "start_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")\n",
    "end_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\"ICAO\"].tolist()\n",
    "start_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "    \"ICAO\"\n",
    "].tolist()\n",
    "\n",
    "print(end_list)\n",
    "print(start_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is going on with the stations that have duplicate start and end times? are they true duplicates?\n",
    "\n",
    "repeat_list_subset[repeat_list_subset[\"ICAO\"].isin(start_list + end_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in single dc file from AWS\n",
    "ds_1 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_72026294076\", temp_dir)\n",
    "ds_2 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_A0000594076\", temp_dir)\n",
    "\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "df_2 = qaqc_ds_to_df(ds_2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = df_1.lon.mean()\n",
    "lat = df_1.lat.mean()\n",
    "# print(\"{}, {:.5f}, {:.5f}\".format(id, lon, lat))\n",
    "\n",
    "\n",
    "# Plot time series of the data\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "df_1.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "df_2.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "\n",
    "ax.set_title(\"{}  ({:.3f}, {:.3f})\".format(id, lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_check_old(station_list):\n",
    "    \"\"\"\n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            list\n",
    "                list of ICAO values of stations that need to be concatenated\n",
    "            list\n",
    "                list of ICAO values of potential duplicate stations\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Generate list of repeat ICAOs\n",
    "    repeat_list = station_list[station_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "    repeat_list = repeat_list[\n",
    "        [\"ICAO\", \"ERA-ID\", \"STATION NAME\", \"start_time\", \"end_time\"]\n",
    "    ]\n",
    "\n",
    "    concat_list = repeat_list[\"ICAO\"].unique().tolist()\n",
    "\n",
    "    # And empty list to add potential duplicates to\n",
    "    duplicate_list = []\n",
    "\n",
    "    ##### Generate boolean for whether or not there are null start and/or end times\n",
    "    # TODO: may not be necessary\n",
    "    time_check = repeat_list.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "    end_nan_list = time_check[time_check[\"end_time\"] == True][\"ICAO\"].tolist()\n",
    "    start_nan_list = time_check[time_check[\"start_time\"] == True][\"ICAO\"].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_nan_list + end_nan_list\n",
    "\n",
    "    duplicate_list = duplicate_list\n",
    "\n",
    "    ##### Identify ICAOs with duplicate start end times\n",
    "    start_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    end_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    end_dup_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "    start_dup_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_dup_list + end_dup_list\n",
    "\n",
    "    # Generate final list of ICAOs for stations to be concatenated\n",
    "    concat_list = [x for x in concat_list if x not in duplicate_list]\n",
    "\n",
    "    return concat_list, duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the subset with only stations to concatenate\n",
    "\n",
    "asosawos_list_concat[\"ICAO\"] = pd.Categorical(\n",
    "    asosawos_list_concat[\"ICAO\"], categories=concat_list, ordered=True\n",
    ")\n",
    "\n",
    "test_list = asosawos_list_concat.sort_values(\"ICAO\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations within a certain distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into GeoDataFrames\n",
    "# using EPSG 3310\n",
    "\n",
    "gdf_asosawos = gpd.GeoDataFrame(\n",
    "    asosawos_list,\n",
    "    geometry=[\n",
    "        Point(lon, lat) for lon, lat in zip(asosawos_list[\"LON\"], asosawos_list[\"LAT\"])\n",
    "    ],\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 3: find the nearest point in the geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert emtpy columns\n",
    "\n",
    "gdf_asosawos[\"nearest_station\"] = pd.Series(dtype=\"U16\")\n",
    "gdf_asosawos[\"distance\"] = pd.Series(dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_asosawos.iterrows():\n",
    "    # geometry of individual row \n",
    "    point = row.geometry\n",
    "    # returns a multipoint object with the geometries of every row in the gdf\n",
    "    multipoint = gdf_asosawos.drop(index, axis=0).geometry.unary_union\n",
    "    # \n",
    "    queried_geom, nearest_geom = nearest_points(point, multipoint)\n",
    "    dist_from_point = \n",
    "    gdf_asosawos.loc[index, 'nearest_geometry'] = nearest_geom\n",
    "    gdf_asosawos.loc[index, 'distance'] = nearest_geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 2: distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the distance between points\n",
    "\n",
    "\n",
    "def distance_sort_filter(row, df2, buffer=None, id=False):\n",
    "\n",
    "    dist = df2.geometry.distance(row).sort_values()\n",
    "\n",
    "    if buffer:\n",
    "        dist = dist[dist < buffer]\n",
    "\n",
    "    if id:\n",
    "        distances = {\n",
    "            df2.loc[idx][\"WBAN\"]: value for idx, value in zip(dist.index, dist.values)\n",
    "        }\n",
    "    else:\n",
    "        distances = {idx: value for idx, value in zip(dist.index, dist.values)}\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 1: using sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a buffer around points in gdf1 (e.g., 10 km buffer)\n",
    "gdf_asosawos[\"buffer\"] = gdf_asosawos.geometry.buffer(\n",
    "    0.1\n",
    ")  # Buffer in degrees, 0.1 degrees approx equals 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join using the buffer\n",
    "merged = gpd.sjoin(\n",
    "    gdf_asosawos, gdf_asosawos[[\"geometry\", \"buffer\"]], how=\"inner\", predicate=\"within\"\n",
    ")\n",
    "\n",
    "# The 'merged' GeoDataFrame contains points from gdf_isd that are within the buffer around points in gdf_asosawos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    merged\n",
    ")  # there are not ISD stations within 10km of an ASOSAWOS station missed by the exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Round asosawos down to 3 decimal points of accuracy\n",
    "# asosawos_round = asosawos_list.round({\"LAT\": 3, \"LON\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential ways to check that two stations are duplicates\n",
    "1. identical total_nobs\n",
    "2. identical ERA IDs\n",
    "3. identical end or start times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract flagged stations\n",
    "\n",
    "asosawos_dup = asosawos_out[~asosawos_out[\"concat_flag\"].isna()]\n",
    "valleywater_dup = valleywater_out[~valleywater_out[\"concat_flag\"].isna()]\n",
    "maritime_dup = maritime_out[~maritime_out[\"concat_flag\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that are potentially duplicates\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Within stations flagged for concatenation, stations are flagged as potential duplicates\n",
    "            if either their start or end times are identical\n",
    "            - TODO: brainstorm alternative approaches\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information that has passed through the concatenation check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    ##### flag stations with repeat end or start times\n",
    "\n",
    "    time_end_list = [\"end_time\", \"end-date\"]\n",
    "    time_start_list = [\"start_time\", \"start-date\"]\n",
    "\n",
    "    end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "\n",
    "    new_station_list = (\n",
    "        new_station_list.groupby(\"concat_flag\")\n",
    "        .apply(lambda x: x.sort_values(end_time_or_date))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return new_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(network_name, station_old, station_new):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Import target datasets and convert to dataframe\n",
    "    old_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "    new_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "\n",
    "    ds_old = xr.open_zarr(old_url)\n",
    "    ds_new = xr.open_zarr(new_url)\n",
    "\n",
    "    df_old = ds_old.to_dataframe()\n",
    "    df_new = ds_new.to_dataframe()\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset. \n",
    "    Also returns a list of the ERA-IDs of all stations that are concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success: \n",
    "            return list of ERA-IDs are stations that are concatenated\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}/concat_list_{}.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    # ! you can truncate the concat list here, for testing\n",
    "    concat_list = concat_list.head(2)\n",
    "    # ! end\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique())\n",
    "\n",
    "    # initiate empty list, to which we will iteratively add the ERA-IDs of stations that are concatenated\n",
    "    final_concat_list = []\n",
    "\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            try: \n",
    "                # retrieve ERA IDs in this subset of stations\n",
    "                station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "                station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "                # import this subset of datasets and convert to dataframe\n",
    "                url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_1\n",
    "                )\n",
    "                url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(\n",
    "                    network_name, station_2\n",
    "                )\n",
    "\n",
    "                ds_1 = xr.open_zarr(url_1)\n",
    "                ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "                df_1,MultiIndex_1,attrs_1,var_attrs_1,era_qc_vars_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "                df_2, MultiIndex_2, attrs_2, var_attrs_2, era_qc_vars_2 = qaqc_ds_to_df(ds_2, verbose=False)\n",
    "\n",
    "                # determine which dataset is older\n",
    "                if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "                    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                    # we also grab the name of the newer station in this step, for use later\n",
    "                    df_new = df_2\n",
    "                    ds_new = ds_2\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_1\n",
    "                    ds_old = ds_1\n",
    "                    MultiIndex_old = MultiIndex_1\n",
    "\n",
    "                else:\n",
    "                    df_new = df_1\n",
    "                    ds_new = df_1\n",
    "                    MultiIndex_new = MultiIndex_2\n",
    "                    attrs_new = attrs_2\n",
    "\n",
    "                    df_old = df_2\n",
    "                    ds_old = ds_2\n",
    "                    MultiIndex_old = MultiIndex_2\n",
    "\n",
    "                # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "                df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "                # if there is no overlap between the two time series, just concatenate\n",
    "                if len(df_overlap) == 0:\n",
    "                    df_concat = concat([df_old, df_new])\n",
    "\n",
    "                # if not, split into subsets and concatenate\n",
    "                else:\n",
    "                    ##### Split datframes into subsets #####\n",
    "\n",
    "                    # Remove data in time overlap between old and new\n",
    "                    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "                    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "                    ##### Concatenate subsets #####\n",
    "                    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "                # ##### Now prepare the final concatenated dataframe for export\n",
    "                station_name_new = MultiIndex_new.get_level_values(\"station\")[1]\n",
    "                \n",
    "                # ! This is where Neil and I made the change to address the issues\n",
    "                # ! \n",
    "                MultiIndex_old = pd.MultiIndex.from_tuples(\n",
    "                    [(station_name_new, lvl1) for _, lvl1 in MultiIndex_old],\n",
    "                    names=MultiIndex_new.names,\n",
    "                )\n",
    "\n",
    "                MultiIndex_concat = MultiIndex_new.union(MultiIndex_old)\n",
    "\n",
    "                # drop duplicate rows that were potentially generated in the concatenation process\n",
    "                df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "                # drop 'station' and 'time'columns\n",
    "                df_concat = df_concat.drop([\"station\", \"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"], axis=1)\n",
    "\n",
    "                print('length of MultiIndex_new')\n",
    "                print(len(MultiIndex_new))\n",
    "                print(\"length of MultiIndex_old\")\n",
    "                print(len(MultiIndex_old))\n",
    "                print(\"length of MultiIndex_concat\")\n",
    "                print(len(MultiIndex_concat))\n",
    "\n",
    "                print(\"length of df_new\")\n",
    "                print(len(df_new))\n",
    "                print(\"length of df_old\")\n",
    "                print(len(df_old))\n",
    "                print(\"length of df_concat\")\n",
    "                print(len(df_concat))\n",
    "\n",
    "                # ! This is where the issue! MultiIndex_concat and df_concat have difference lengths\n",
    "                df_concat.index = MultiIndex_concat\n",
    "\n",
    "                # # Convert concatenated dataframe to dataset\n",
    "                # ds_concat = df_concat.to_xarray()\n",
    "\n",
    "                # # #### Prepare for export #####\n",
    "\n",
    "                # # Convert datatype of station coordinate\n",
    "                # ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U20\")\n",
    "\n",
    "                # # # Include past attributes\n",
    "                # ds_concat.attrs.update(attrs_new)\n",
    "\n",
    "                # # Update 'history' attribute\n",
    "                # timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "                # ds_concat.attrs[\"history\"] = ds_concat.attrs[\n",
    "                #     \"history\"\n",
    "                # ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "                # # Update 'comment' attribute\n",
    "                # ds_concat.attrs[\"comment\"] = (\n",
    "                #     \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "                # )\n",
    "\n",
    "                # # Add new qaqc_files_merged attribute\n",
    "                # station_name_old = MultiIndex_old.get_level_values(\"station\")[1]\n",
    "                # ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                #     \"{}, {} merged. Overlap retained from newer station data.\".format(\n",
    "                #         station_name_old, station_name_new\n",
    "                #     )\n",
    "                # )\n",
    "\n",
    "                # ! this is here the renaming will go\n",
    "\n",
    "                # !\n",
    "\n",
    "                # ## Export ###\n",
    "                # ! a test name is used below\n",
    "                # ! the final name will be that of the newer dataframe\n",
    "                # export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                #     network_name, \"test_concat\", station_name_new\n",
    "                # )\n",
    "                # ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "                # record that the stations were concatenated\n",
    "                final_concat_list.append(station_1)\n",
    "                final_concat_list.append(station_2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"Error concatenating subset {}: {}\".format(subset_i, e)\n",
    "                )\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # return final_concat_list # ! this will be the final return statement, below is inlcluded for testing\n",
    "    # return (\n",
    "    #     df_new,\n",
    "    #     df_old,\n",
    "    #     df_concat,\n",
    "    #     ds_concat,\n",
    "    #     final_concat_list,\n",
    "    # )\n",
    "\n",
    "    return df_1, df_2, MultiIndex_1, MultiIndex_2, df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = \"MARITIME\" # \"VALLEYWATER\", \"MARITIME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of MultiIndex_new\n",
      "135209\n",
      "length of MultiIndex_old\n",
      "135209\n",
      "length of MultiIndex_concat\n",
      "135209\n",
      "length of df_new\n",
      "1409901\n",
      "length of df_old\n",
      "135209\n",
      "length of df_concat\n",
      "1509221\n",
      "Error concatenating subset            ERA-ID concat_subset\n",
      "0  MARITIME_LJAC1    MARITIME_0\n",
      "1  MARITIME_LJPC1    MARITIME_0: Length mismatch: Expected axis has 1509221 elements, new values have 135209 elements\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m df_1, df_2, MultiIndex_1, MultiIndex_2 \u001b[38;5;241m=\u001b[39m concatenate_station_pairs(network_name)\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "df_1, df_2, MultiIndex_1, MultiIndex_2 = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-04-01 02:00:00\n",
      "2022-08-31 23:54:00\n"
     ]
    }
   ],
   "source": [
    "# LJAC1 - this should be new\n",
    "print(df_1['time'].min())\n",
    "print(df_1[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 01:30:00\n",
      "2022-08-31 23:20:00\n"
     ]
    }
   ],
   "source": [
    "# LJPC1\n",
    "print(df_2[\"time\"].min())\n",
    "print(df_2[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which dataset is older\n",
    "if df_2[\"time\"].max() > df_1[\"time\"].max():\n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    MultiIndex_new = MultiIndex_2\n",
    "\n",
    "    df_old = df_1\n",
    "    MultiIndex_old = MultiIndex_1\n",
    "\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = df_1\n",
    "    MultiIndex_new = MultiIndex_1\n",
    "\n",
    "    df_old = df_2\n",
    "    MultiIndex_old = MultiIndex_2\n",
    "\n",
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is no overlap between the two time series, just concatenate\n",
    "if len(df_overlap) == 0:\n",
    "    df_concat = concat([df_old, df_new])\n",
    "\n",
    "# if not, split into subsets and concatenate\n",
    "else:\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_overlap[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_overlap[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Now prepare the final concatenated dataframe for export\n",
    "station_name_new = MultiIndex_new.get_level_values(\"station\")[1]\n",
    "\n",
    "MultiIndex_old = pd.MultiIndex.from_tuples(\n",
    "    [(station_name_new, lvl1) for _, lvl1 in MultiIndex_old],\n",
    "    names=MultiIndex_new.names,\n",
    ")\n",
    "\n",
    "MultiIndex_concat = MultiIndex_new.union(MultiIndex_old)\n",
    "\n",
    "\n",
    "# MultiIndex_concat = pd.MultiIndex.from_tuples(\n",
    "#     [(station_name_new, lvl1) for _, lvl1 in MultiIndex_concat],\n",
    "#     names=MultiIndex_concat.names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows that were potentially generated in the concatenation process\n",
    "df_concat = df_concat.drop_duplicates(subset=[\"time\"])\n",
    "\n",
    "# drop 'station' and 'time'columns\n",
    "df_concat = df_concat.drop([\"station\", \"time\",\"hour\",\"day\",\"month\",\"year\",\"date\"], axis=1)\n",
    "\n",
    "df_concat.index = MultiIndex_concat\n",
    "\n",
    "# Convert concatenated dataframe to dataset\n",
    "ds_concat = df_concat.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:               (station: 1, time: 1509221)\n",
       "Coordinates:\n",
       "  * station               (station) object &#x27;MARITIME_LJAC1&#x27;\n",
       "  * time                  (time) datetime64[ns] 2005-01-01T01:30:00 ... 2022-...\n",
       "Data variables: (12/14)\n",
       "    anemometer_height_m   (station, time) float64 20.2 20.2 20.2 ... 8.2 8.2 8.2\n",
       "    elevation             (station, time) float64 0.0 0.0 0.0 ... 9.3 9.3 9.3\n",
       "    elevation_eraqc       (station, time) float64 nan nan nan ... nan nan nan\n",
       "    lat                   (station, time) float64 32.87 32.87 ... 32.87 32.87\n",
       "    lon                   (station, time) float64 -117.3 -117.3 ... -117.3\n",
       "    sfcWind               (station, time) float64 2.0 2.0 2.0 ... 2.7 2.9 3.1\n",
       "    ...                    ...\n",
       "    sfcWind_eraqc         (station, time) float64 nan nan nan ... nan nan nan\n",
       "    tas                   (station, time) float64 286.3 286.4 ... 296.3 296.4\n",
       "    tas_eraqc             (station, time) float64 nan nan nan ... nan nan nan\n",
       "    thermometer_height_m  (station, time) float64 6.1 6.1 6.1 ... 7.2 7.2 7.2\n",
       "    ps                    (station, time) float64 nan nan ... 1.008e+05\n",
       "    ps_eraqc              (station, time) float64 nan nan nan ... nan nan nan</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-6ff2a933-78f9-4abf-ad95-49e83f20f0fd' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-6ff2a933-78f9-4abf-ad95-49e83f20f0fd' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>station</span>: 1</li><li><span class='xr-has-index'>time</span>: 1509221</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-2ea46e43-85b4-41ed-92cc-a3dcb6aca88d' class='xr-section-summary-in' type='checkbox'  checked><label for='section-2ea46e43-85b4-41ed-92cc-a3dcb6aca88d' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>station</span></div><div class='xr-var-dims'>(station)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;MARITIME_LJAC1&#x27;</div><input id='attrs-0793d75b-2d40-4497-b910-cf8152b599bc' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-0793d75b-2d40-4497-b910-cf8152b599bc' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-edbb207f-6491-4152-aa8d-b3caffe63236' class='xr-var-data-in' type='checkbox'><label for='data-edbb207f-6491-4152-aa8d-b3caffe63236' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;MARITIME_LJAC1&#x27;], dtype=object)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2005-01-01T01:30:00 ... 2022-08-...</div><input id='attrs-20828731-6de7-4a35-8caf-c7d92274b326' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-20828731-6de7-4a35-8caf-c7d92274b326' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-078f78ba-d37f-4527-b839-90732bfbdda5' class='xr-var-data-in' type='checkbox'><label for='data-078f78ba-d37f-4527-b839-90732bfbdda5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2005-01-01T01:30:00.000000000&#x27;, &#x27;2005-01-01T02:30:00.000000000&#x27;,\n",
       "       &#x27;2005-01-01T03:30:00.000000000&#x27;, ..., &#x27;2022-08-31T23:42:00.000000000&#x27;,\n",
       "       &#x27;2022-08-31T23:48:00.000000000&#x27;, &#x27;2022-08-31T23:54:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-57b167cb-b220-4427-9147-1e1e590d2a2b' class='xr-section-summary-in' type='checkbox'  checked><label for='section-57b167cb-b220-4427-9147-1e1e590d2a2b' class='xr-section-summary' >Data variables: <span>(14)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>anemometer_height_m</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>20.2 20.2 20.2 20.2 ... 8.2 8.2 8.2</div><input id='attrs-2086609a-6a43-4c9d-91fe-c73cb5c16bdb' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2086609a-6a43-4c9d-91fe-c73cb5c16bdb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5ee6faf9-bcd6-4bcb-ab0c-7e2e38e3d560' class='xr-var-data-in' type='checkbox'><label for='data-5ee6faf9-bcd6-4bcb-ab0c-7e2e38e3d560' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[20.2, 20.2, 20.2, ...,  8.2,  8.2,  8.2]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>elevation</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0 0.0 0.0 0.0 ... 9.3 9.3 9.3 9.3</div><input id='attrs-d155e54b-170b-4f2b-9a57-a18c9601b9ef' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-d155e54b-170b-4f2b-9a57-a18c9601b9ef' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-46f5e834-aeb5-47ac-806d-f2cd32f96b6e' class='xr-var-data-in' type='checkbox'><label for='data-46f5e834-aeb5-47ac-806d-f2cd32f96b6e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[0. , 0. , 0. , ..., 9.3, 9.3, 9.3]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>elevation_eraqc</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-f1f38619-adcd-453f-95b6-8d8f7d925ae3' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f1f38619-adcd-453f-95b6-8d8f7d925ae3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6cd767ba-4c1a-4f8b-87dc-1daf21bcf0b2' class='xr-var-data-in' type='checkbox'><label for='data-6cd767ba-4c1a-4f8b-87dc-1daf21bcf0b2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[nan, nan, nan, ..., nan, nan, nan]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>lat</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>32.87 32.87 32.87 ... 32.87 32.87</div><input id='attrs-9242f33f-579d-4296-91c2-93d273dbe8f5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9242f33f-579d-4296-91c2-93d273dbe8f5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a90ec21a-f312-488c-8e2a-1562a7fdbac5' class='xr-var-data-in' type='checkbox'><label for='data-a90ec21a-f312-488c-8e2a-1562a7fdbac5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[32.867, 32.867, 32.867, ..., 32.867, 32.867, 32.867]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>lon</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-117.3 -117.3 ... -117.3 -117.3</div><input id='attrs-d202da07-aab3-43a8-8ecd-b699226fff25' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-d202da07-aab3-43a8-8ecd-b699226fff25' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-26b03f22-cba0-44bd-a8a2-05ff7c300d3e' class='xr-var-data-in' type='checkbox'><label for='data-26b03f22-cba0-44bd-a8a2-05ff7c300d3e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-117.257, -117.257, -117.257, ..., -117.257, -117.257, -117.257]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>sfcWind</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>2.0 2.0 2.0 3.0 ... 2.9 2.7 2.9 3.1</div><input id='attrs-f1d5c3f1-7fa4-49a6-9f84-d465fedbf6f7' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f1d5c3f1-7fa4-49a6-9f84-d465fedbf6f7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2b6ecf4e-03f6-43c8-aceb-ce28c5691455' class='xr-var-data-in' type='checkbox'><label for='data-2b6ecf4e-03f6-43c8-aceb-ce28c5691455' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[2. , 2. , 2. , ..., 2.7, 2.9, 3.1]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>sfcWind_dir</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>280.0 310.0 310.0 ... 23.0 24.0</div><input id='attrs-111c09bb-b9dd-4145-aef2-c7582b9ac0b4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-111c09bb-b9dd-4145-aef2-c7582b9ac0b4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-74248b56-4997-4f71-a71f-ac9a5acc5c32' class='xr-var-data-in' type='checkbox'><label for='data-74248b56-4997-4f71-a71f-ac9a5acc5c32' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[280., 310., 310., ...,  17.,  23.,  24.]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>sfcWind_dir_eraqc</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-69b5da10-3dad-4859-98b2-bb77336fa658' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-69b5da10-3dad-4859-98b2-bb77336fa658' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-709da78b-04c3-423e-a9cd-881ee95606b7' class='xr-var-data-in' type='checkbox'><label for='data-709da78b-04c3-423e-a9cd-881ee95606b7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[nan, nan, nan, ..., nan, nan, nan]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>sfcWind_eraqc</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-3aa8eb8b-b501-4e38-ada1-d1457c8cdb61' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-3aa8eb8b-b501-4e38-ada1-d1457c8cdb61' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a196da83-7e26-4382-8605-40155eade267' class='xr-var-data-in' type='checkbox'><label for='data-a196da83-7e26-4382-8605-40155eade267' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[nan, nan, nan, ..., nan, nan, nan]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>tas</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>286.3 286.4 286.5 ... 296.3 296.4</div><input id='attrs-247bdb6e-1fb5-4014-81cb-0096a97ff896' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-247bdb6e-1fb5-4014-81cb-0096a97ff896' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cd74f786-7eea-4211-8870-bcd6cc5badb0' class='xr-var-data-in' type='checkbox'><label for='data-cd74f786-7eea-4211-8870-bcd6cc5badb0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[286.35, 286.45, 286.55, ..., 296.35, 296.35, 296.45]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>tas_eraqc</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-58a83c20-ee8a-43ef-aac3-d5e0dc744e30' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-58a83c20-ee8a-43ef-aac3-d5e0dc744e30' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3c6f6613-8db2-4a6b-83f6-c3fdd9e2ecaa' class='xr-var-data-in' type='checkbox'><label for='data-3c6f6613-8db2-4a6b-83f6-c3fdd9e2ecaa' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[nan, nan, nan, ..., nan, nan, nan]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>thermometer_height_m</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>6.1 6.1 6.1 6.1 ... 7.2 7.2 7.2 7.2</div><input id='attrs-8eb82f8f-6872-4930-bc8f-87c8088ac931' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8eb82f8f-6872-4930-bc8f-87c8088ac931' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a1e73dfc-7d31-49cd-972a-9d70745ac039' class='xr-var-data-in' type='checkbox'><label for='data-a1e73dfc-7d31-49cd-972a-9d70745ac039' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[6.1, 6.1, 6.1, ..., 7.2, 7.2, 7.2]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ps</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan ... 1.008e+05 1.008e+05</div><input id='attrs-dbc680a2-5f5f-42ba-a2d8-0816cc5b3b4c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-dbc680a2-5f5f-42ba-a2d8-0816cc5b3b4c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0b5bccf3-6af7-4aaa-99bc-10a0828e7d52' class='xr-var-data-in' type='checkbox'><label for='data-0b5bccf3-6af7-4aaa-99bc-10a0828e7d52' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[    nan,     nan,     nan, ..., 100800., 100800., 100800.]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>ps_eraqc</span></div><div class='xr-var-dims'>(station, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>nan nan nan nan ... nan nan nan nan</div><input id='attrs-6509941d-b5ad-44b4-8e54-0c610d1764de' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-6509941d-b5ad-44b4-8e54-0c610d1764de' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-508930d7-ce8b-46fd-86eb-ad4e41a2621e' class='xr-var-data-in' type='checkbox'><label for='data-508930d7-ce8b-46fd-86eb-ad4e41a2621e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[nan, nan, nan, ..., nan, nan, nan]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8132017f-5965-4d82-86d7-0003bf794ab2' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-8132017f-5965-4d82-86d7-0003bf794ab2' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:               (station: 1, time: 1509221)\n",
       "Coordinates:\n",
       "  * station               (station) object 'MARITIME_LJAC1'\n",
       "  * time                  (time) datetime64[ns] 2005-01-01T01:30:00 ... 2022-...\n",
       "Data variables: (12/14)\n",
       "    anemometer_height_m   (station, time) float64 20.2 20.2 20.2 ... 8.2 8.2 8.2\n",
       "    elevation             (station, time) float64 0.0 0.0 0.0 ... 9.3 9.3 9.3\n",
       "    elevation_eraqc       (station, time) float64 nan nan nan ... nan nan nan\n",
       "    lat                   (station, time) float64 32.87 32.87 ... 32.87 32.87\n",
       "    lon                   (station, time) float64 -117.3 -117.3 ... -117.3\n",
       "    sfcWind               (station, time) float64 2.0 2.0 2.0 ... 2.7 2.9 3.1\n",
       "    ...                    ...\n",
       "    sfcWind_eraqc         (station, time) float64 nan nan nan ... nan nan nan\n",
       "    tas                   (station, time) float64 286.3 286.4 ... 296.3 296.4\n",
       "    tas_eraqc             (station, time) float64 nan nan nan ... nan nan nan\n",
       "    thermometer_height_m  (station, time) float64 6.1 6.1 6.1 ... 7.2 7.2 7.2\n",
       "    ps                    (station, time) float64 nan nan ... 1.008e+05\n",
       "    ps_eraqc              (station, time) float64 nan nan nan ... nan nan nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Union is the issue - mismatch in timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-04-01 02:00:00\n",
      "2022-08-31 23:54:00\n"
     ]
    }
   ],
   "source": [
    "print(df_1['time'].min())\n",
    "print(df_1[\"time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 01:30:00\n",
      "2022-08-31 23:20:00\n"
     ]
    }
   ],
   "source": [
    "print(df_2[\"time\"].min())\n",
    "print(df_2[\"time\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_concat should span 2005-01-01 01:30:00 - 2022-08-31 23:54:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(MultiIndex_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'anemometer_height_m', 'elevation', 'elevation_eraqc', 'lat',\n",
       "       'lon', 'sfcWind', 'sfcWind_dir', 'sfcWind_dir_eraqc', 'sfcWind_eraqc',\n",
       "       'tas', 'tas_eraqc', 'thermometer_height_m', 'station', 'hour', 'day',\n",
       "       'month', 'year', 'date', 'ps', 'ps_eraqc'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows that were potentially generated in the concatenation process\n",
    "df_concat_drop_dups = df_concat.drop_duplicates(subset=[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(df_concat_drop_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiIndex_concat and df_concat_drop_dups['time']\n",
    "index_time = list(MultiIndex_1.get_level_values(\"time\"))\n",
    "#df_time = list(df_concat_drop_dups['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-04-01 02:00:00', '2005-04-01 03:00:00',\n",
       "               '2005-04-01 04:00:00', '2005-04-01 05:00:00',\n",
       "               '2005-04-01 06:00:00', '2005-04-01 07:00:00',\n",
       "               '2005-04-01 08:00:00', '2005-04-01 09:00:00',\n",
       "               '2005-04-01 10:00:00', '2005-04-01 11:00:00',\n",
       "               ...\n",
       "               '2022-08-31 23:00:00', '2022-08-31 23:06:00',\n",
       "               '2022-08-31 23:12:00', '2022-08-31 23:18:00',\n",
       "               '2022-08-31 23:24:00', '2022-08-31 23:30:00',\n",
       "               '2022-08-31 23:36:00', '2022-08-31 23:42:00',\n",
       "               '2022-08-31 23:48:00', '2022-08-31 23:54:00'],\n",
       "              dtype='datetime64[ns]', name='time', length=1409901, freq=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultiIndex_1.get_level_values(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2005-04-01 02:00:00\n",
       "1         2005-04-01 03:00:00\n",
       "2         2005-04-01 04:00:00\n",
       "3         2005-04-01 05:00:00\n",
       "4         2005-04-01 06:00:00\n",
       "                  ...        \n",
       "1409896   2022-08-31 23:30:00\n",
       "1409897   2022-08-31 23:36:00\n",
       "1409898   2022-08-31 23:42:00\n",
       "1409899   2022-08-31 23:48:00\n",
       "1409900   2022-08-31 23:54:00\n",
       "Name: time, Length: 1409901, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_new['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2005-01-01 01:30:00\n",
       "1         2005-01-01 02:30:00\n",
       "2         2005-01-01 03:30:00\n",
       "3         2005-01-01 04:30:00\n",
       "4         2005-01-01 05:30:00\n",
       "                  ...        \n",
       "1409896   2022-08-31 23:30:00\n",
       "1409897   2022-08-31 23:36:00\n",
       "1409898   2022-08-31 23:42:00\n",
       "1409899   2022-08-31 23:48:00\n",
       "1409900   2022-08-31 23:54:00\n",
       "Name: time, Length: 1509221, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_concat['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df_concat[df_concat['time'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>anemometer_height_m</th>\n",
       "      <th>elevation</th>\n",
       "      <th>elevation_eraqc</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>sfcWind</th>\n",
       "      <th>sfcWind_dir</th>\n",
       "      <th>sfcWind_dir_eraqc</th>\n",
       "      <th>sfcWind_eraqc</th>\n",
       "      <th>...</th>\n",
       "      <th>tas_eraqc</th>\n",
       "      <th>thermometer_height_m</th>\n",
       "      <th>station</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>ps</th>\n",
       "      <th>ps_eraqc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [time, anemometer_height_m, elevation, elevation_eraqc, lat, lon, sfcWind, sfcWind_dir, sfcWind_dir_eraqc, sfcWind_eraqc, tas, tas_eraqc, thermometer_height_m, station, hour, day, month, year, date, ps, ps_eraqc]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'station' and 'time'columns\n",
    "df_concat = df_concat.drop(\n",
    "    [\"station\", \"time\", \"hour\", \"day\", \"month\", \"year\", \"date\"], axis=1\n",
    ")\n",
    "\n",
    "df_concat.index = MultiIndex_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 1\n",
    "\n",
    "Run concatenate_station_pairs() as is, so the function does not export and instead returns df_concat, df_new, df_old, and df_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error concatenation stations of subset            ERA-ID concat_subset\n",
      "0  MARITIME_LJAC1    MARITIME_0\n",
      "1  MARITIME_LJPC1    MARITIME_0: Length mismatch: Expected axis has 1509221 elements, new values have 135209 elements\n",
      "Error concatenation stations of subset            ERA-ID concat_subset\n",
      "2  MARITIME_ICAC1    MARITIME_1\n",
      "3  MARITIME_SMOC1    MARITIME_1: Length mismatch: Expected axis has 1367110 elements, new values have 282368 elements\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ds_concat' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[130], line 7\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m (\n",
      "\u001b[1;32m      2\u001b[0m     df_new,\n",
      "\u001b[1;32m      3\u001b[0m     df_old,\n",
      "\u001b[1;32m      4\u001b[0m     df_concat,\n",
      "\u001b[1;32m      5\u001b[0m     ds_concat,\n",
      "\u001b[1;32m      6\u001b[0m     final_concat_list,\n",
      "\u001b[0;32m----> 7\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_station_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[128], line 181\u001b[0m, in \u001b[0;36mconcatenate_station_pairs\u001b[0;34m(network_name)\u001b[0m\n",
      "\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# return final_concat_list # ! this will be the final return statement, below is inlcluded for testing\u001b[39;00m\n",
      "\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n",
      "\u001b[1;32m    178\u001b[0m     df_new,\n",
      "\u001b[1;32m    179\u001b[0m     df_old,\n",
      "\u001b[1;32m    180\u001b[0m     df_concat,\n",
      "\u001b[0;32m--> 181\u001b[0m     \u001b[43mds_concat\u001b[49m,\n",
      "\u001b[1;32m    182\u001b[0m     final_concat_list,\n",
      "\u001b[1;32m    183\u001b[0m )\n",
      "\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ds_concat' referenced before assignment"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_new,\n",
    "    df_old,\n",
    "    df_concat,\n",
    "    ds_concat,\n",
    "    final_concat_list,\n",
    ") = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test option 2: \n",
    "\n",
    "Run concatenate_station_pairs() with the first return statement uncommented and the second commented, and the export section uncommented. So that the function actually exports the concatenated datasets. I've generated all the concatention lists (for VALLEYWATER, MARITIME, and ASOSAWOS) needed to run the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = concatenate_station_pairs(network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output\n",
    "# TODO: you'll need to change the url\n",
    "url_output = \"s3://wecc-historical-wx/3_qaqc_wx/{}/test_concat_{}.zarr\".format(\n",
    "    network_name, network_name\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_concat = xr.open_zarr(url_output)\n",
    "\n",
    "df_concat = ds_concat.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=\"3_qaqc_wx/{}/{}_concat_list_{}.csv\".format(\n",
    "        network_name, network_name, network_name\n",
    "    ),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_1)\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}.zarr\".format(network_name, station_2)\n",
    "\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1 = ds_1.to_dataframe()\n",
    "df_2 = ds_2.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time index for plotting\n",
    "df_1 = df_1.reset_index(level=\"time\")\n",
    "df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "\n",
    "df_concat = df_concat.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    ds_new = ds_2\n",
    "\n",
    "    df_old = df_1\n",
    "    ds_old = ds_1\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = ds_1\n",
    "\n",
    "    df_old = df_2\n",
    "    ds_old = ds_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_new_overlap = df_new[df_new[\"time\"].isin(df_concat[\"time\"])]\n",
    "df_concat_overlap = df_concat[df_concat[\"time\"].isin(df_new[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_overlap.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat_overlap.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the two original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_var = 'ps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_new[\"time\"], df_new[vis_var])\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_old[\"time\"], df_old[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"input dfs\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plotting the time series of given dataframe\n",
    "plt.plot(df_concat[\"time\"], df_concat[vis_var])\n",
    "\n",
    "# Giving title to the chart using plt.title\n",
    "plt.title(\"concatenated df\")\n",
    "\n",
    "# rotating the x-axis tick labels at 30degree\n",
    "# towards right\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "\n",
    "# Providing x and y label to the chart\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(vis_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
