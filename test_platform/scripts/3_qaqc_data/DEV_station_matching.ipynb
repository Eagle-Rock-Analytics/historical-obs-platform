{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station Matching\n",
    "\n",
    "The goal of this notebook is to identify stations that changed IDs. This has been known to occur for Maritime and ASOSOAWOS stations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from pandas import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "\n",
    "# Silence warnings\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=ShapelyDeprecationWarning\n",
    ")  # Warning is raised when creating Point object from coords. Can't figure out why.\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials\n",
    "#s3 = s3fs.S3FileSystem  # must be set to this to use such commands as ls\n",
    "# s3 = boto3.resource('s3')\n",
    "# s3_client = boto3.client(\"s3\")\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")\n",
    "\n",
    "## AWS buckets\n",
    "bucket = \"wecc-historical-wx\"\n",
    "qaqcdir = \"3_qaqc_wx/\"\n",
    "mergedir = \"4_merge_wx/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporary directory in local drive for downloading data from S3 bucket\n",
    "# If the directory doesn't exist, it will be created\n",
    "# If we used zarr, this wouldn't be neccessary\n",
    "temp_dir = \"./tmp\"\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nc_from_s3_clean(network_name, station_id, temp_dir):\n",
    "    \"\"\"Read netcdf file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    I'd like to see us use a zarr workflow if possible to avoid this.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".nc\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/2_clean_wx/{}/{}.nc\".format(\n",
    "        network_name, station_id\n",
    "    )\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"h5netcdf\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zarr_from_s3(station_id, temp_dir):\n",
    "    \"\"\"Read zarr file containing station data for a single station of interest from AWS s3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_name: str\n",
    "        Name of network (i.e. \"ASOSAWOS\")\n",
    "        Must correspond with a valid directory in the s3 bucket (i.e. \"CAHYDRO\", \"CDEC\", \"ASOSAWOS\")\n",
    "    station_id: str\n",
    "        Station identifier; i.e. the name of the netcdf file in the bucket (i.e. \"ASOSAWOS_72012200114.nc\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    station_data: xr.Dataset\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The data is first downloaded from AWS into a tempfile, which is then deleted after xarray reads in the file\n",
    "    \"\"\"\n",
    "\n",
    "    # Temp file for downloading from s3\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        dir=temp_dir, prefix=\"\", suffix=\".zarr\", delete=True\n",
    "    )\n",
    "\n",
    "    # Create s3 file system\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "    # Get URL to netcdf in S3\n",
    "    s3_url = \"s3://wecc-historical-wx/3_qaqc_wx/VALLEYWATER/VALLEYWATER_{}.zarr\".format(\n",
    "        station_id\n",
    "    )\n",
    "    print(s3_url)\n",
    "\n",
    "    # Read in the data using xarray\n",
    "    s3_file_obj = s3.get(s3_url, temp_file.name)\n",
    "    station_data = xr.open_dataset(temp_file.name, engine=\"zarr\").load()\n",
    "\n",
    "    # Close temporary file\n",
    "    temp_file.close()\n",
    "\n",
    "    return station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaqc_ds_to_df(ds, verbose=False):\n",
    "    \"\"\"Converts xarray ds for a station to pandas df in the format needed for the pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        input data from the clean step\n",
    "    verbose : bool, optional\n",
    "        if True, provides runtime output to the terminal\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        converted xr.Dataset into dataframe\n",
    "    MultiIndex : pd.Index\n",
    "        multi-index of station and time\n",
    "    attrs : list of str\n",
    "        attributes from xr.Dataset\n",
    "    var_attrs : list of str\n",
    "        variable attributes from xr.Dataset\n",
    "    era_qc_vars : list of str\n",
    "        QAQC variables\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This is the notebook friendly version (no logger statements).\n",
    "    \"\"\"\n",
    "    ## Add qc_flag variable for all variables, including elevation;\n",
    "    ## defaulting to nan for fill value that will be replaced with qc flag\n",
    "\n",
    "    for key, val in ds.variables.items():\n",
    "        if val.dtype == object:\n",
    "            if key == \"station\":\n",
    "                if str in [type(v) for v in ds[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "            else:\n",
    "                if str in [type(v) for v in ds.isel(station=0)[key].values]:\n",
    "                    ds[key] = ds[key].astype(str)\n",
    "\n",
    "    exclude_qaqc = [\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"qaqc_process\",\n",
    "        \"sfcWind_method\",\n",
    "        \"pr_duration\",\n",
    "        \"pr_depth\",\n",
    "        \"PREC_flag\",\n",
    "        \"rsds_duration\",\n",
    "        \"rsds_flag\",\n",
    "        \"anemometer_height_m\",\n",
    "        \"thermometer_height_m\",\n",
    "    ]  # lat, lon have different qc check\n",
    "\n",
    "    raw_qc_vars = []  # qc_variable for each data variable, will vary station to station\n",
    "    era_qc_vars = []  # our ERA qc variable\n",
    "    old_era_qc_vars = []  # our ERA qc variable\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if \"q_code\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variable, need to keep for comparison, then drop\n",
    "        if \"_qc\" in var:\n",
    "            raw_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "        if \"_eraqc\" in var:\n",
    "            era_qc_vars.append(\n",
    "                var\n",
    "            )  # raw qc variables, need to keep for comparison, then drop\n",
    "            old_era_qc_vars.append(var)\n",
    "\n",
    "    print(f\"era_qc existing variables:\\n{era_qc_vars}\")\n",
    "    n_qc = len(era_qc_vars)\n",
    "\n",
    "    for var in ds.data_vars:\n",
    "        if var not in exclude_qaqc and var not in raw_qc_vars and \"_eraqc\" not in var:\n",
    "            qc_var = var + \"_eraqc\"  # variable/column label\n",
    "\n",
    "            # if qaqc var does not exist, adds new variable in shape of original variable with designated nan fill value\n",
    "            if qc_var not in era_qc_vars:\n",
    "                print(f\"nans created for {qc_var}\")\n",
    "                ds = ds.assign({qc_var: xr.ones_like(ds[var]) * np.nan})\n",
    "                era_qc_vars.append(qc_var)\n",
    "\n",
    "    print(\"{} created era_qc variables\".format(len(era_qc_vars) - len(old_era_qc_vars)))\n",
    "    if len(era_qc_vars) != n_qc:\n",
    "        print(\"{}\".format(np.setdiff1d(old_era_qc_vars, era_qc_vars)))\n",
    "\n",
    "    # Save attributes to inheret them to the QAQC'ed file\n",
    "    attrs = ds.attrs\n",
    "    # var_attrs = {var: ds[var].attrs for var in list(ds.data_vars.keys())}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "    # instrumentation heights\n",
    "    if \"anemometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"anemometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.anemometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling anemometer_height_m with NaN.\", flush=True)\n",
    "            df[\"anemometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "    if \"thermometer_height_m\" not in df.columns:\n",
    "        try:\n",
    "            df[\"thermometer_height_m\"] = (\n",
    "                np.ones(ds[\"time\"].shape) * ds.thermometer_height_m\n",
    "            )\n",
    "        except:\n",
    "            print(\"Filling thermometer_height_m with NaN.\", flush=True)\n",
    "            df[\"thermometer_height_m\"] = np.ones(len(df)) * np.nan\n",
    "        finally:\n",
    "            pass\n",
    "\n",
    "    # De-duplicate time axis\n",
    "    df = df[~df.index.duplicated()].sort_index()\n",
    "\n",
    "    # Save station/time multiindex\n",
    "    MultiIndex = df.index\n",
    "    station = df.index.get_level_values(0)\n",
    "    df[\"station\"] = station\n",
    "\n",
    "    # Station pd.Series to str\n",
    "    station = station.unique().values[0]\n",
    "\n",
    "    # Convert time/station index to columns and reset index\n",
    "    df = df.droplevel(0).reset_index()\n",
    "\n",
    "    # Add time variables needed by multiple functions\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"time\"]).dt.hour\n",
    "    df[\"day\"] = pd.to_datetime(df[\"time\"]).dt.day\n",
    "    df[\"month\"] = pd.to_datetime(df[\"time\"]).dt.month\n",
    "    df[\"year\"] = pd.to_datetime(df[\"time\"]).dt.year\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"]).dt.date\n",
    "\n",
    "    return df  # , MultiIndex, attrs, var_attrs, era_qc_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load station lists for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in ASOSAWOS stations\n",
    "\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "asosawos = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/ASOSAWOS/stationlist_ASOSAWOS_cleaned.csv\",\n",
    ")\n",
    "asosawos_list = pd.read_csv(BytesIO(asosawos[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valleywater = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/VALLEYWATER/stationlist_VALLEYWATER_cleaned.csv\",\n",
    ")\n",
    "valleywater_list = pd.read_csv(BytesIO(valleywater[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime = s3_cl.get_object(\n",
    "    Bucket=\"wecc-historical-wx\",\n",
    "    Key=\"2_clean_wx/MARITIME/stationlist_MARITIME_cleaned.csv\",\n",
    ")\n",
    "maritime_list = pd.read_csv(BytesIO(maritime[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify candidates for concatenation and upload to AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do so by identifying stations with exactly matching latitudes and longitudes.\n",
    "\n",
    "Some additional methods to use:\n",
    "1. matching IDs, for stations in which those exist (NOT currently used)\n",
    "2. stations within a certain distance of each other (I've investigated this some, but would take consideraly more time to fully develop and may not be necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of networks to be checked for concatenation\n",
    "target_networks = [\"VALLEYWATER\"]  # , \"ASOSAWOS\", \"MARITIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that need to be concatenated.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Stations are flagged if they have identical latitudes and longitudes\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "                input station list with a flag column assigning an integer to each group of repeat latitudes and longitudes\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Flag stations with identical latitudes and longitudes, then assign each group a unique integer\n",
    "\n",
    "    # List of possible variable names for longitudes and latitudes\n",
    "    lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\", 'lat','lon']\n",
    "    # Extract the latitude and longitude variable names from the input dataframe\n",
    "    lat_lon_cols = [col for col in station_list.columns if col in lat_lon_list]\n",
    "\n",
    "    # Generate column flagging duplicate latitudes and longitudes\n",
    "    station_list[\"concat_subset\"] = station_list.duplicated(\n",
    "        subset=lat_lon_cols, keep=False\n",
    "    )\n",
    "    # within each group of identical latitudes and longitudes, assign a unique integer\n",
    "    station_list[\"concat_subset\"] = (\n",
    "        station_list[station_list[\"concat_subset\"] == True].groupby(lat_lon_cols).ngroup()\n",
    "    )\n",
    "\n",
    "    ##### Order station list by flag\n",
    "    concat_station_list = station_list.sort_values(\"concat_subset\")\n",
    "\n",
    "    ##### Keep only flagged stations\n",
    "    concat_station_list = concat_station_list[~concat_station_list[\"concat_subset\"].isna()]\n",
    "\n",
    "    ##### Format final list\n",
    "    # Convert flags to integers - this is necessary for the final concatenation step\n",
    "    concat_station_list[\"concat_subset\"] = concat_station_list[\"concat_subset\"].astype(\n",
    "        \"int32\"\n",
    "    )\n",
    "    # Now keep only the ERA-ID and flag column\n",
    "    era_id_list = ['ERA-ID','era-id']\n",
    "    era_id_col = [col for col in station_list.columns if col in era_id_list]\n",
    "    concat_station_list = concat_station_list[era_id_col + [\"concat_subset\"]]\n",
    "\n",
    "    # Standardize ERA id to \"ERA-ID\" (this is specific to Valleywater stations)\n",
    "    if 'era-id' in era_id_col:\n",
    "        concat_station_list.rename(columns={\"era-id\": \"ERA-ID\"}, inplace=True)\n",
    "\n",
    "    return concat_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_concat_check(station_names_list):\n",
    "    \"\"\"\n",
    "    This function applies the conatenation check to a list of target stations. \n",
    "    It then upload a csv containing the ERA IDs and concatenation subset ID for \n",
    "    all identified stations in a network.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station__names_list: pd.DataFrame\n",
    "            list of target station names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            uploads list of stations to be concatenated to AWS\n",
    "        if failure:\n",
    "            None\n",
    "\n",
    "    \"\"\"\n",
    "    final_list = pd.DataFrame([])\n",
    "    for station in station_names_list:\n",
    "\n",
    "        ##### Import station list of target station\n",
    "        key = \"2_clean_wx/{}/stationlist_{}_cleaned.csv\".format(station,station)\n",
    "        bucket_name = \"wecc-historical-wx\"\n",
    "        list_import = s3_cl.get_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "        )\n",
    "        station_list = pd.read_csv(BytesIO(list_import[\"Body\"].read()))\n",
    "\n",
    "        ##### Apply concatenation check\n",
    "        concat_list = concatenation_check(station_list)\n",
    "\n",
    "        ##### Rename the flags for each subset to <station>_<subset number>\n",
    "        concat_list[\"concat_subset\"] = station + '_' + concat_list[\"concat_subset\"].astype(str)\n",
    "\n",
    "        ##### Append to final list of stations to concatenate\n",
    "        final_list = pd.concat([final_list,concat_list])\n",
    "\n",
    "        ##### Upload to QAQC directory in AWS\n",
    "        new_buffer = StringIO()\n",
    "        final_list.to_csv(new_buffer, index = False)\n",
    "        content = new_buffer.getvalue()\n",
    "\n",
    "        s3_cl.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = content,\n",
    "            Key = qaqcdir + station + \"_copy\" \"/\"+ station + \"/\" + station + \"_concat_list_TEST.csv\"\n",
    "            #Key = qaqcdir + station + \"/{}_concat_list_TEST.csv\".format(station)\n",
    "        )\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = apply_concat_check(target_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that stations already indentified for concatenation are flagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maritime station:\n",
    "\n",
    "- MTYC1 and MEYC1\n",
    "\n",
    "- SMOC1 and ICAC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maritime_out\n",
    "\n",
    "# Flagged Stations:\n",
    "# MARITIME_LJAC1 <=> MARITIME_LJPC1\n",
    "# MARITIME_ICAC1 <=> MARITIME_SMOC1\n",
    "# MARITIME_MEYC1 <=> MARITIME_MTYC1 <=> MARITIME_MYXC1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously identified stations are indeed flagged. Along with an additional pair: MARITIME_LJAC1 and MARITIME_LJPC1. And a third station included with MARITIME_MEYC1 abd MARITIME_MTYC1: MARITIME_MYXC1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using ICAO values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_list = asosawos_list[asosawos_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "\n",
    "# how many unique ICAO duplicates are there?\n",
    "print(len(repeat_list[\"ICAO\"].unique()))\n",
    "\n",
    "print(repeat_list.groupby(\"ICAO\").count().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(repeat_list[\"ICAO\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate problem station KMLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf = repeat_list[repeat_list[\"ICAO\"] == \"KMLF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmlf[[\"STATION NAME\", \"LAT\", \"LON\", \"start_time\", \"end_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using station locations (lat, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataframe\n",
    "\n",
    "test = asosawos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_list = [\"LAT\", \"LON\", \"latitude\", \"longitude\", \"LATITUDE\", \"LONGITUDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols = [col for col in test.columns if col in lat_lon_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = asosawos_list.duplicated(subset=lat_lon_cols, keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"concat_flag\"] = test[test[\"concat_flag\"] == True].groupby(lat_lon_cols).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_var_list = [\"end_time\", \"end-date\"]\n",
    "end_time_col = [col for col in test.columns if col in time_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(\"concat_flag\")\n",
    "test = (\n",
    "    test.groupby([\"concat_flag\"])\n",
    "    .apply(lambda x: x.sort_values(end_time_col))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing ICAO identification and lat lon identification for ASOSAWOS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Carry Out Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the order of operations:\n",
    "\n",
    "1. Read in target stations, for each concat_flag\n",
    "2. Check if there is overlap in time ranges\n",
    "    1. IF so:  \n",
    "\n",
    "        split overall time range\n",
    "\n",
    "        construct dataset by grabbing newest station for each time range subset\n",
    "\n",
    "    \n",
    "    2. ELSE:\n",
    "\n",
    "        concatenate, with NAs in the gap\n",
    "\n",
    "\n",
    "Another option: pairwise concetenation\n",
    "\n",
    "For each subset of matching stations, first concatenate the two newest stations. Then, the next oldest, etc.\n",
    "\n",
    "\n",
    "Issues to address:\n",
    "\n",
    "1. when the time range of one station completely includes that of another in a subset (this occures a few times with ASOSAWOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate pairs of stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = 'VALLEYWATER'\n",
    "network_list = s3_cl.get_object(\n",
    "    Bucket=bucket,\n",
    "    # Key=\"3_qaqc_wx/{}/concat_list_{}.csv\".format(network_name,network_name)\n",
    "    Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(network_name, network_name, network_name),\n",
    ")\n",
    "concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve ERA IDs in this subset of stations\n",
    "station_1 = concat_list[\"ERA-ID\"].iloc[0]\n",
    "station_2 = concat_list[\"ERA-ID\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this subset of datasets and convert to dataframe\n",
    "url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}_copy/{}/{}.zarr\".format(\n",
    "    network_name, network_name, station_1\n",
    ")\n",
    "url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}_copy/{}/{}.zarr\".format(\n",
    "    network_name, network_name, station_2\n",
    ")\n",
    "\n",
    "# TODO: open_zarr will be used for QAQC'd datasets\n",
    "ds_1 = xr.open_zarr(url_1)\n",
    "ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "df_1 = ds_1.to_dataframe()\n",
    "df_2 = ds_2.to_dataframe()\n",
    "\n",
    "\n",
    "# apply reset index only to 'time', as we will need that for concatenation\n",
    "df_1 = df_1.reset_index(level=\"time\")\n",
    "df_2 = df_2.reset_index(level=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which dataset is older\n",
    "if df_1[\"time\"].max() < df_2[\"time\"].max():\n",
    "    # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "    # we also grab the name of the newer station in this step, for use later\n",
    "    df_new = df_2\n",
    "    ds_new = ds_2\n",
    "\n",
    "    df_old = df_1\n",
    "    ds_old = ds_1\n",
    "else:\n",
    "    df_new = df_1\n",
    "    ds_new = ds_1\n",
    "\n",
    "    df_old = df_2\n",
    "    ds_old = ds_2\n",
    "\n",
    "# now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is overlap between the two time series\n",
    "if len(df_overlap) == 0:\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "# if not, concatenate\n",
    "else:\n",
    "    df_concat = concat([df_new, df_overlap, df_old])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Now prepare the final concatenated dataframe for export\n",
    "\n",
    "# Modify index for df_old_cleaned\n",
    "# We want the final dataset to show up as the new station, not the old\n",
    "# station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "# final_station_name = \"{}_{}\".format(network_name, station_name_new)\n",
    "# new_index = [final_station_name] * len(df_concat)\n",
    "\n",
    "# df_concat['station'] = new_index\n",
    "\n",
    "# df_concat.set_index(['time','station'])\n",
    "\n",
    "########################## TODO below is the original #########################\n",
    "station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "final_station_name = \"{}_{}\".format(network_name, station_name_new)\n",
    "new_index = [final_station_name] * len(df_concat)\n",
    "df_concat.index = new_index\n",
    "df_concat.index.name = \"station\"\n",
    "\n",
    "# Add 'time' back into multi index\n",
    "df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "########################## TODO above is the original #########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows that were potentially generated in the concatenation process\n",
    "df_concat = df_concat.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert concatenated dataframe to dataset\n",
    "ds_concat = df_concat.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Update attributes and datatypes #####\n",
    "\n",
    "# Include past attributes\n",
    "ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "# Update 'history' attribute\n",
    "timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "    \"history\"\n",
    "] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "# Update 'comment' attribute\n",
    "ds_concat.attrs[\"comment\"] = (\n",
    "    \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    ")\n",
    "\n",
    "# Add new qaqc_files_merged attribute\n",
    "station_name_old = ds_old.coords[\"station\"].values[0]\n",
    "ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "    \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "        network_name, station_name_old, network_name, station_name_new\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of variables to be assigned\n",
    "\n",
    "float32_variables = [\n",
    "    \"anemometer_height_m\",\n",
    "    \"elevation\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"pr_15min\",\n",
    "    \"thermometer_height_m\",\n",
    "    \"ps\",\n",
    "    \"tas\",\n",
    "    \"tdps\",\n",
    "    \"pr\",\n",
    "    \"sfcWind\",\n",
    "    \"sfcWind_dir\",\n",
    "    \"ps_altimeter\",\n",
    "    \"pr_duration\",\n",
    "    \"ps_eraqc\",\n",
    "    \"tas_eraqc\",\n",
    "    \"tdps_eraqc\",\n",
    "    \"pr_eraqc\",\n",
    "    \"sfcWind_eraqc\",\n",
    "    \"sfcWind_dir_eraqc\",\n",
    "    \"elevation_eraqc\",\n",
    "    \"ps_altimeter_eraqc\",\n",
    "    \"pr_15min_eraqc\",\n",
    "]\n",
    "U16_variables = [\n",
    "    \"raw_qc\",\n",
    "    \"qaqc_process\",\n",
    "    \"ps_qc\",\n",
    "    \"ps_altimeter_qc\",\n",
    "    \"psl_qc\",\n",
    "    \"tas_qc\",\n",
    "    \"tdps_qc\",\n",
    "    \"pr_qc\",\n",
    "    \"pr_depth_qc\",\n",
    "    \"sfcWind_qc\",\n",
    "    \"sfcWind_method\",\n",
    "    \"sfcWind_dir_qc\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all datatypes, to enable export\n",
    "existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "existing_U16 = [col for col in U16_variables if col in df_concat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_station_pairs(network_name):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Read in concatenation list of input network\n",
    "    network_list = s3_cl.get_object(\n",
    "        Bucket=bucket,\n",
    "        Key=\"3_qaqc_wx/{}_copy/{}/{}_concat_list_TEST.csv\".format(\n",
    "            network_name, network_name, network_name\n",
    "        ),\n",
    "    )\n",
    "    concat_list = pd.read_csv(BytesIO(network_list[\"Body\"].read()))\n",
    "\n",
    "    subset_number = len(concat_list['concat_subset'].unique()\n",
    "                        )\n",
    "    for i in range(0,subset_number):\n",
    "\n",
    "        # count the number of staions in subset i\n",
    "        subset_i = concat_list[\n",
    "            concat_list[\"concat_subset\"].str.contains(\"{}\".format(i))\n",
    "        ]\n",
    "\n",
    "        n = subset_i.count()[0]\n",
    "\n",
    "        # if there are only two stations, proceed with concatenation\n",
    "        if n == 2:\n",
    "            # retrieve ERA IDs in this subset of stations\n",
    "            station_1 = subset_i[\"ERA-ID\"].iloc[0]\n",
    "            station_2 = subset_i[\"ERA-ID\"].iloc[1]\n",
    "\n",
    "            # import this subset of datasets and convert to dataframe\n",
    "            url_1 = \"s3://wecc-historical-wx/3_qaqc_wx/{}_copy/{}/{}.zarr\".format(\n",
    "                network_name, network_name, station_1\n",
    "            )\n",
    "            url_2 = \"s3://wecc-historical-wx/3_qaqc_wx/{}_copy/{}/{}.zarr\".format(\n",
    "                network_name,network_name, station_2\n",
    "            )\n",
    "\n",
    "            # TODO: open_zarr will be used for QAQC'd datasets\n",
    "            ds_1 = xr.open_zarr(url_1)\n",
    "            ds_2 = xr.open_zarr(url_2)\n",
    "\n",
    "            df_1 = ds_1.to_dataframe()\n",
    "            df_2 = ds_2.to_dataframe()\n",
    "\n",
    "            # apply reset index only to 'time', as we will need that for concatenation\n",
    "            df_1 = df_1.reset_index(level=\"time\")\n",
    "            df_2 = df_2.reset_index(level=\"time\")\n",
    "\n",
    "            # df_1 = df_1.reset_index()\n",
    "            # df_2 = df_2.reset_index()\n",
    "\n",
    "            # determine which dataset is older\n",
    "            if df_1[\"time\"].max() < df_2[\"time\"].max(): \n",
    "                # if df_1 has an earlier end tiem than df_2, then d_2 is newer\n",
    "                # we also grab the name of the newer station in this step, for use later\n",
    "                df_new = df_2\n",
    "                ds_new = ds_2\n",
    "\n",
    "                df_old = df_1\n",
    "                ds_old = ds_1\n",
    "            else:\n",
    "                df_new = df_1\n",
    "                ds_new = ds_1\n",
    "                df_old = df_2\n",
    "                ds_old = ds_2\n",
    "\n",
    "            print('about to time')\n",
    "            # now set things up to determine if there is temporal overlap between df_new and df_old\n",
    "            df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "            # if there is overlap between the two time series\n",
    "            if len(df_overlap) == 0:\n",
    "                ##### Split datframes into subsets #####\n",
    "\n",
    "                # Remove data in time overlap between old and new\n",
    "                df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "                df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "                # Data in new input that overlaps in time with old input\n",
    "                df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "                ##### Concatenate subsets #####\n",
    "                df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "            # if not, concatenate\n",
    "            else: \n",
    "                df_concat = concat([df_new, df_overlap, df_old])\n",
    "\n",
    "            ##### Now prepare the final concatenated dataframe for export\n",
    "            station_name_new = ds_new.coords[\"station\"].values[0]\n",
    "            final_station_name = \"{}_{}\".format(network_name, station_name_new)\n",
    "            new_index = [final_station_name] * len(df_concat)\n",
    "            df_concat.index = new_index\n",
    "            df_concat.index.name = \"station\"\n",
    "\n",
    "            # Add 'time' back into multi index\n",
    "            df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "            # drop duplicate rows that were potentially generated in the concatenation process\n",
    "            df_concat = df_concat.drop_duplicates()\n",
    "            \n",
    "            # Convert concatenated dataframe to dataset\n",
    "            ds_concat = df_concat.to_xarray()\n",
    "\n",
    "            ##### Update attributes and datatypes #####\n",
    "\n",
    "            # Include past attributes\n",
    "            ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "            # Update 'history' attribute\n",
    "            timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "            ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "                \"history\"\n",
    "            ] + \" \\n maritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "            # Update 'comment' attribute\n",
    "            ds_concat.attrs[\"comment\"] = (\n",
    "                \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "            )\n",
    "\n",
    "            # Add new qaqc_files_merged attribute\n",
    "            station_name_old = ds_old.coords[\"station\"].values[0]\n",
    "            ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "                \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "                    network_name, station_name_old, network_name, station_name_new\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert all datatypes, to enable export\n",
    "            existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "            #existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "            ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "            #ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"<U16\")\n",
    "\n",
    "            ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "            ### Export ###\n",
    "            export_url = \"s3://wecc-historical-wx/3_qaqc_wx/{}_copy/{}/{}_{}.zarr\".format(\n",
    "                network_name, network_name, network_name, \"test\"\n",
    "            )\n",
    "            ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "        # if there are more than two stations in the subset, continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to time\n"
     ]
    }
   ],
   "source": [
    "concatenate_station_pairs('VALLEYWATER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_test(concat_list):\n",
    "    \"\"\"\n",
    "    Performs concatenation for stations in list of stations flagged for concatenation.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    ##### Import target datasets and convert to dataframe\n",
    "    flag_range = list(\n",
    "        range(concat_list[\"concat_flag\"].min(), concat_list[\"concat_flag\"].max())\n",
    "    )\n",
    "\n",
    "    for i in flag_range:\n",
    "        subset_list = concat_list[concat_list[\"concat_flag\"] == i]\n",
    "        subset_range = list(range(0, len(subset_list)))\n",
    "\n",
    "        url = {}\n",
    "        ds = {}\n",
    "        df = {}\n",
    "\n",
    "        for i in subset_range:\n",
    "\n",
    "            # extract information needed for dataset import\n",
    "            row_i = subset_list.iloc[[i]]\n",
    "            network_name = row_i[\"ERA-ID\"].split(\"_\")[\n",
    "                0\n",
    "            ]  # TODO: this does not work, when it really should\n",
    "            station_name = row_i[\"ERA-ID\"]\n",
    "\n",
    "            url[i] = \"s3://wecc-historical-wx/3_qaqc_wx/{}/{}_{}.zarr\".format(\n",
    "                network_name, network_name, station_name\n",
    "            )\n",
    "\n",
    "            ds[i] = xr.open_zarr(url[i])\n",
    "\n",
    "            df[i] = ds[i].to_dataframe()\n",
    "\n",
    "            # Apply reset index only to 'time', as we will need that for concatenation\n",
    "            df[i] = df[i].reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE SCRAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = asosawos_list_concat.groupby([\"ICAO\"]).apply(\n",
    "    lambda x: x.sort_values([\"end_time\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort by end_time or end-date, depending on the station TODO: this is not necessary\n",
    "# time_var_list = ['end_time','end-date']\n",
    "# end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "# new_station_list = new_station_list.groupby('concat_flag').apply(lambda x: x.sort_values(end_time_or_date)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(df):\n",
    "    \"\"\"\n",
    "    Concatenates station data that has been flagged for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.dataframe\n",
    "            staton data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "    # if there is overlap, then create subsets\n",
    "\n",
    "    # if no overlap, just concatenate\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_station_list(station_list, concat_list, duplicate_list):\n",
    "    \"\"\"\n",
    "    Reorders the input station list, necessary for concatenation\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.dataframe\n",
    "\n",
    "        concat_list: pd.dataframe\n",
    "\n",
    "        duplicate_list: pd.dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            output station list with stations to be concatenated at top, followed by potential duplicates\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    ##### subsets of station list\n",
    "\n",
    "    # stations that will be concatenated\n",
    "    concat_stations = station_list[station_list[\"ICAO\"].isin(concat_list)]\n",
    "\n",
    "    # potential duplicate stations\n",
    "    duplicate_stations = station_list[station_list[\"ICAO\"].isin(duplicate_list)]\n",
    "\n",
    "    # all remaining stations\n",
    "    remaining_stations = station_list[\n",
    "        ~station_list[\"ICAO\"].isin(duplicate_list + concat_list)\n",
    "    ]\n",
    "\n",
    "    ##### sort concat list alphabetically, to ensure that stations with the same ICAO are grouped together\n",
    "    concat_stations = concat_stations.sort_values(\"ICAO\")\n",
    "    duplicate_stations = duplicate_stations.sort_values(\"ICAO\")\n",
    "\n",
    "    ##### now within each ICAO, order by end time\n",
    "    concat_stations = concat_stations.groupby([\"ICAO\"]).apply(\n",
    "        lambda x: x.sort_values([\"end_time\"])\n",
    "    )\n",
    "\n",
    "    ##### concatenate susbets and reset index\n",
    "    new_list = concat(\n",
    "        [concat_stations, duplicate_stations, remaining_stations]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for presence of start and end times\n",
    "\n",
    "time_check = repeat_list_subset.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "print(\"number of null start times:\")\n",
    "print(time_check[\"start_time\"].sum())\n",
    "\n",
    "print(\"number of null end times:\")\n",
    "print(time_check[\"end_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the start and end times are identical\n",
    "\n",
    "start_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")\n",
    "end_duplicate_check = (\n",
    "    repeat_list_subset.groupby(\"ICAO\")\n",
    "    .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "    .rename(\"check\")\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\"ICAO\"].tolist()\n",
    "start_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "    \"ICAO\"\n",
    "].tolist()\n",
    "\n",
    "print(end_list)\n",
    "print(start_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is going on with the stations that have duplicate start and end times? are they true duplicates?\n",
    "\n",
    "repeat_list_subset[repeat_list_subset[\"ICAO\"].isin(start_list + end_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in single dc file from AWS\n",
    "ds_1 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_72026294076\", temp_dir)\n",
    "ds_2 = read_nc_from_s3_clean(\"ASOSAWOS\", \"ASOSAWOS_A0000594076\", temp_dir)\n",
    "\n",
    "\n",
    "# convert to formatted pandas dataframe\n",
    "df_1 = qaqc_ds_to_df(ds_1, verbose=False)\n",
    "df_2 = qaqc_ds_to_df(ds_2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = df_1.lon.mean()\n",
    "lat = df_1.lat.mean()\n",
    "# print(\"{}, {:.5f}, {:.5f}\".format(id, lon, lat))\n",
    "\n",
    "\n",
    "# Plot time series of the data\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "\n",
    "df_1.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "df_2.plot(ax=ax, x=\"time\", y=\"sfcWind\")\n",
    "\n",
    "ax.set_title(\"{}  ({:.3f}, {:.3f})\".format(id, lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_check_old(station_list):\n",
    "    \"\"\"\n",
    "    Resamples meteorological variables to hourly timestep according to standard conventions.\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.)\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        df: pd.DataFrame\n",
    "            list of station information\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            list\n",
    "                list of ICAO values of stations that need to be concatenated\n",
    "            list\n",
    "                list of ICAO values of potential duplicate stations\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Generate list of repeat ICAOs\n",
    "    repeat_list = station_list[station_list.duplicated(subset=[\"ICAO\"], keep=False)]\n",
    "    repeat_list = repeat_list[\n",
    "        [\"ICAO\", \"ERA-ID\", \"STATION NAME\", \"start_time\", \"end_time\"]\n",
    "    ]\n",
    "\n",
    "    concat_list = repeat_list[\"ICAO\"].unique().tolist()\n",
    "\n",
    "    # And empty list to add potential duplicates to\n",
    "    duplicate_list = []\n",
    "\n",
    "    ##### Generate boolean for whether or not there are null start and/or end times\n",
    "    # TODO: may not be necessary\n",
    "    time_check = repeat_list.groupby(\"ICAO\").apply(lambda x: x.isnull().any())\n",
    "\n",
    "    end_nan_list = time_check[time_check[\"end_time\"] == True][\"ICAO\"].tolist()\n",
    "    start_nan_list = time_check[time_check[\"start_time\"] == True][\"ICAO\"].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_nan_list + end_nan_list\n",
    "\n",
    "    duplicate_list = duplicate_list\n",
    "\n",
    "    ##### Identify ICAOs with duplicate start end times\n",
    "    start_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"start_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    end_duplicate_check = (\n",
    "        repeat_list.groupby(\"ICAO\")\n",
    "        .apply(lambda x: x.duplicated(subset=[\"end_time\"]))\n",
    "        .rename(\"check\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    end_dup_list = end_duplicate_check[end_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "    start_dup_list = start_duplicate_check[start_duplicate_check[\"check\"] == True][\n",
    "        \"ICAO\"\n",
    "    ].tolist()\n",
    "\n",
    "    # add ICAOs of stations with nan start or end times to potential duplicates list\n",
    "    duplicate_list = duplicate_list + start_dup_list + end_dup_list\n",
    "\n",
    "    # Generate final list of ICAOs for stations to be concatenated\n",
    "    concat_list = [x for x in concat_list if x not in duplicate_list]\n",
    "\n",
    "    return concat_list, duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the subset with only stations to concatenate\n",
    "\n",
    "asosawos_list_concat[\"ICAO\"] = pd.Categorical(\n",
    "    asosawos_list_concat[\"ICAO\"], categories=concat_list, ordered=True\n",
    ")\n",
    "\n",
    "test_list = asosawos_list_concat.sort_values(\"ICAO\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stations within a certain distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into GeoDataFrames\n",
    "# using EPSG 3310\n",
    "\n",
    "gdf_asosawos = gpd.GeoDataFrame(\n",
    "    asosawos_list,\n",
    "    geometry=[\n",
    "        Point(lon, lat) for lon, lat in zip(asosawos_list[\"LON\"], asosawos_list[\"LAT\"])\n",
    "    ],\n",
    "    crs=\"EPSG:4326\",\n",
    ").to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 3: find the nearest point in the geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert emtpy columns\n",
    "\n",
    "gdf_asosawos[\"nearest_station\"] = pd.Series(dtype=\"U16\")\n",
    "gdf_asosawos[\"distance\"] = pd.Series(dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in gdf_asosawos.iterrows():\n",
    "    # geometry of individual row \n",
    "    point = row.geometry\n",
    "    # returns a multipoint object with the geometries of every row in the gdf\n",
    "    multipoint = gdf_asosawos.drop(index, axis=0).geometry.unary_union\n",
    "    # \n",
    "    queried_geom, nearest_geom = nearest_points(point, multipoint)\n",
    "    dist_from_point = \n",
    "    gdf_asosawos.loc[index, 'nearest_geometry'] = nearest_geom\n",
    "    gdf_asosawos.loc[index, 'distance'] = nearest_geom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 2: distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the distance between points\n",
    "\n",
    "\n",
    "def distance_sort_filter(row, df2, buffer=None, id=False):\n",
    "\n",
    "    dist = df2.geometry.distance(row).sort_values()\n",
    "\n",
    "    if buffer:\n",
    "        dist = dist[dist < buffer]\n",
    "\n",
    "    if id:\n",
    "        distances = {\n",
    "            df2.loc[idx][\"WBAN\"]: value for idx, value in zip(dist.index, dist.values)\n",
    "        }\n",
    "    else:\n",
    "        distances = {idx: value for idx, value in zip(dist.index, dist.values)}\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### approach 1: using sjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a buffer around points in gdf1 (e.g., 10 km buffer)\n",
    "gdf_asosawos[\"buffer\"] = gdf_asosawos.geometry.buffer(\n",
    "    0.1\n",
    ")  # Buffer in degrees, 0.1 degrees approx equals 10 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a spatial join using the buffer\n",
    "merged = gpd.sjoin(\n",
    "    gdf_asosawos, gdf_asosawos[[\"geometry\", \"buffer\"]], how=\"inner\", predicate=\"within\"\n",
    ")\n",
    "\n",
    "# The 'merged' GeoDataFrame contains points from gdf_isd that are within the buffer around points in gdf_asosawos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    merged\n",
    ")  # there are not ISD stations within 10km of an ASOSAWOS station missed by the exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Round asosawos down to 3 decimal points of accuracy\n",
    "# asosawos_round = asosawos_list.round({\"LAT\": 3, \"LON\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential ways to check that two stations are duplicates\n",
    "1. identical total_nobs\n",
    "2. identical ERA IDs\n",
    "3. identical end or start times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract flagged stations\n",
    "\n",
    "asosawos_dup = asosawos_out[~asosawos_out[\"concat_flag\"].isna()]\n",
    "valleywater_dup = valleywater_out[~valleywater_out[\"concat_flag\"].isna()]\n",
    "maritime_dup = maritime_out[~maritime_out[\"concat_flag\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_check(station_list):\n",
    "    \"\"\"\n",
    "    This function flags stations that are potentially duplicates\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) Within stations flagged for concatenation, stations are flagged as potential duplicates\n",
    "            if either their start or end times are identical\n",
    "            - TODO: brainstorm alternative approaches\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        station_list: pd.DataFrame\n",
    "            list of station information that has passed through the concatenation check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            new_station_list: pd.DataFrame\n",
    "\n",
    "\n",
    "        if failure:\n",
    "            None\n",
    "    Notes\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    ##### flag stations with repeat end or start times\n",
    "\n",
    "    time_end_list = [\"end_time\", \"end-date\"]\n",
    "    time_start_list = [\"start_time\", \"start-date\"]\n",
    "\n",
    "    end_time_or_date = [col for col in station_list.columns if col in time_var_list]\n",
    "\n",
    "    new_station_list = (\n",
    "        new_station_list.groupby(\"concat_flag\")\n",
    "        .apply(lambda x: x.sort_values(end_time_or_date))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return new_station_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_target_stations_old(network_name, station_old, station_new):\n",
    "    \"\"\"\n",
    "    Concatenates two input datasets, deletes the originals, and exports the final concatenated dataset\n",
    "\n",
    "    Rules\n",
    "    ------\n",
    "        1.) concatenation: keep the newer station data in the time range in which both stations overlap\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "        network_name: string\n",
    "            weather station network\n",
    "        station_old: string\n",
    "            name of the older weather station\n",
    "        station_new: string\n",
    "            name of the newer weather station\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        if success:\n",
    "            all processed datasets are exported to the merge folder in AWS and the original datasets are deleted\n",
    "        if failure:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Import target datasets and convert to dataframe\n",
    "    old_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "    new_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "\n",
    "    ds_old = xr.open_zarr(old_url)\n",
    "    ds_new = xr.open_zarr(new_url)\n",
    "\n",
    "    df_old = ds_old.to_dataframe()\n",
    "    df_new = ds_new.to_dataframe()\n",
    "\n",
    "    # Apply reset index only to 'time', as we will need that for concatenation\n",
    "    df_old = df_old.reset_index(level=\"time\")\n",
    "    df_new = df_new.reset_index(level=\"time\")\n",
    "\n",
    "    ##### Split datframes into subsets #####\n",
    "\n",
    "    # Remove data in time overlap between old and new\n",
    "    df_old_cleaned = df_old[~df_old[\"time\"].isin(df_new[\"time\"])]\n",
    "    df_new_cleaned = df_new[~df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Data in new input that overlaps in time with old input\n",
    "    df_overlap = df_new[df_new[\"time\"].isin(df_old[\"time\"])]\n",
    "\n",
    "    # Set index to new input for df_old_cleaned\n",
    "    # We want the final dataset to show up as the new station, not the old\n",
    "    final_station_name = \"{}_{}\".format(network_name, station_new)\n",
    "    new_index = [final_station_name] * len(df_old_cleaned)\n",
    "\n",
    "    df_old_cleaned.index = new_index\n",
    "    df_old_cleaned.index.name = \"station\"\n",
    "\n",
    "    ##### Concatenate subsets #####\n",
    "\n",
    "    df_concat = concat([df_old_cleaned, df_overlap, df_new_cleaned])\n",
    "\n",
    "    # Add 'time' back into multi index\n",
    "    df_concat.set_index(\"time\", append=True, inplace=True)\n",
    "\n",
    "    # Convert concatenated dataframe to dataset\n",
    "    ds_concat = df_concat.to_xarray()\n",
    "\n",
    "    ##### Update attributes and datatypes #####\n",
    "\n",
    "    # Include past attributes\n",
    "    ds_concat.attrs = ds_new.attrs\n",
    "\n",
    "    # Update 'history' attribute\n",
    "    timestamp = datetime.datetime.utcnow().strftime(\"%m-%d-%Y, %H:%M:%S\")\n",
    "    ds_concat.attrs[\"history\"] = ds_new.attrs[\n",
    "        \"history\"\n",
    "    ] + \" \\nmaritime_merge.ipynb run on {} UTC\".format(timestamp)\n",
    "\n",
    "    # Update 'comment' attribute\n",
    "    ds_concat.attrs[\"comment\"] = (\n",
    "        \"Final v1 data product. This data has been subjected to cleaning, QA/QC, and standardization.\"\n",
    "    )\n",
    "\n",
    "    # Add new qaqc_files_merged attribute\n",
    "    ds_concat.attrs[\"qaqc_files_merged\"] = (\n",
    "        \"{}_{}, {}_{} merged. Overlap retained from newer station data.\".format(\n",
    "            network_name, station_old, network_name, station_new\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert all datatypes, to enable export\n",
    "    existing_float32 = [col for col in float32_variables if col in df_concat.columns]\n",
    "    existing_U16 = [col for col in U16_variables if col in df_concat.columns]\n",
    "\n",
    "    ds_concat[existing_float32] = ds_concat[existing_float32].astype(\"float32\")\n",
    "    ds_concat[existing_U16] = ds_concat[existing_U16].astype(\"U16\")\n",
    "\n",
    "    ds_concat.coords[\"station\"] = ds_concat.coords[\"station\"].astype(\"<U16\")\n",
    "\n",
    "    ### Export ###\n",
    "\n",
    "    # delete old inputs\n",
    "    bucket = \"wecc-historical-wx\"\n",
    "    key_new = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_new\n",
    "    )\n",
    "    key_old = \"4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, station_old\n",
    "    )\n",
    "\n",
    "    delete_folder(bucket, key_new)\n",
    "    delete_folder(bucket, key_old)\n",
    "\n",
    "    # Export final, concatenated dataset\n",
    "    export_url = \"s3://wecc-historical-wx/4_merge_wx/{}_dev/{}_{}.zarr\".format(\n",
    "        network_name, network_name, \"test\"\n",
    "    )\n",
    "    ds_concat.to_zarr(export_url, mode=\"w\")\n",
    "\n",
    "    return None  # ds_concat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
