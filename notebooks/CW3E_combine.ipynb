{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the CW3E network, individual stations have been split into multiple netcdf files due to file size limitations when the initial cleaning step was done \n",
    "# Here, we take all netcdf files corresponding to the same station, and merge them into one file. \n",
    "import xarray as xr \n",
    "import s3fs\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f82ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"wecc-historical-wx\"\n",
    "CW3E_cleaned_folder = \"2_clean_wx/CW3E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv from s3\n",
    "csv_filepath_s3 = \"s3://wecc-historical-wx/2_clean_wx/temp_clean_all_station_list.csv\"\n",
    "stations_df = pd.read_csv(csv_filepath_s3)\n",
    "\n",
    "# Filter the dataframe to only include rows corresponding to CW3E\n",
    "# And, only cleaned stations\n",
    "network_df = stations_df[\n",
    "    (stations_df[\"network\"] == \"CW3E\") & (stations_df[\"cleaned\"] == \"Y\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_in_s3_folder(bucket, folder):\n",
    "    \"\"\"Get a list of files in s3 bucket.\n",
    "    Make sure you follow the naming rules exactly for the two function arguments.\n",
    "    See example in the function docstrings for more details.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    bucket : str\n",
    "        Simply, the name of the bucket, with no slashes, prefixes, suffixes, etc...\n",
    "    folder : str\n",
    "        Folder within the bucket that you want the filenames from\n",
    "        MAKE SURE folder doesn't have a trailing \"/\"\n",
    "        i.e. it should be \"[folder]\", not \"[folder]/\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    files_in_s3 : list of str\n",
    "        List of filenames in the bucket\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    You want to get all the filenames in a s3 bucket with the following path:\n",
    "    s3 URI: \"s3://wecc-historical-wx/1_raw_wx/VALLEYWATER/\"\n",
    "    >>> get_filenames_in_s3_folder(\n",
    "    >>>    bucket = \"wecc-historical-wx\",\n",
    "    >>>    folder = \"1_raw_wx/VALLEYWATER\"\n",
    "    >>> )\n",
    "    ['ValleyWater_6001_1900-01-01_2024-11-11.csv','ValleyWater_6004_1900-01-01_2024-11-11.csv']\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    [1] https://stackoverflow.com/questions/59225939/get-only-file-names-from-s3-bucket-folder\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "    # Get all the filenames\n",
    "    # Just get relative path (f.key.split(folder + \"/\")[1])\n",
    "    files_in_s3 = [\n",
    "        f.key.split(folder + \"/\")[1]\n",
    "        for f in s3_bucket.objects.filter(Prefix=folder).all()\n",
    "    ]\n",
    "\n",
    "    # Delete empty filenames\n",
    "    # I think the \"empty\" filename/s is just the bucket path, which isn't a file but is read as an object by the objects.filter function\n",
    "    files_in_s3 = [f for f in files_in_s3 if f != \"\"]\n",
    "\n",
    "    return files_in_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cw3e_files_all = get_filenames_in_s3_folder(bucket, CW3E_cleaned_folder)\n",
    "cw3e_nc_files = [file for file in cw3e_files_all if file.split(\".\")[1]==\"nc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = network_df[\"era-id\"].values\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "print(f\"Processing {len(station_ids)} stations for network: CW3E\")\n",
    "for station_id in tqdm(station_ids[:1]):\n",
    "    # Get s3 filepaths for that station \n",
    "    filenames_in_s3 = [file for file in cw3e_nc_files if station_id in file]\n",
    "    if len(filenames_in_s3) > 0: \n",
    "        filepaths_in_s3 = [f\"s3://{bucket}/{CW3E_cleaned_folder}/{filename_in_s3}\" for filename_in_s3 in filenames_in_s3]\n",
    "    else: \n",
    "        print(f\"No netcdfs found for station: {station_id}\")\n",
    "        continue # Skip to next loop iteration\n",
    "    \n",
    "    # Read in all the files for that station\n",
    "    ds_list = []\n",
    "    for filepath in filepaths_in_s3: \n",
    "        try: \n",
    "            with fs.open(filepath) as fileObj:\n",
    "                # Now we use the open file handle with xarray, without closing it prematurely\n",
    "                ds = xr.open_dataset(fileObj).load()\n",
    "                ds_list.append(ds)\n",
    "        except: \n",
    "            print(\"File {filepath} for station {station_id} could not be read in\")\n",
    "            continue # Skip to next loop iteration\n",
    "\n",
    "    # Concat along time dimension \n",
    "    # And, sort by time so that its in chronological order (not default behavior of xr.concat)\n",
    "    if len(ds_list) > 0: \n",
    "        station_ds = xr.concat(ds_list, dim=\"time\").sortby(\"time\")\n",
    "    else: \n",
    "        continue # Skip to next loop iteration\n",
    "\n",
    "    # Write to zarr \n",
    "    zarr_s3_path = f\"s3://{bucket}/{CW3E_cleaned_folder}/{station_id}.zarr\"\n",
    "    try: \n",
    "        station_ds.to_zarr(\n",
    "            zarr_s3_path,\n",
    "            mode=\"w\",\n",
    "            consolidated=True,\n",
    "        )\n",
    "    except: \n",
    "        print(\"zarr for station {station_id} could not be successfully written to s3 bucket\")\n",
    "        continue # Skip to next loop iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
