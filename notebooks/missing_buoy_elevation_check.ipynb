{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5148db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import geopandas as gpd\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maritime_clean.py\n",
    "def get_elevs(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = soup.find_all(\"pre\")\n",
    "\n",
    "    # Table 1 is smaller than table 2 and 3 by one column\n",
    "    # Start with table 1\n",
    "    tabletext = tables[0]\n",
    "    columns = [\n",
    "        \"Station_ID\",\n",
    "        \"Site_Elevation\",\n",
    "        \"Air_Temp_Elevation\",\n",
    "        \"Anemometer_Elevation\",\n",
    "        \"Barometer_Elevation\",\n",
    "    ]\n",
    "    table = tabletext.get_text().rsplit(\"ELEVATION\", 1)[1]  # Remove headers\n",
    "    table = table.split()  # Remove whitespace\n",
    "    # Should be 5 for table 0 and 6 for table 1+2\n",
    "    composite_list = [\n",
    "        table[x : x + 5] for x in range(0, len(table), 5)\n",
    "    ]  # Split into rows\n",
    "    df = pd.DataFrame(composite_list)\n",
    "    df.columns = columns\n",
    "    df[\"Tide_Reference\"] = (\n",
    "        np.NAN\n",
    "    )  # Add 6th column -- don't really  need this column, drop in update\n",
    "    #     df = df.reindex(columns = [\"Station_ID\", \"Site_Elevation\", \"Air_Temp_Elevation\", \"Anemometer_Elevation\", \"Tide_Reference\", \"Barometer_Elevation\"])\n",
    "\n",
    "    # Table 2 has 6 columns\n",
    "    tabletext = tables[1]\n",
    "    columns = [\n",
    "        \"Station_ID\",\n",
    "        \"Site_Elevation\",\n",
    "        \"Air_Temp_Elevation\",\n",
    "        \"Anemometer_Elevation\",\n",
    "        \"Tide_Reference\",\n",
    "        \"Barometer_Elevation\",\n",
    "    ]\n",
    "    table = tabletext.get_text().rsplit(\"ELEVATION\", 1)[1]  # Remove headers\n",
    "    table = table.split()  # Remove whitespace\n",
    "    # Should be 5 for table 0 and 6 for table 1+2\n",
    "    composite_list = [\n",
    "        table[x : x + 6] for x in range(0, len(table), 6)\n",
    "    ]  # Split into rows\n",
    "    dftemp = pd.DataFrame(composite_list)\n",
    "    dftemp.columns = columns\n",
    "    df = pd.concat([df, dftemp])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #   Table 3 has 9 columns, but we only want the first 6\n",
    "    tabletext = tables[2]\n",
    "    columns = [\n",
    "        \"Station_ID\",\n",
    "        \"Site_Elevation\",\n",
    "        \"Air_Temp_Elevation\",\n",
    "        \"Anemometer_Elevation\",\n",
    "        \"Tide_Reference\",\n",
    "        \"Barometer_Elevation\",\n",
    "    ]\n",
    "    table = tabletext.get_text().rsplit(\"CIRCLE\", 1)[1]  # Remove headers\n",
    "    table = table.split()  # Remove whitespace\n",
    "    # Should be 5 for table 0 and 6 for table 1+2\n",
    "    composite_list = [\n",
    "        table[x : x + 9] for x in range(0, len(table), 9)\n",
    "    ]  # Split into rows\n",
    "    dftemp = pd.DataFrame(composite_list)\n",
    "    dftemp = dftemp.iloc[:, 0:6]  # Drop last three columns\n",
    "    dftemp.columns = columns\n",
    "    df = pd.concat([df, dftemp])\n",
    "    df = df.reset_index(drop=True)\n",
    "    # print(df) # testing\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ndbc.noaa.gov/bmanht.shtml\"\n",
    "elevs_df = get_elevs(url)\n",
    "elevs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "mar_stns = pd.read_csv(\"stationlist_MARITIME.csv\")\n",
    "mar_stns = mar_stns.iloc[:, 3:]  # cleaning up empty cols\n",
    "ndbc_stns = pd.read_csv(\"stationlist_NDBC.csv\")\n",
    "ndbc_stns = ndbc_stns.iloc[:, 3:]  # cleaning up empty cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dataframe of stations that have DO NOT have a valid elevation and are in the network stationlist\n",
    "na_elevs = elevs_df[elevs_df.Site_Elevation.str.contains(\"NA\")]\n",
    "ndbc_elevs = na_elevs[(na_elevs[\"Station_ID\"].isin(ndbc_stns[\"STATION_ID\"]))]\n",
    "mar_elevs = na_elevs[(na_elevs[\"Station_ID\"].isin(mar_stns[\"STATION_ID\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ddcdf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subset from each network's station list those buoys from elevs_df that have nan elevation value\n",
    "empty_ndbc = ndbc_stns[(ndbc_stns[\"STATION_ID\"].isin(ndbc_elevs[\"Station_ID\"]))]\n",
    "empty_mar = mar_stns[(mar_stns[\"STATION_ID\"].isin(mar_elevs[\"Station_ID\"]))]\n",
    "\n",
    "# these were identified by running maritime_clean script and which buoys outputted valid data\n",
    "mar_valid_wx_buoys = [\n",
    "    \"chao3\",\n",
    "    \"pxac1\",\n",
    "    \"smoc1\",\n",
    "    \"agxc1\",\n",
    "    \"baxc1\",\n",
    "    \"mlto3\",\n",
    "    \"ohbc1\",\n",
    "    \"okxc1\",\n",
    "    \"omhc1\",\n",
    "    \"pfdc1\",\n",
    "    \"pfxc1\",\n",
    "    \"ppxc1\",\n",
    "    \"prjc1\",\n",
    "    \"psxc1\",\n",
    "]\n",
    "ndbc_valid_wx_buoys = [\n",
    "    \"46089\",\n",
    "    \"46109\",\n",
    "    \"46110\",\n",
    "    \"46111\",\n",
    "    \"46112\",\n",
    "    \"46113\",\n",
    "    \"46274\",\n",
    "    \"46235\",\n",
    "]\n",
    "\n",
    "# grab the buoys out of the empty_buoy dfs that have valid wx data\n",
    "mar_wx_buoys = empty_mar[(empty_mar[\"STATION_ID\"].isin(mar_valid_wx_buoys))]\n",
    "ndbc_wx_buoys = empty_ndbc[(empty_ndbc[\"STATION_ID\"].isin(ndbc_valid_wx_buoys))]\n",
    "\n",
    "# merge these two together\n",
    "nan_elev_buoys = pd.concat([mar_wx_buoys, ndbc_wx_buoys], axis=0)\n",
    "nan_elev_buoys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    nan_elev_buoys,\n",
    "    geometry=gpd.points_from_xy(nan_elev_buoys.LONGITUDE, nan_elev_buoys.LATITUDE),\n",
    ")\n",
    "# gdf = gdf.query('45 < LATITUDE < 52')\n",
    "# gdf = gdf.query('-117.5 < LONGITUDE < -117')\n",
    "gdf.set_crs(epsg=4326, inplace=True)  # Set CRS\n",
    "gdf_wm = gdf.to_crs(epsg=3857)  # Web mercator\n",
    "\n",
    "# clip basemap to buoys\n",
    "shapepath = \"tl_2021_us_state.shp\"\n",
    "us = gpd.read_file(shapepath)\n",
    "us = us.to_crs(epsg=3857)\n",
    "gdf_us = gdf_wm.clip(us)\n",
    "\n",
    "# plot figure\n",
    "ax = gdf_wm.plot(\n",
    "    \"STATION_ID\", figsize=(10, 10), markersize=20, legend=True, cmap=\"tab20\"\n",
    ")\n",
    "cx.add_basemap(ax, source=cx.providers.Stamen.TonerLite)\n",
    "ax.set_axis_off()\n",
    "\n",
    "# ax.figure.savefig(\"empty_buoys_all_fnl.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffb8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
