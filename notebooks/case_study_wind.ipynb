{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f98ffc0",
   "metadata": {},
   "source": [
    "### Extreme Event Case Study: October 2007 Santa Ana winds + wildfire\n",
    "\n",
    "Provide 2-3 sentences on the HDP purpose (victoria to provide)\n",
    "\n",
    "Provide 2-3 sentences on historical context of the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa38cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import tempfile  # Used for downloading (and then deleting) netcdfs to local drive from s3 bucket\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "import time  # Used for progress bar\n",
    "import sys  # Used for progress bar\n",
    "from case_study_eval_utils import *\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import qaqc stage plot functions\n",
    "sys.path.append(os.path.abspath(\"../scripts/3_qaqc_data\"))\n",
    "from qaqc_plot import flagged_timeseries_plot, _plot_format_helper, id_flag\n",
    "\n",
    "\n",
    "# Set AWS credentials\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3_cl = boto3.client(\"s3\")  # for lower-level processes\n",
    "\n",
    "# Set relative paths to other folders and objects in repository.\n",
    "BUCKET_NAME = \"wecc-historical-wx\"\n",
    "QAQC_DIR = \"3_qaqc_wx\"\n",
    "MERGE_DIR = \"4_merge_wx\"\n",
    "stations_csv_path = f\"s3://{BUCKET_NAME}/{QAQC_DIR}/all_network_stationlist_qaqc.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b06da3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2356845",
   "metadata": {},
   "source": [
    "### from qaqc_plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_format_helper(var: str) -> tuple[str, str, float, float]:\n",
    "    \"\"\"\n",
    "    Formatting helper function for variable names, units, ylabels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var : str\n",
    "        variable name being plotted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ylab : str\n",
    "        variable name for y-axis\n",
    "    unit : str\n",
    "        units of variable\n",
    "    miny : float\n",
    "        min var value for y axis\n",
    "    maxy : float\n",
    "        max var value for y axis\n",
    "    \"\"\"\n",
    "\n",
    "    pr_vars = [\n",
    "        \"pr\",\n",
    "        \"pr_5min\",\n",
    "        \"pr_15min\",\n",
    "        \"pr_1h\",\n",
    "        \"pr_24h\",\n",
    "        \"pr_localmid\",\n",
    "    ]\n",
    "    ps_vars = [\"ps\", \"psl\", \"ps_derived\", \"ps_altimeter\"]\n",
    "\n",
    "    if var == \"tas\":\n",
    "        ylab = \"Air Temperature at 2m\"\n",
    "        unit = \"K\"\n",
    "\n",
    "    elif var == \"tdps\" or var == \"tdps_derived\":\n",
    "        ylab = \"Dewpoint Temperature\"\n",
    "        unit = \"K\"\n",
    "\n",
    "    elif var == \"sfcWind\":\n",
    "        ylab = \"Surface Wind Speed\"\n",
    "        unit = \"$m s^{-1}$\"\n",
    "\n",
    "    elif var == \"sfcWind_dir\":\n",
    "        ylab = \"Surface Wind Direction\"\n",
    "        unit = \"degrees\"\n",
    "\n",
    "    elif var == \"rsds\":\n",
    "        ylab = \"Surface Radiation\"\n",
    "        unit = \"$W m^{-2}$\"\n",
    "\n",
    "    elif var == \"hurs\":\n",
    "        ylab = \"Humidity\"\n",
    "        unit = \"%\"\n",
    "\n",
    "    elif var == \"hurs_derived\":  # added in hurs_derived\n",
    "        ylab = \"Humidity - derived\"\n",
    "        unit = \"%\"\n",
    "\n",
    "    elif var in pr_vars:\n",
    "        ylab = \"Precipitation\"\n",
    "        unit = \"mm\"\n",
    "\n",
    "    elif var == \"accum_pr\":\n",
    "        ylab = \"Annual Accumulated Precipitation\"\n",
    "        unit = \"mm\"\n",
    "\n",
    "    elif var in ps_vars:\n",
    "        ylab = \"Pressure\"\n",
    "        unit = \"Pa\"\n",
    "\n",
    "    elif var == \"elevation\":\n",
    "        ylab = \"Elevation\"\n",
    "        unit = \"m\"\n",
    "\n",
    "    # ideally this would be in utils because it is in qaqc_wholestation\n",
    "    T_X = {\"North_America\": 329.92}  # temperature, K\n",
    "    T_N = {\"North_America\": 210.15}  # temperature, K\n",
    "    D_X = {\"North_America\": 329.85}  # dewpoint temperature, K\n",
    "    D_N = {\"North_America\": 173.15}  # dewpoint temperature, K\n",
    "    W_X = {\"North_America\": 113.2}  # wind speed, m/s\n",
    "    W_N = {\"North_America\": 0.0}  # wind speed, m/s\n",
    "    R_X = {\"North_America\": 1500}  # solar radiation, W/m2\n",
    "    R_N = {\"North_America\": -5}  # solar radiation, W/m2\n",
    "\n",
    "    # for other non-record variables (wind direction, humidity)\n",
    "    N_X = {\"North_America\": 360}  # wind direction, degrees\n",
    "    N_N = {\"North_America\": 0}  # wind direction, degrees\n",
    "    H_X = {\"North_America\": 100}  # humidity, max\n",
    "    H_N = {\"North_America\": 0}  # humidity, min\n",
    "    E_X = {\"North_America\": 6210.0}  # elevation, m\n",
    "    E_N = {\"North_America\": -100}  # elevation, m\n",
    "\n",
    "    # pressure, with elevation options\n",
    "    S_X = {\"North_America\": 108330}  # pressure, Pa\n",
    "    S_N = {\"North_America\": 87000}  # sea level pressure only, Pa\n",
    "    SALT_N = {\n",
    "        \"North_America\": 45960\n",
    "    }  # non-sea level pressure, Pa, reduced min based on max elevation (6190 m)\n",
    "\n",
    "    # precipitation, with variations depending on reporting interval\n",
    "    P_X = {\"North_America\": 656}  # precipitation, mm, 24-hr rainfall\n",
    "    PALT5_X = {\"North_America\": 31.8}  # precipitation, mm, 5-min rainfall, WECC-wide\n",
    "    PALT15_X = {\n",
    "        \"North_America\": 25.4\n",
    "    }  # precipitation, mm, 15-min rainfall, specific to VALLEYWATER\n",
    "    PACC_X = {\n",
    "        \"North_America\": 10000\n",
    "    }  # accumulated precipitation, mm, arbirtarily set to a high max value\n",
    "    P_N = {\"North_America\": 0}  # precipitaiton, mm\n",
    "\n",
    "    maxes = {\n",
    "        \"tas\": T_X,\n",
    "        \"tdps\": D_X,\n",
    "        \"tdps_derived\": D_X,\n",
    "        \"sfcWind\": W_X,\n",
    "        \"sfcWind_dir\": N_X,\n",
    "        \"psl\": S_X,\n",
    "        \"ps\": S_X,\n",
    "        \"ps_derived\": S_X,\n",
    "        \"ps_altimeter\": S_X,\n",
    "        \"rsds\": R_X,\n",
    "        \"pr\": P_X,\n",
    "        \"pr_5min\": PALT5_X,\n",
    "        \"pr_15min\": PALT15_X,\n",
    "        \"pr_1h\": P_X,\n",
    "        \"pr_24h\": P_X,\n",
    "        \"pr_localmid\": P_X,\n",
    "        \"accum_pr\": PACC_X,\n",
    "        \"hurs\": H_X,\n",
    "        \"hurs_derived\": H_X, # added in hurs_derived\n",
    "        \"elevation\": E_X,\n",
    "    }\n",
    "    mins = {\n",
    "        \"tas\": T_N,\n",
    "        \"tdps\": D_N,\n",
    "        \"tdps_derived\": D_N,\n",
    "        \"sfcWind\": W_N,\n",
    "        \"sfcWind_dir\": N_N,\n",
    "        \"psl\": S_N,\n",
    "        \"ps\": SALT_N,\n",
    "        \"ps_derived\": SALT_N,\n",
    "        \"ps_altimeter\": SALT_N,\n",
    "        \"rsds\": R_N,\n",
    "        \"pr\": P_N,\n",
    "        \"pr_5min\": P_N,\n",
    "        \"pr_15min\": P_N,\n",
    "        \"pr_1h\": P_N,\n",
    "        \"pr_24h\": P_N,\n",
    "        \"pr_localmid\": P_N,\n",
    "        \"accum_pr\": P_N,\n",
    "        \"hurs\": H_N,\n",
    "        \"hurs_derived\": H_N,  # added in hurs_derived\n",
    "        \"elevation\": E_N,\n",
    "    }\n",
    "    miny = mins[var][\"North_America\"]\n",
    "    maxy = maxes[var][\"North_America\"]\n",
    "\n",
    "    return ylab, unit, miny, maxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab9c95",
   "metadata": {},
   "source": [
    "### from case_study_eval_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_issue_check(network: str, var: str, stn: str):\n",
    "    \"\"\"\n",
    "    Identifies if station under evaluation has a known network issue.\n",
    "    At present, only prints out a statement if there is an issue.\n",
    "    Eventually may want to do <something>\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : str\n",
    "        Name of network to check\n",
    "    var : str\n",
    "        Name of variable to check\n",
    "    stn : str\n",
    "        Name of station to check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1. See \"Known Network Issues for QA/QC Validation\" planning doc.\n",
    "    \"\"\"\n",
    "    print(\"Checking for known station issues...\")\n",
    "\n",
    "    # RAWS\n",
    "    if network == \"RAWS\":\n",
    "        if var == \"tas\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: values may be too high (on order of 10Â°F) if sun is shining strongly and winds are light.\"\n",
    "            )\n",
    "\n",
    "        elif var == \"pr\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: stations are not maintained in winter, instrument may freeze. Consider subsetting for May-September.\"\n",
    "            )\n",
    "            # V2 note: exclude RAWS data during specific notes -- would require new function to flag\n",
    "\n",
    "    # SNOTEL\n",
    "    if network == \"SNOTEL\":\n",
    "        if var == \"tas\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: values may remain at exactly 0.0Â°C for two or more consecutive days. Should be caught by unusual_streaks.\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: SNOTEL temperature sensors transition between mid-1990s and mid-2000s to new sensory type produces warm bias at \\\n",
    "            colder temperatures. Min temperature may be too high, max temperature may be too low.\"\n",
    "            )\n",
    "            # V2 note: trend analysis may identify these issues, nearest neighbor check could identify\n",
    "\n",
    "    # ASOSAWOS + OtherISD\n",
    "    if network == \"ASOSAWOS\":\n",
    "        if var == \"tdps\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: values may be stuck at around 0.0Â°C, or have excessive mirror contamination. Should be caught by unsusual_streaks.\"\n",
    "            )\n",
    "\n",
    "    if network == \"ASOSAWOS\" or network == \"OtherISD\":\n",
    "        if var == \"pr\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: ASOS network began installation in 1996, with poor instrumentation for measuring snowfall. Precipitation between \\\n",
    "            1980-1996 may be more likely to be flagged.\"\n",
    "            )\n",
    "\n",
    "    # CIMIS\n",
    "    if network == \"CIMIS\":\n",
    "        if var == \"pr\":\n",
    "            print(\n",
    "                f\"Known network issue for {network} {var}: stations located in flat agricultural areas, sensor may be detecting sprinkler irrigation events. \\\n",
    "            Network does have stringent QC protocol.\"\n",
    "            )\n",
    "            # V2 note: nearest neighbor check could confirm\n",
    "\n",
    "    # NDBC / MARITIME\n",
    "    if network == \"NDBC\" or network == \"MARITIME\":\n",
    "        print(\n",
    "            f\"Known network issue for {network}: some buoys have data past their known disestablishment dates. Should be caught by spurious_buoy_check.\"\n",
    "        )\n",
    "\n",
    "        if stn == \"NDBC_46044\":\n",
    "            print(\n",
    "                \"Known network issue for NDBC_46044: buoy went adrift during reporting period. Confirm if data was flagged by QA/QC.\"\n",
    "            )\n",
    "            # V2 note: if not flagged, needs to be -- would require new function\n",
    "\n",
    "        if (\n",
    "            stn == \"MARITIME_MTYC1\"\n",
    "            or stn == \"MARITIME_MEYC1\"\n",
    "            or stn == \"MARITIME_SMOC1\"\n",
    "            or stn == \"MARITIME_ICAC1\"\n",
    "        ):\n",
    "            print(\n",
    "                f\"Known network issue for {network} station {stn}: buoy was renamed and/or relocated. May cause issue for station proximity tests.\"\n",
    "            )\n",
    "            # V2 note: noted in qaqc_buoy_check but not handled -- would require new function\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def subset_eval_stns(\n",
    "    event_to_eval: str,\n",
    "    stn_list: pd.DataFrame,\n",
    "    specific_station: str | None = None,\n",
    "    subset: int | None = None,\n",
    "    return_stn_ids: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies stations to evaluate for specific V1 QA/QC events.\n",
    "    Option to subset to a more manageable number of random stations for initial evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_to_eval : str\n",
    "        options: santa_ana_wind, winter_storm, AR, mudslide, aug2020_heatwave, sep2020_heatwave, aug2022_heatwave, offshore_wind\n",
    "    stn_list : pd.DataFrame\n",
    "        station list\n",
    "    specific_station : str, optional\n",
    "        name of specific station to check\n",
    "    subset : int, optional\n",
    "        optional value to specify number of stations to return, useful for big events\n",
    "    return_stn_ids : bool, optional\n",
    "        Option to return string names for ease of use\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eval_stns : pd.DataFrame\n",
    "        Subset of stations relevant to a desired extreme event for analysis\n",
    "\n",
    "    To Dos\n",
    "    -------\n",
    "    1. Validation check on event_to_eval options\n",
    "    2. Need an option for \"WECC wide\" (or no spatial subsetting)\n",
    "    3. Update station list being used here -- SNOTEL dates have been fixed.\n",
    "    \"\"\"\n",
    "\n",
    "    event_flags = []\n",
    "    event_flags.append(\"all\")\n",
    "    event_flags.append(event_to_eval)\n",
    "\n",
    "    # grab stations per event\n",
    "    event_stns = stn_list[stn_list[\"event_type\"].isin(event_flags)]\n",
    "\n",
    "    # exclude \"manual check on end date\" for time being -- SNOTEl stations all have 2100 as their end date regardless of when data actually ends\n",
    "    mask = event_stns[\"notes\"] == \"manual check on end date\"\n",
    "    event_stns = event_stns[~mask]\n",
    "    # print('{} potential stations available for evaluation for {} event!'.format(len(event_stns), event_to_eval))\n",
    "\n",
    "    # identify stations in geographic region we are looking for\n",
    "    ca_county = gpd.read_file(CENSUS_SHP)\n",
    "\n",
    "    # different areas based on events\n",
    "    if event_to_eval == \"santa_ana_wind\":\n",
    "        counties_to_grab = [\n",
    "            \"Los Angeles\",\n",
    "            \"Orange\",\n",
    "            \"San Diego\",\n",
    "            \"San Bernardino\",\n",
    "            \"Riverside\",\n",
    "        ]\n",
    "\n",
    "    elif event_to_eval == \"winter_storm\":\n",
    "        # focus on Northern/Central/Bay Area to begin with // WECC wide\n",
    "        counties_to_grab = [\n",
    "            \"Butte\",\n",
    "            \"Colusa\",\n",
    "            \"Del Norte\",\n",
    "            \"Glenn\",\n",
    "            \"Humboldt\",\n",
    "            \"Lake\",\n",
    "            \"Lassen\",\n",
    "            \"Mendocino\",\n",
    "            \"Modoc\",\n",
    "            \"Nevada\",\n",
    "            \"Plumas\",\n",
    "            \"Shasta\",\n",
    "            \"Sierra\",\n",
    "            \"Siskiyou\",\n",
    "            \"Tehama\",\n",
    "            \"Trinity\",\n",
    "            \"Alpine\",\n",
    "            \"Amador\",\n",
    "            \"Calaveras\",\n",
    "            \"El Dorado\",\n",
    "            \"Fresno\",\n",
    "            \"Inyo\",\n",
    "            \"Kings\",\n",
    "            \"Madera\",\n",
    "            \"Mariposa\",\n",
    "            \"Merced\",\n",
    "            \"Mono\",\n",
    "            \"Placer\",\n",
    "            \"Sacramento\",\n",
    "            \"San Joaquin\",\n",
    "            \"Stanislaus\",\n",
    "            \"Sutter\",\n",
    "            \"Yuba\",\n",
    "            \"Tulare\",\n",
    "            \"Tuolumne\",\n",
    "            \"Yolo\",\n",
    "            \"Alameda\",\n",
    "            \"Contra Costa\",\n",
    "            \"Marin\",\n",
    "            \"Monterey\",\n",
    "            \"Napa\",\n",
    "            \"San Benito\",\n",
    "            \"San Francisco\",\n",
    "            \"San Mateo\",\n",
    "            \"Santa Clara\",\n",
    "            \"Santa Cruz\",\n",
    "            \"Solano\",\n",
    "            \"Sonoma\",\n",
    "        ]\n",
    "\n",
    "    elif event_to_eval == \"mudslide\":\n",
    "        counties_to_grab = [\"Santa Barbara\"]\n",
    "\n",
    "    elif event_to_eval == \"AR\":\n",
    "        counties_to_grab = []  # CA\n",
    "\n",
    "    elif event_to_eval == \"aug2020_heatwave\":\n",
    "        counties_to_grab = []  # CA\n",
    "\n",
    "    elif event_to_eval == \"sep2020_heatwave\":\n",
    "        counties_to_grab = [\n",
    "            \"San Luis Obispo\",\n",
    "            \"Kern\",\n",
    "            \"San Bernadino\",\n",
    "            \"Santa Barbara\",\n",
    "            \"Ventura\",\n",
    "            \"Los Angeles\",\n",
    "            \"Orange\",\n",
    "            \"Riverside\",\n",
    "            \"San Diego\",\n",
    "            \"Imperial\",\n",
    "        ]\n",
    "\n",
    "    elif event_to_eval == \"aug2022_heatwave\":\n",
    "        # August 2022 -- Labor Day Heatwave \"aug2022_heatwave\"\n",
    "        counties_to_grab = [\n",
    "            \"San Luis Obispo\",\n",
    "            \"Kern\",\n",
    "            \"San Bernadino\",\n",
    "            \"Santa Barbara\",\n",
    "            \"Ventura\",\n",
    "            \"Los Angeles\",\n",
    "            \"Orange\",\n",
    "            \"Riverside\",\n",
    "            \"San Diego\",\n",
    "            \"Imperial\",\n",
    "        ]\n",
    "\n",
    "    elif event_to_eval == \"offshore_wind\":\n",
    "        counties_to_grab = [\n",
    "            \"San Diego\",\n",
    "            \"Orange\",\n",
    "            \"Los Angeles\",\n",
    "            \"Ventura\",\n",
    "            \"Santa Barbara\",\n",
    "            \"San Luis Obispo\",\n",
    "            \"Monterey\",\n",
    "            \"Santa Cruz\",\n",
    "            \"San Mateo\",\n",
    "            \"Santa Clara\",\n",
    "            \"Alameda\",\n",
    "            \"San Francisco\",\n",
    "            \"Contra Costa\",\n",
    "            \"Solano\",\n",
    "            \"Marin\",\n",
    "            \"Sonoma\",\n",
    "            \"Mendocino\",\n",
    "            \"Humboldt\",\n",
    "            \"Del Norte\",\n",
    "        ]\n",
    "\n",
    "    target_counties = ca_county[ca_county[\"NAME\"].isin(counties_to_grab)]\n",
    "    target_counties = GeoDataFrame(target_counties, geometry=target_counties.geometry)\n",
    "\n",
    "    geometry = [\n",
    "        Point(latlon_to_mercator_cartopy(lat, lon))\n",
    "        for lat, lon in zip(event_stns.latitude, event_stns.longitude)\n",
    "    ]\n",
    "    event_stns = GeoDataFrame(event_stns, geometry=geometry).set_crs(\n",
    "        crs=\"EPSG:3857\", allow_override=True\n",
    "    )\n",
    "    # adding geometry column\n",
    "    event_stns_local = gpd.overlay(event_stns, target_counties, how=\"intersection\")\n",
    "    num_event_stns_local = len(event_stns_local)\n",
    "    # subsetting for stations within county boundaries\n",
    "    print(\n",
    "        f\"{num_event_stns_local} potential stations available for evaluation for {event_to_eval} event.\"\n",
    "    )\n",
    "\n",
    "    # Check if a specific_station is requested and return that one\n",
    "    if specific_station is not None:\n",
    "        eval_stns = event_stns[event_stns[\"era-id\"] == specific_station]\n",
    "        if len(eval_stns) == 0:\n",
    "            raise ValueError(\n",
    "                f\"Station {specific_station} is not within the training/event dataset\"\n",
    "            )\n",
    "        return eval_stns\n",
    "\n",
    "    if subset != None:\n",
    "        if num_event_stns_local <= subset:\n",
    "            eval_stns = event_stns_local\n",
    "        else:\n",
    "            eval_stns = event_stns_local.sample(subset, replace=False)\n",
    "            print(\n",
    "                f\"{subset} stations selected for evaluation for {event_to_eval} event!\"\n",
    "            )\n",
    "    else:\n",
    "        eval_stns = event_stns_local\n",
    "\n",
    "    if return_stn_ids:\n",
    "        print(\"Stations selected for evaluation:\\n\", list(eval_stns[\"era-id\"]))\n",
    "\n",
    "    return eval_stns\n",
    "\n",
    "\n",
    "def id_all_flags(ds: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Prints all unique values of all eraqaqc flags\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        station data in xr format (not pd.DataFrame)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    ds_vars = list(ds.keys())\n",
    "    qc_vars = [i for i in ds_vars if \"_eraqc\" in i]\n",
    "    if len(qc_vars) == 0:\n",
    "        print(\n",
    "            \"Station has no eraqc variables -- please double check that this station has completed QA/QC!\"\n",
    "        )\n",
    "    else:\n",
    "        for var in qc_vars:\n",
    "            print(var, np.unique(ds[var].data))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def event_info(\n",
    "    event: str, alt_start_date: str | None = None, alt_end_date: str | None = None\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Utility function to return useful information for a specific designated event.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    event : str\n",
    "        name of event to evaluate\n",
    "    alt_start_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "    alt_end_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, str]\n",
    "        start and end dates for requested event\n",
    "\n",
    "    To dos\n",
    "    ------\n",
    "    1. Alternative start / end date format check\n",
    "    \"\"\"\n",
    "\n",
    "    start_date = {\n",
    "        \"santa_ana_wind\": \"2007-10-19\",\n",
    "        \"winter_storm\": \"1990-12-20\",\n",
    "        \"AR\": \"2017-01-16\",\n",
    "        \"mudslide\": \"2018-01-05\",\n",
    "        \"aug2020_heatwave\": \"2020-08-14\",\n",
    "        \"sep2020_heatwave\": \"2020-09-05\",\n",
    "        \"aug2022_heatwave\": \"2022-08-30\",\n",
    "        \"offshore_wind\": \"2021-01-15\",\n",
    "        \"alternative\": alt_start_date,\n",
    "    }\n",
    "\n",
    "    end_date = {\n",
    "        \"santa_ana_wind\": \"2007-11-16\",\n",
    "        \"winter_storm\": \"1990-12-24\",\n",
    "        \"AR\": \"2017-01-20\",\n",
    "        \"mudslide\": \"2018-01-09\",\n",
    "        \"aug2020_heatwave\": \"2020-08-15\",\n",
    "        \"sep2020_heatwave\": \"2020-09-08\",\n",
    "        \"aug2022_heatwave\": \"2022-09-09\",\n",
    "        \"offshore_wind\": \"2021-01-16\",\n",
    "        \"alternative\": alt_end_date,\n",
    "    }\n",
    "\n",
    "    event_start = start_date[event]\n",
    "    event_end = end_date[event]\n",
    "\n",
    "    return (event_start, event_end)\n",
    "\n",
    "\n",
    "def event_subset(\n",
    "    df: pd.DataFrame,\n",
    "    event: str,\n",
    "    buffer: int | None = 7,\n",
    "    alt_start_date: str | None = None,\n",
    "    alt_end_date: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Subsets for the event itself + buffer around to identify event.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    df: pd.DataFrame\n",
    "        stationlist dataframe\n",
    "    event : str\n",
    "        name of event\n",
    "    buffer : int, optional\n",
    "        number of days to include as a buffer around event start/end date\n",
    "    alt_start_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "    alt_end_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    event_sub : pd.DataFrame\n",
    "        subset of stationlist within date range of event or alternative\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Subsetting station record for event duration with {str(buffer)} day buffer...\"\n",
    "    )\n",
    "\n",
    "    # set to searchable datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    # grab dates from lookup dictionary\n",
    "    event_start, event_end = event_info(event, alt_start_date, alt_end_date)\n",
    "\n",
    "    # subset for event dates + buffer\n",
    "    datemask = (\n",
    "        df[\"time\"] >= (pd.Timestamp(event_start) - datetime.timedelta(days=buffer))\n",
    "    ) & (df[\"time\"] <= (pd.Timestamp(event_end) + datetime.timedelta(days=buffer)))\n",
    "    event_sub = df.loc[datemask]\n",
    "\n",
    "    return event_sub\n",
    "\n",
    "\n",
    "def flags_during_event(subset_df: pd.DataFrame, var: str, event: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Provides info on which flags were placed during event for evaluation\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    subset_df : pd.DataFrame\n",
    "\n",
    "    var : str\n",
    "        name of variable to assess flags\n",
    "    event : str\n",
    "        name of case study event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_event_flags : list[str]\n",
    "        all event flags set\n",
    "    \"\"\"\n",
    "\n",
    "    event_flags = subset_df[var + \"_eraqc\"].unique()\n",
    "    print(f\"Flags set on {var} during {event} event: {event_flags}\")\n",
    "    all_event_flags = []\n",
    "    for item in subset_df[var + \"_eraqc\"].unique():\n",
    "        all_event_flags.append(item)\n",
    "\n",
    "    return all_event_flags\n",
    "\n",
    "\n",
    "def find_other_events(\n",
    "    df, event_start, event_end, buffer=14, subset=None, return_stn_ids=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Event finder not tied to specified case study events.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    df : pd.DataFrame\n",
    "        stationlist\n",
    "    event_start : str\n",
    "        start of event, format \"YYYY-MM-DD\"\n",
    "    event_end : str\n",
    "        end of event, format \"YYYY-MM-DD\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eval_stns : pd.DataFrame\n",
    "        subset of stations for other events of interest\n",
    "\n",
    "    To dos\n",
    "    ------\n",
    "    1. Manual end date check no longer relevant, make sure stationlist passed is the correct updated version.\n",
    "    2. Start / end date format check\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Subsetting station record for event duration with {str(buffer)} day buffer...\"\n",
    "    )\n",
    "\n",
    "    df[\"start_date\"] = pd.to_datetime(df[\"start_date\"])\n",
    "    df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\n",
    "    event_start = pd.to_datetime(event_start).tz_localize(\"UTC\")\n",
    "    event_end = pd.to_datetime(event_end).tz_localize(\"UTC\")\n",
    "\n",
    "    event_sub = df.loc[\n",
    "        (df[\"start_date\"] <= (event_start - datetime.timedelta(days=buffer)))\n",
    "        & (df[\"end_date\"] >= (event_end + datetime.timedelta(days=buffer)))\n",
    "    ]\n",
    "\n",
    "    # exclude \"manual check on end date\" stations since we don't know when they actually end\n",
    "    event_sub = event_sub.loc[event_sub[\"notes\"] != \"manual check on end date\"]\n",
    "\n",
    "    # subset to make more manageable\n",
    "    if subset != None:\n",
    "        if len(event_sub) <= subset:\n",
    "            eval_stns = event_sub\n",
    "        else:\n",
    "            eval_stns = event_sub.sample(subset, replace=False)\n",
    "            print(f\"{subset} stations selected for evaluation for comparison!\")\n",
    "    else:\n",
    "        eval_stns = event_sub\n",
    "\n",
    "    # return station ids for ease\n",
    "    if return_stn_ids:\n",
    "        print(\"Stations selected for evaluation:\\n\", list(eval_stns[\"era-id\"]))\n",
    "\n",
    "    return eval_stns\n",
    "\n",
    "\n",
    "def latlon_to_mercator_cartopy(lat: float, lon: float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Converts lat/lon coordinates to mercator for plotting.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    lat : float\n",
    "        latitude\n",
    "    lon : float\n",
    "        longitude\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "    \"\"\"\n",
    "\n",
    "    proj_latlon = CRS(\"EPSG:4326\")\n",
    "    proj_mercator = CRS(\"EPSG:3857\")\n",
    "\n",
    "    # Transform the coordinates\n",
    "    transformer = Transformer.from_crs(proj_latlon, proj_mercator, always_xy=True)\n",
    "    x, y = transformer.transform(lon, lat)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def stn_visualize(stn_id, stn_list, event_to_eval):\n",
    "    \"\"\"\n",
    "    Produces simple map of station relevant to event boundary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stn_id : str\n",
    "        name of station\n",
    "    stn_list : pd.DataFrame\n",
    "        stationlist\n",
    "    event_to_eval : str\n",
    "        name of case study event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # grab station id info and reproject coords\n",
    "    stn = stn_list.loc[stn_list[\"era-id\"] == stn_id]\n",
    "    lon, lat = stn.longitude.values[0], stn.latitude.values[0]\n",
    "    x, y = latlon_to_mercator_cartopy(lat, lon)\n",
    "\n",
    "    # figure set-up\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.epsg(3857)})\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cf.BORDERS)\n",
    "    ax.add_feature(cf.STATES, lw=0.5)\n",
    "\n",
    "    ax.set_extent([lon + 1, lon - 1, lat - 1, lat + 1])\n",
    "\n",
    "    # Obtain the limits of the plot\n",
    "    x0, x1, y0, y1 = ax.get_extent()\n",
    "\n",
    "    # Create a polygon with the limits of the plot\n",
    "    polygon = Polygon(((x0, y0), (x0, y1), (x1, y1), (x1, y0)))\n",
    "\n",
    "    # Use only the counties that overlap with the actual plot\n",
    "    ca_county = gpd.read_file(CENSUS_SHP)\n",
    "    counties = ca_county[ca_county.overlaps(polygon)]\n",
    "\n",
    "    # Plot the counties' geometries\n",
    "    for geometry in counties.geometry:\n",
    "        ax.add_geometries(\n",
    "            geometry.boundary,\n",
    "            crs=ax.projection,\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=\"teal\",\n",
    "            lw=0.5,\n",
    "        )\n",
    "\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax.plot(lon, lat, \"ok\", markersize=8, transform=ccrs.PlateCarree(), mfc=\"none\")\n",
    "    ax.plot(x, y, \".r\", markersize=4)\n",
    "    ax.annotate(f\"{stn_id}\", xy=(x, y), xytext=(x + 10, y + 10), fontsize=6)\n",
    "    # station name\n",
    "    gl = ax.gridlines(\n",
    "        crs=ccrs.PlateCarree(), draw_labels=[\"bottom\", \"left\"], ls=\":\", lw=0.5\n",
    "    )\n",
    "    ax.set_title(f\"{event_to_eval} evaluation \\nat {stn_id}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def event_plot(\n",
    "    df: pd.DataFrame,\n",
    "    var: str,\n",
    "    event: str,\n",
    "    alt_start_date: str | None = None,\n",
    "    alt_end_date: str | None = None,\n",
    "    dpi: int | None = None,\n",
    "):\n",
    "    \"\"\"Produces timeseries of variables that have flags placed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        stationlist\n",
    "    var : str\n",
    "        name of variable\n",
    "    event : str\n",
    "        name of case study event\n",
    "    alt_start_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "    alt_end_date : str\n",
    "        date of different event, must be in format \"YYYY-MM-DD\"\n",
    "    dpi : int\n",
    "        figure resolution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "    # plot all observations\n",
    "    df.plot(\n",
    "        ax=ax,\n",
    "        x=\"time\",\n",
    "        y=var,\n",
    "        marker=\".\",\n",
    "        ms=4,\n",
    "        lw=1,\n",
    "        color=\"k\",\n",
    "        alpha=0.5,\n",
    "        label=\"Cleaned data\",\n",
    "    )\n",
    "\n",
    "    # plot event timeline\n",
    "    event_start, event_end = event_info(event, alt_start_date, alt_end_date)\n",
    "    ax.axvspan(event_start, event_end, color=\"red\", alpha=0.1, label=\"{}\".format(event))\n",
    "\n",
    "    # ax.axhline(event_start, color='red', lw=2, alpha=0.25)\n",
    "    # ax.axhline(event_end, color='red', lw=2, alpha=0.25)\n",
    "    # ax.fill_between(x='time', 0, 1, where=y)\n",
    "\n",
    "    # plot any flags placed by QA/QC\n",
    "    if len(df[var + \"_eraqc\"].dropna().unique()) != 0:\n",
    "        # identify flagged data, can handle multiple flags\n",
    "        for flag in df[var + \"_eraqc\"].dropna().unique():\n",
    "            flag_name = id_flag(flag)\n",
    "            flag_str = 100 * len(df.loc[df[var + \"_eraqc\"] == flag, var]) / len(df)\n",
    "            flag_label = f\"{flag_str:.3f}% of data flagged by {flag_name}\"\n",
    "\n",
    "            flagged_data = df[~df[var + \"_eraqc\"].isna()]\n",
    "            flagged_data.plot(\n",
    "                x=\"time\",\n",
    "                y=var,\n",
    "                ax=ax,\n",
    "                marker=\"o\",\n",
    "                ms=7,\n",
    "                lw=0,\n",
    "                mfc=\"none\",\n",
    "                color=\"C3\",\n",
    "                label=flag_label,\n",
    "            )\n",
    "\n",
    "    legend = ax.legend(loc=\"upper left\", prop={\"size\": 8})\n",
    "\n",
    "    # plot aesthetics\n",
    "    ylab, units, miny, maxy = _plot_format_helper(var)\n",
    "    plt.ylabel(f\"{ylab} [{units}]\")\n",
    "    plt.xlabel(\"\")\n",
    "    stn = df[\"station\"].unique()[0]\n",
    "    plt.title(\n",
    "        f\"QA/QC event evaluation: {event}: {stn}\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96113b1",
   "metadata": {},
   "source": [
    "## Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using google doc for event\n",
    "# subset all stations for time dates, probably within a 2 week window on either side\n",
    "# depending on the event, susbet for specific variables (ask Victoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a list of weather stations\n",
    "# We just use the names of the stations to filter\n",
    "# Ideally I'd like to see this moved to the AWS bucket\n",
    "train_stns = pd.read_csv(\n",
    "    \"s3://wecc-historical-wx/4_merge_wx/all_network_stationlist_merge.csv\"\n",
    ")\n",
    "\n",
    "# Create new geometry column from lat and lon columns\n",
    "# This will enable us to filter through the GeoDataFrame using our county geometry\n",
    "train_stns = gpd.GeoDataFrame(\n",
    "    train_stns,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        train_stns.longitude, train_stns.latitude, crs=\"EPSG:4326\"\n",
    "    ),\n",
    ")\n",
    "# train_stns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9871eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in county shapefiles from S3\n",
    "ca_counties = gpd.read_file(\n",
    "    \"s3://wecc-historical-wx/0_maps/ca_counties/CA_Counties.shp\"\n",
    ")\n",
    "ca_counties = ca_counties.to_crs(train_stns.crs)  # Convert to station CRS\n",
    "# ca_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db73da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_start_date = \"2007-10-05\"  # Start a few days before the mudslide\n",
    "event_end_date = \"2007-11-30\"\n",
    "\n",
    "event_geom = ca_counties[ca_counties[\"NAME\"] == \"San Diego\"]  # Get event geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211080b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stns[\"intersects\"] = train_stns.intersects(\n",
    "    event_geom.unary_union\n",
    ")  # See which stations intersect with the event polygon\n",
    "event_stns = train_stns[train_stns[\"intersects\"] == True].reset_index(\n",
    "    drop=True\n",
    ")  # Get just those stations, drop the others\n",
    "event_stns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed933a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot to visualize the stations and event poly\n",
    "fig, ax = plt.subplots()\n",
    "event_geom.boundary.plot(color=\"black\", ax=ax, zorder=10)\n",
    "event_stns.geometry.plot(markersize=15, ax=ax, zorder=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for variables of interest\n",
    "event_stns.columns\n",
    "# sfcWind, hurs, pr, pr_24h, pr_1h, pr_5min, pr_localmid\n",
    "\n",
    "# Victoria\n",
    "# sfcWind, relative humidity, air temp\n",
    "# usually massive drop in hum and bump in air temp for stana ana wind events, should see temp going up and hurs staying low even afte wind event has died down\n",
    "# wind direction also\n",
    "# should see winds staying low\n",
    "# winds help push wildfires out, but also put it out\n",
    "# should see bump in sfcwind\n",
    "\n",
    "# sum up:\n",
    "# wind speed, secondarily wind diraction (less), air temp, relative hum\n",
    "# should see winds rel low, variable, then leading into event see spike in wind speed, fairly unidirectional wind direction from NE to SW, then after wind event has ended, wind speed drops down\n",
    "# during peak of wind, see bump in tem (and diuran) and plummeting of rh\n",
    "# as wind event dies, air temp stays high if not approach world records (for wildfire) and rh stay low\n",
    "\n",
    "# any functions I devleop, make sure methods are consistent, put those functions into the script\n",
    "\n",
    "# Victoria will start on case study next week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"s3://{BUCKET_NAME}/{MERGE_DIR}/ASOSAWOS/ASOSAWOS_72293193107.zarr\"\n",
    "ds = xr.open_zarr(url)\n",
    "df = ds.to_dataframe()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_info(\"santa_ana_wind\",\"2007-10-05\",\"2007-11-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = event_subset(df, 'santa_ana_wind', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_during_event(subset, 'hurs_derived', 'santa_ana_wind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_list = pd.read_csv(\n",
    "    \"s3://wecc-historical-wx/4_merge_wx/all_network_stationlist_merge.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_visualize(\"ASOSAWOS_72293193107\", stn_list, \"santa_ana_wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_plot(df, \"sfcWind_dir\", \"santa_ana_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020e7aa",
   "metadata": {},
   "source": [
    "wind speed, secondarily wind diraction (less), air temp, relative hum\n",
    "\n",
    "'hurs_derived' - relative humidity, derived\n",
    "\n",
    "'sfcWind' - wind speed at 10m\n",
    "\n",
    "'sfcWind_dir' - wind direction\n",
    "\n",
    "no air temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cf2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stns_gdf = gpd.GeoDataFrame(\n",
    "    stn_list,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        stn_list.longitude, stn_list.latitude, crs=\"EPSG:4326\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "witch_perim = gpd.read_file(\n",
    "    \"Historic_Geomac_Perimeters_Combined_2000_2018/US_HIST_FIRE_PERIMTRS_2000_2018_DD83.shp\"\n",
    ")\n",
    "witch_perim = witch_perim.to_crs(stns_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f21002",
   "metadata": {},
   "outputs": [],
   "source": [
    "stns_gdf[\"intersects\"] = stns_gdf.intersects(\n",
    "    witch_perim.unary_union\n",
    ")  # See which stations intersect with the event polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stns = stns_gdf[stns_gdf[\"intersects\"] == True].reset_index(\n",
    "    drop=True\n",
    ")  # Get just those stations, drop the others\n",
    "event_stns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e93923",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_start_date = \"2007-10-05\"  # Start a few days before the mudslide\n",
    "event_end_date = \"2007-11-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stns['start-date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stns = event_stns[\n",
    "    (event_stns[\"start-date\"] >= event_start_date) & (df[\"end-date\"] <= event_end_date)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "witch_perim.boundary.plot(color=\"black\", ax=ax, zorder=10)\n",
    "event_stns.geometry.plot(markersize=15, ax=ax, zorder=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'sfcWind'\n",
    "for i in event_stns.index:\n",
    "    stn = event_stns.iloc[i]['era-id']\n",
    "    network = event_stns.iloc[i]['network']\n",
    "    url = f\"s3://{BUCKET_NAME}/{MERGE_DIR}/{network}/{stn}.zarr\"\n",
    "    ds = xr.open_zarr(url)\n",
    "    df = ds.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "    print(stn)\n",
    "    print(network)\n",
    "    print(df.columns)\n",
    "    if var in df.columns:\n",
    "        event_plot(df, \"sfcWind\", \"santa_ana_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537b81e",
   "metadata": {},
   "source": [
    "## Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d46f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce simple timeseries plots of variable over the event\n",
    "# include QC flags\n",
    "# if possible, add shaded bars or something (look at old code) to indicate the event itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc080f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    ax=ax,\n",
    "    x=\"time\",\n",
    "    y=var,\n",
    "    marker=\".\",\n",
    "    ms=4,\n",
    "    lw=1,\n",
    "    color=\"k\",\n",
    "    alpha=0.5,\n",
    "    label=\"Cleaned data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e9c63",
   "metadata": {},
   "source": [
    "## Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d15fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some kind of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table / stats \"read out\" on extremes during the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table / stats \"read out\" on QC flags, including if we think refinement to QC tests would improve coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function/thing in terms of how many stations \"detected\" the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary information via markdown close out of what we have learned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist-obs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
